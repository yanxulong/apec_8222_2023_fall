[
  {
    "objectID": "Introduction/introduction_what-is-big-data-and-why-does-it-matter.html",
    "href": "Introduction/introduction_what-is-big-data-and-why-does-it-matter.html",
    "title": "1¬† What is Big Data and Why Does it Matter",
    "section": "",
    "text": "Agenda"
  },
  {
    "objectID": "Introduction/introduction_what-is-big-data-and-why-does-it-matter.html#computers-and-software",
    "href": "Introduction/introduction_what-is-big-data-and-why-does-it-matter.html#computers-and-software",
    "title": "1¬† What is Big Data and Why Does it Matter",
    "section": "Computers and software",
    "text": "Computers and software\n\nNecessary to succeed: mastering software and code.\n\nApplied Economics itself is shifting in this direction.\n\nMore focus on code expertise.\nDistinguishes us (in a very positive way) from traditional Econ programs.\n\n\nIn this course, we will use both R and Python\n\nR is dominant in applied econometrics (and thus is the basis of our department‚Äôs coding approach)\nOther disciplines (including machine learning) use R much less\nIn your career, you will likely have to learn new coding languages.\n\nLet‚Äôs become bilingual!\n\n\nI will lead the Python-specific component of the course\n\nWill walk you through installation, language basics etc., and then use it apply machine-learning models to big data"
  },
  {
    "objectID": "Introduction/introduction_what-is-big-data-and-why-does-it-matter.html#readings-free",
    "href": "Introduction/introduction_what-is-big-data-and-why-does-it-matter.html#readings-free",
    "title": "1¬† What is Big Data and Why Does it Matter",
    "section": "Readings (FREE!)",
    "text": "Readings (FREE!)"
  },
  {
    "objectID": "Introduction/introduction_what-is-big-data-and-why-does-it-matter.html#office-hours",
    "href": "Introduction/introduction_what-is-big-data-and-why-does-it-matter.html#office-hours",
    "title": "1¬† What is Big Data and Why Does it Matter",
    "section": "Office hours",
    "text": "Office hours\n\nJustin: Tuesdays 1:00-2:00 P.M. in person (classroom or office 337F). If virtual is needed, go to https://umn.zoom.us/my/jajohns"
  },
  {
    "objectID": "Introduction/introduction_what-is-big-data-and-why-does-it-matter.html#computers-in-class",
    "href": "Introduction/introduction_what-is-big-data-and-why-does-it-matter.html#computers-in-class",
    "title": "1¬† What is Big Data and Why Does it Matter",
    "section": "Computers in class",
    "text": "Computers in class\nFor Python, we will work through installation of the programming language and several supporting tools. This course strongly recommends that you bring your own laptop because having a well-setup laptop will serve you well into your academic career. If this is not possible, please contact the instructors within the first week of class and we can discuss alternatives.¬† It is possible to use a PC, Mac or Linux for this course, though all examples will be given on a PC.¬† Becoming skilled in Big Data is partly about mastering the tools and it will be your responsibility to come to class with your computer setup in a way for you to succeed. We will discuss any setup steps necessary in the lecture before it is to be used."
  },
  {
    "objectID": "Introduction/introduction_what-is-big-data-and-why-does-it-matter.html#big-data-means-many-things-to-different-groups",
    "href": "Introduction/introduction_what-is-big-data-and-why-does-it-matter.html#big-data-means-many-things-to-different-groups",
    "title": "1¬† What is Big Data and Why Does it Matter",
    "section": "Big data means many things to different groups",
    "text": "Big data means many things to different groups\n\nStandard definition: Data sets¬†that are so large or complex that traditional¬†data processing¬†applications are inadequate\n\nStreams of data (e.g., video collected by a self-driving car)\nMassive consumer data (they are watching you)\nRemotely sensed data (satellites or drones taking pictures of the earth)\nTraditional data, but just really big.\n\nMany related subfields also constitute ‚Äúwhat is‚Äù big data.\n\nMachine learning (core to this course)\nArtificial Intelligence (AI)\nTechnological advancement in computer science and hardware\nEconometrics exactly as we‚Äôve done before, but just with bigger tables"
  },
  {
    "objectID": "Introduction/introduction_what-is-big-data-and-why-does-it-matter.html#why-should-economists-care-about-big-data",
    "href": "Introduction/introduction_what-is-big-data-and-why-does-it-matter.html#why-should-economists-care-about-big-data",
    "title": "1¬† What is Big Data and Why Does it Matter",
    "section": "Why should economists care about ‚Äúbig data‚Äù?",
    "text": "Why should economists care about ‚Äúbig data‚Äù?\n\nToo many observations leads to 1000s of years of runtime.\nOLS: requires inverting a matrix. How do you invert ùëã^‚Ä≤ ùëã^(‚àí1) when ùëõ is super huge? What if it‚Äôs so big it can‚Äôt even fit in your computers‚Äô memory? Big data has solutions to this problem.\nBig data enables many new approaches, functional forms, and greater ability to predict.\n\n\nBut what‚Äôs the risk? We‚Äôll return to this.\n\n\nLots of new (and big) datasets aren‚Äôt well structured. What is ùë•_ùëñùëò for a tweet?\nThis is where the field is going."
  },
  {
    "objectID": "Introduction/introduction_what-is-big-data-and-why-does-it-matter.html#voice-analysis",
    "href": "Introduction/introduction_what-is-big-data-and-why-does-it-matter.html#voice-analysis",
    "title": "1¬† What is Big Data and Why Does it Matter",
    "section": "Voice Analysis",
    "text": "Voice Analysis\n\n\n\nFigure¬†5.1: Input: time-series of amplitude of different pitches\n\n\n\nText data MEANING. Via e.g., Natural Language Processing (NLP) methods"
  },
  {
    "objectID": "Introduction/introduction_what-is-big-data-and-why-does-it-matter.html#image-analysis",
    "href": "Introduction/introduction_what-is-big-data-and-why-does-it-matter.html#image-analysis",
    "title": "1¬† What is Big Data and Why Does it Matter",
    "section": "Image Analysis",
    "text": "Image Analysis\n\n\n\nFigure¬†5.2: Image Analysis\n\n\n\n\n\nFigure¬†5.3: Categorization"
  },
  {
    "objectID": "Introduction/introduction_what-is-big-data-and-why-does-it-matter.html#sentiment-analysis",
    "href": "Introduction/introduction_what-is-big-data-and-why-does-it-matter.html#sentiment-analysis",
    "title": "1¬† What is Big Data and Why Does it Matter",
    "section": "Sentiment Analysis",
    "text": "Sentiment Analysis"
  },
  {
    "objectID": "Introduction/introduction_what-is-big-data-and-why-does-it-matter.html#big-data-from-remote-sensing-satellites",
    "href": "Introduction/introduction_what-is-big-data-and-why-does-it-matter.html#big-data-from-remote-sensing-satellites",
    "title": "1¬† What is Big Data and Why Does it Matter",
    "section": "Big data from remote sensing (satellites)",
    "text": "Big data from remote sensing (satellites)\n\nHowMaps: Just a 2-dim matrix of values\n\n\n\nCreates terabytes of information per day\n\nCan assess economic factor like poverty\nOr environmental factors like freshwater availability\n\nData types:\n\nRaster data (matrices of spatial values)\nVector data (link databases of survey data to georeferenced household locations)"
  },
  {
    "objectID": "Introduction/introduction_what-is-big-data-and-why-does-it-matter.html#important-example-land-use-land-cover-lulc-maps-national-land-cover-database-nlcd",
    "href": "Introduction/introduction_what-is-big-data-and-why-does-it-matter.html#important-example-land-use-land-cover-lulc-maps-national-land-cover-database-nlcd",
    "title": "1¬† What is Big Data and Why Does it Matter",
    "section": "Important example: Land-use, Land Cover (LULC) maps-National Land Cover Database (NLCD)",
    "text": "Important example: Land-use, Land Cover (LULC) maps-National Land Cover Database (NLCD)\n\nWithout Zoom-inLet‚Äôs Zoom-inLet‚Äôs Zoom-in again\n\n\n\n\n\nFigure¬†5.4: ?(caption)\n\n\n\n\n\n\n\nFigure¬†5.5: ?(caption)\n\n\n\n\n\n\n\nFigure¬†5.6: Why do I care so much about LULC maps? Primary input to environmental economic models."
  },
  {
    "objectID": "Introduction/introduction_what-is-big-data-and-why-does-it-matter.html#lulc-zoomed-super-far-in",
    "href": "Introduction/introduction_what-is-big-data-and-why-does-it-matter.html#lulc-zoomed-super-far-in",
    "title": "1¬† What is Big Data and Why Does it Matter",
    "section": "LULC zoomed super-far in",
    "text": "LULC zoomed super-far in\n\n\n\nFigure¬†5.7: Literally just a 2-dimensional array (matrix) of integers"
  },
  {
    "objectID": "Introduction/introduction_what-is-big-data-and-why-does-it-matter.html#example-conservation-prioritization-using-basic-raster-math",
    "href": "Introduction/introduction_what-is-big-data-and-why-does-it-matter.html#example-conservation-prioritization-using-basic-raster-math",
    "title": "1¬† What is Big Data and Why Does it Matter",
    "section": "Example: Conservation prioritization using basic raster-math",
    "text": "Example: Conservation prioritization using basic raster-math\n\n\n\nFigure¬†6.1: NMB (higher values indicate higher conservation priority)\n\n\n\\[\nn m b_{x y}^f=\\frac{p_{x y}^f}{\\Delta \\pi\\left(h_{x y}^b\\right)-\\Delta \\pi\\left(h_{x y}^c\\right)}\n\\]\n\nNet marginal benefit of conservation per profit forgone\nOptimal price to pay for protection on the xth, yth grid-cell\nForgone profit from farming"
  },
  {
    "objectID": "Introduction/introduction_what-is-big-data-and-why-does-it-matter.html#example-spatial-regression-to-predict-land-use-change",
    "href": "Introduction/introduction_what-is-big-data-and-why-does-it-matter.html#example-spatial-regression-to-predict-land-use-change",
    "title": "1¬† What is Big Data and Why Does it Matter",
    "section": "Example: Spatial regression to predict land-use change",
    "text": "Example: Spatial regression to predict land-use change\n\n\nSuppose you want to predict where agricultural expansion will happen\nCould run regressions on biophysical parameters\nBut maybe also gridded socioeconomic data"
  },
  {
    "objectID": "Introduction/introduction_what-is-big-data-and-why-does-it-matter.html#example-given-different-scenarios-of-land-use-change-how-will-the-economy-be-affected-via-ecosystem-services",
    "href": "Introduction/introduction_what-is-big-data-and-why-does-it-matter.html#example-given-different-scenarios-of-land-use-change-how-will-the-economy-be-affected-via-ecosystem-services",
    "title": "1¬† What is Big Data and Why Does it Matter",
    "section": "Example: Given different scenarios of land-use change, how will the economy be affected (via ecosystem services)?",
    "text": "Example: Given different scenarios of land-use change, how will the economy be affected (via ecosystem services)?\n\n\nFor more ecosystem services: https://naturalcapitalproject.stanford.edu/"
  },
  {
    "objectID": "Introduction/introduction_what-is-big-data-and-why-does-it-matter.html#lots-of-data-same-old-econometrics",
    "href": "Introduction/introduction_what-is-big-data-and-why-does-it-matter.html#lots-of-data-same-old-econometrics",
    "title": "1¬† What is Big Data and Why Does it Matter",
    "section": "Lots of data, same old econometrics",
    "text": "Lots of data, same old econometrics\nWhen n is very large (or both n and k are)"
  },
  {
    "objectID": "Introduction/introduction_what-is-big-data-and-why-does-it-matter.html#new-prediction-approaches",
    "href": "Introduction/introduction_what-is-big-data-and-why-does-it-matter.html#new-prediction-approaches",
    "title": "1¬† What is Big Data and Why Does it Matter",
    "section": "New prediction approaches",
    "text": "New prediction approaches"
  },
  {
    "objectID": "Introduction/introduction_what-is-big-data-and-why-does-it-matter.html#so-weve-got-better-models-and-huge-data.-whats-the-risk",
    "href": "Introduction/introduction_what-is-big-data-and-why-does-it-matter.html#so-weve-got-better-models-and-huge-data.-whats-the-risk",
    "title": "1¬† What is Big Data and Why Does it Matter",
    "section": "So we‚Äôve got better models and huge data. What‚Äôs the risk?",
    "text": "So we‚Äôve got better models and huge data. What‚Äôs the risk?\n\nRequires rethinking what it means to be ‚Äògood‚Äô at prediction.\n\nIn econometrics, we often measure our prediction quality using in-sample analysis. For example, with R2.\nNormally we cheer when our p-values are tiny.\nWith big data, our p-values are (almost) ALWAYS tiny.\nIs this a good thing?\n\nIn the Python component of the course, we‚Äôre going to introduce a new metric of prediction quality.\n\nOut-of-sample prediction quality through cross-validation.\nHas been around forever of course, but big data greatly improves opportunities our ability to do cross-validation."
  },
  {
    "objectID": "Introduction/introduction_what-is-big-data-and-why-does-it-matter.html#big-data-improves-opportunities-for-cross-validation-well-learn-this-soon",
    "href": "Introduction/introduction_what-is-big-data-and-why-does-it-matter.html#big-data-improves-opportunities-for-cross-validation-well-learn-this-soon",
    "title": "1¬† What is Big Data and Why Does it Matter",
    "section": "Big Data improves opportunities for cross-validation (We‚Äôll learn this soon)",
    "text": "Big Data improves opportunities for cross-validation (We‚Äôll learn this soon)"
  },
  {
    "objectID": "Introduction/introduction_what-is-big-data-and-why-does-it-matter.html#model-complexity",
    "href": "Introduction/introduction_what-is-big-data-and-why-does-it-matter.html#model-complexity",
    "title": "1¬† What is Big Data and Why Does it Matter",
    "section": "Model complexity",
    "text": "Model complexity\n\nWith big data, you can make your model very, very complex\nWhat is the risk of this?\n\nPrediction error out of sample gets worse"
  },
  {
    "objectID": "Introduction/introduction_what-is-big-data-and-why-does-it-matter.html#another-term-overfitting-vs-underfitting",
    "href": "Introduction/introduction_what-is-big-data-and-why-does-it-matter.html#another-term-overfitting-vs-underfitting",
    "title": "1¬† What is Big Data and Why Does it Matter",
    "section": "Another term: overfitting vs underfitting",
    "text": "Another term: overfitting vs underfitting\n\nOverfitting a model: making the model overly complex to that accuracy falls on the test data.\nWe will talk about ways to methodologically hit the ‚Äúsweet spot‚Äù of model complexity."
  },
  {
    "objectID": "Introduction/introduction_what-is-big-data-and-why-does-it-matter.html#criticism-of-big-data",
    "href": "Introduction/introduction_what-is-big-data-and-why-does-it-matter.html#criticism-of-big-data",
    "title": "1¬† What is Big Data and Why Does it Matter",
    "section": "Criticism of Big Data",
    "text": "Criticism of Big Data\n\nPredicting the world with big data means we‚Äôre focused (obsessed?) by how the world was in the past (or at best, present)\n\nEmbeds racism, sexism, etc.\nEnables algorithmic discrimination\n\nBig data: not actually new ‚Äì just bigger."
  },
  {
    "objectID": "Introduction/introduction_what-is-big-data-and-why-does-it-matter.html#research-area-1-invest-and-the-natural-capital-project",
    "href": "Introduction/introduction_what-is-big-data-and-why-does-it-matter.html#research-area-1-invest-and-the-natural-capital-project",
    "title": "1¬† What is Big Data and Why Does it Matter",
    "section": "Research area 1: InVEST and The Natural Capital Project",
    "text": "Research area 1: InVEST and The Natural Capital Project\n\nPartnership of WWF, The Nature Conservancy, Stanford University, University of Minnesota and Chinese Academy of sciences\n\n\n\nInVEST is an open-source software tool to estimate 20+ ecosystem services\n\n\n\nSpatially-explicit, high-resolution, processed-based production functions, global extent\n\n\nInVEST Carbon Model is essentially a ‚Äúlookup table‚Äù\n\nTable values are heavily dependent on ‚ÄúEcofloristic Zone‚Äù"
  },
  {
    "objectID": "Introduction/introduction_what-is-big-data-and-why-does-it-matter.html#research-area-2-spatial-regression",
    "href": "Introduction/introduction_what-is-big-data-and-why-does-it-matter.html#research-area-2-spatial-regression",
    "title": "1¬† What is Big Data and Why Does it Matter",
    "section": "Research area 2: Spatial Regression",
    "text": "Research area 2: Spatial Regression\n\nAlgorithms used: Elastic Net and LASSO-LARS-CV\nDependent variable was 30-meter global aboveground biomass (~3 billion observations)\nIndependent variables included 40+ layers of soil, topographic, climate, socioeconomic and other variables\n\n\n\nDefinition of adjacency or ‚Äúneighborhood effect‚Äù\n\nGridded data preserve spatial structure\n\nNearby cells are highly correlated\nThe actual pattern may be a good predictor\n\n\n\n\n\nFigure¬†8.1: A. Example 1-dimensional adjacency relationships\n\n\n\nCan use 2-dimensional convolutions to express this structure\n\nIdentified different shapes of the adjacency relationship to predict carbon storage\n\n\n\n\n\nFigure¬†8.2: B. Expression in 2- and 3-dimensions\n\n\n\n\nLASSO-LARS-CV\n\\[\n\\begin{aligned}\n& \\min \\left(\\Sigma_i\\left(\\left(y_i-\\widehat{y}_i\\right)^2\\right)\\right. \\\\\n& \\text { subject to } \\Sigma_j\\left|\\beta_j\\right| \\leq s\n\\end{aligned}\n\\]\n\nLASSO (least absolute shrinkage and selection operator) is like ordinary leas squares but solved subject to a constraint on the absolute size of the coefficients\n\nCan tune the model via choosing s (or alpha) \nIn our approach, we solved for the optimal s via a least-angle regression selection approach (with out-of-sample cross validation) as in top figure\nUsed this method to find optimal set of parameters (bottom figure) \n\n\n\n\nResults\n\n\nGreatly outperforms the look-up table approach (IPCC method)\nEdge effects definitely exist\n\nBut still working to identify exact structure"
  },
  {
    "objectID": "Introduction/introduction_what-is-big-data-and-why-does-it-matter.html#research-area-3-connecting-general-equilibrium-models-to-ecosystem-service-models---gtap-invest",
    "href": "Introduction/introduction_what-is-big-data-and-why-does-it-matter.html#research-area-3-connecting-general-equilibrium-models-to-ecosystem-service-models---gtap-invest",
    "title": "1¬† What is Big Data and Why Does it Matter",
    "section": "Research area 3: Connecting general equilibrium models to ecosystem service models - GTAP-InVEST",
    "text": "Research area 3: Connecting general equilibrium models to ecosystem service models - GTAP-InVEST\n\nGrowing consensus that sustainability requires considering the entire bio-economic system\n\n\n\nTwo domains, types of models\n\n\n\nWe created a new model, GTAP-InVEST, that enables this by linking an economic model with an ecosystem services model\n\n\n\nThree big questions important for Earth-Economy modeling to answer.\n\nHow much does nature benefit the economy?\n\n\nMust be in measurable terms. Must account for whole-economy impacts.\n\n\nWho loses (or wins) when nature is depleted?\n\n\nMust assess fairness. Must consider economic activity hurt by conservation.\n\n\nHow can policy protect nature‚Äôs value and equity?\n\n\nWhat can we do?\n\n\n\nTwo domains, two different types of data\n\n\nThis is newly possible! We are now able to calculate high-resolution, global ecosystem services (Chaplin-Kramer et al.¬†2019, Science).\n \n\nGTAP-InVEST includes five (currently) global ecosystem services\n\n\n\n\n\n\n\n\n\n\n\n\nPollination\nCoastal protection\nWater yield\nCarbon storage\nMarine fisheries\n\n\n\n\n\n‚Üì\n‚Üì\n‚Üì\n‚Üì\n‚Üì\n\n\nInputs to GTAP\nLowered agricultural productivity\nReduced land-capital, productivity losses\nReduced water input to irrigated agriculture\nReduced extraction efficiency of forestry sector; Social cost of carbon\nLowered fisheries productivity\n\n\n\n\n\nWild pollinator ecosystem service provision\nModel estimates changes in pollinators in response to land use change brought about by policy scenarios. This spatially explicit input is used to estimate region-specific impacts.\n\n\n\n\nCarbon storage and timber ecosystem service provision\n\n\n\nMarine fisheries ecosystem service provision\n\n\n\nCoastal protection ecosystem service provision\n\n\n\nWater yield ecosystem service provision"
  },
  {
    "objectID": "Introduction/introduction_what-is-big-data-and-why-does-it-matter.html#research-area-4-predicting-land-use-change",
    "href": "Introduction/introduction_what-is-big-data-and-why-does-it-matter.html#research-area-4-predicting-land-use-change",
    "title": "1¬† What is Big Data and Why Does it Matter",
    "section": "Research Area 4: Predicting Land-use Change",
    "text": "Research Area 4: Predicting Land-use Change\nMissing Link between GTAP-AEZ results and ecosystem service models: downscaling\n\nCreated new model extends this downscaling:\n\nSEALS: Spatial economic allocation landscape simulator (Suh et al.¬†2020)\n\nKey advances:\n\nEmpirically calibrated with time-series data\nCan incorporate grid-cell level interventions (Riparian buffers)\n\n\n\n\n \n\nWhy is high-resolution important?\nThe exact, high-resolution location of where the agricultural expansion happens determines how much environmental impact occurs\n\n\nWhy is multi-scale modeling important?\nThe exact, high-resolution location of where the agricultural expansion happens determines how much environmental impact occurs\n\n\n\nFigure¬†8.3: Zoom-in GLUS"
  },
  {
    "objectID": "Python_Introduction/expanding-your-toolset.html",
    "href": "Python_Introduction/expanding-your-toolset.html",
    "title": "2¬† Expanding Your Toolset",
    "section": "",
    "text": "Agenda\nHopefully you have already created a new account on GitHub\nToday, we are going to use VS Code to get python code\nHopefully you‚Äôve already used the Extension tab to install the Python Extension\nThe full installation includes Anaconda Navigator, packed full of good tools\nProtip for context: This will launch a standard CMD prompt, but with Conda activated. You could do the same by launching a standard CMD.exe, using it to launch launching conda.exe, then initiating the environment with conda init. This is similar to what we did with the Miniforge prompt."
  },
  {
    "objectID": "Python_Introduction/expanding-your-toolset.html#overall-shift-in-the-academy-from-individual-geniuses-to-groups-of-people",
    "href": "Python_Introduction/expanding-your-toolset.html#overall-shift-in-the-academy-from-individual-geniuses-to-groups-of-people",
    "title": "2¬† Expanding Your Toolset",
    "section": "Overall shift in the academy from individual geniuses to groups of people",
    "text": "Overall shift in the academy from individual geniuses to groups of people\n\nLess evident in economics, but still evident.\nWhat‚Äôs the main challenge: groups of people are complex to manage.\nWriting a text document with many authors gets messy quick. With writing code it‚Äôs exponentially harder."
  },
  {
    "objectID": "Python_Introduction/expanding-your-toolset.html#two-example-that-have-managed-this-complexity-well-software-companies.",
    "href": "Python_Introduction/expanding-your-toolset.html#two-example-that-have-managed-this-complexity-well-software-companies.",
    "title": "2¬† Expanding Your Toolset",
    "section": "Two example that have managed this complexity well: software companies.",
    "text": "Two example that have managed this complexity well: software companies.\n\nIn the last decade there has been an explosion in tools for teams of 1000+ to all write code to the same codebase.\nExample: Microsoft\n\nA single code repository with 3.5M files being edited by 4000+ engineers\n\nOpen-source example: Tensorflow\n\nMachine-learning library with 1800+ git contributors\nHave created a massive public good.\n\n\n\nExample of a repository in GitHub: Scikit-learn\n\n\n\nMy shining beacon of hope: Open-Source Software\n\nDefinition:\n\n‚ÄúOpen-source software (OSS) is a type of computer software in which source code is released under a license in which the copyright holder grants users the rights to use, study, change, and distribute the software to anyone and for any purpose.‚Äù\nExample of OSS with InVEST from The Natural Capital Project\n\nR is another great example of OSS.\n\nIt‚Äôs not just a free, inferior version of STATA. It‚Äôs so much more.\nHowever, R is specific to statistics (though it can be used to do more), and interacting with a large, interdisciplinary research team can involve broader tasks.\n\nToday we‚Äôre going to discuss an ‚Äúopen-source scientific computing stack‚Äù that expands to a broader set of domains.\n\n\n\nWhy Python? Why an extra language?\n\nAlthough econometrics is still heavily centered on R, the Machine Learning community is dominantly focused on Python\n\nMostly because all of the key machine learning tools were developed in python: Scikit-Learn, PyTorch, TensorFlow, FastAI, etc.\n\nPython is quickly gaining in usage relative to R, even within econometrics\n\n\n\n\nFigure¬†4.1: Monthly active users on Github Source: https://towardsdatascience.com/data-science-101-is-python-better-than-r-b8f258f57b0f\n\n\n\n\nGeneral purpose programming language\n\nPython is a General-Purpose programming language beyond just statistics, meaning you could:\n\nWrite your own web server, create a desktop application, make a computer game to test consumer decision making, automate your coffee pot, email your partner automatically when you are stuck at work late making slides for the lecture the next day but have lost track of time, deal with hundreds of thousands of files automatically, launch zoom with an AI-imposter of yourself listening attentively‚Ä¶\n\nQuick example from economics that illustrates this: Agent-Based Simulation\nBut perhaps most importantly, ‚ÄúWhy not both?‚Äù\n\nYou can call R in Python and Python in R, leveraging the benefits of each.\nOur aim in this class is not to make you an expert in Python, but instead make it so you do not ‚Äúget stuck‚Äù when a challenging problem that needs Python arises.\n\n\n\nPython is very easy compared to other languages\n\nPython or RJavaC++ASSEMBLER X86(DOS, MASM)Hexadecimal representationsBinary representations\n\n\n\n\nCode\nprint(\"Hello world!\")\n\n\nHello world!\n\n\n\n\n\n\n\nFigure¬†4.2: Java HelloWorld\n\n\n\n\n\n\n\nFigure¬†4.3: C++ HelloWorld\n\n\n\n\n\n\n\nFigure¬†4.4: DOS HelloWorld\n\n\n\n\n\n\n\nFigure¬†4.5: Hexadecimal HelloWorld\n\n\n\n\n\n\n\nFigure¬†4.6: Binary HelloWorld\n\n\n\n\n\n\n\nA note on econometrics and causal inference\n\nBefore we dive into ML, where does econometrics fit into this landscape?\n\nWe are good at causal inference.\nFrom Varian 2014: ‚ÄúInstrumental variables, regression discontinuity, difference-in-differences, various forms of natural and designed experiments (Angrist and Krueger 2001).‚Äù\n\nMachine Learning is only beginning to explore causality."
  },
  {
    "objectID": "Python_Introduction/expanding-your-toolset.html#the-command-palate",
    "href": "Python_Introduction/expanding-your-toolset.html#the-command-palate",
    "title": "2¬† Expanding Your Toolset",
    "section": "The Command Palate",
    "text": "The Command Palate\n\nVS Code is easiest to control using the ‚ÄúCommand Palate‚Äù\n\nOpen it by pressing ctrl-shift-p (MAC: cmd-shift-p)\n\n\n\n\nIn this box, you can control nearly everything in VS Code\n\nFor instance, if you type the words ‚Äúlight theme‚Äù and select that option, you can change to the light theme like me."
  },
  {
    "objectID": "Python_Introduction/expanding-your-toolset.html#connect-vs-code-to-github",
    "href": "Python_Introduction/expanding-your-toolset.html#connect-vs-code-to-github",
    "title": "2¬† Expanding Your Toolset",
    "section": "Connect VS Code to Github",
    "text": "Connect VS Code to Github\n\nTo do this, we will ‚ÄúEnable Settings Sync‚Äù, which will prompt you to Sign in to GitHub.\nTo do this, open the command palate &lt;Ctrl-Shift-P&gt; and type ‚Äúsettings sync‚Äù\n\n\n\nSelect GitHub as the way to sign on\n\nThis will launch a browser window to authenticate your computer.\nAllow this to happen with ‚ÄúOpen Visual Studio Code‚Äù \n\nAfter login, GitHub will authenticate into VS Code\n\nAllow it.\nWe are now able to push and pull code, clone repositories, and contribute to OSS."
  },
  {
    "objectID": "Python_Introduction/expanding-your-toolset.html#before-we-use-git-lets-talk-about-file-structure",
    "href": "Python_Introduction/expanding-your-toolset.html#before-we-use-git-lets-talk-about-file-structure",
    "title": "2¬† Expanding Your Toolset",
    "section": "Before we use Git, let‚Äôs talk about file structure",
    "text": "Before we use Git, let‚Äôs talk about file structure\n\nMake sure you have all the files in google drive.\n\nThree folders: Assignments, Data and Readings (red)\n\nIn your class folder, make a new folder named ‚ÄúRepos‚Äù. This is where we will clone the course repository to.\n\nIf you have your directories arranged exactly this way, it will make ‚ÄúRelative Paths‚Äù work easily.\n\n\n\n\nGoogle drive link: https://drive.google.com/drive/u/1/folders/1oKDfEOqVLVdq93te-6qTegapdKB_dtq3"
  },
  {
    "objectID": "Python_Introduction/expanding-your-toolset.html#clone-the-course-repository",
    "href": "Python_Introduction/expanding-your-toolset.html#clone-the-course-repository",
    "title": "2¬† Expanding Your Toolset",
    "section": "Clone the course repository",
    "text": "Clone the course repository\n\nBack in VS Code, in the command palate type the word ‚Äúclone‚Äù (use crtl-shift-p)\n\nThis will give you a ‚ÄúClone from Github‚Äù option. Paste in the class repository link https://github.com/jandrewjohnson/apec_8222_2022_fall and hit enter.\n\n\nIt will ask you where you want to put the Repository.\n\nMake a new folder wherever you store your class content named ‚ÄúRepos‚Äù, like in the picture here.\n\nWhen you hit enter, Git will ‚Äúclone‚Äù (download) the repository, including its entire file history, to your computer.\n\n\nIt‚Äôs hard to explain how awesome Git and version control are, so I won‚Äôt explain it. Instead, I‚Äôll just say this is the bedrock of the open-source software revolution.\n\nWhen it finishes cloning, it‚Äôll ask you if you want to open the repository.\n\nSay yes!\n\n\nIf all of the above failed, you can at least get the code by manually downloading from github\n\nhttps://github.com/jandrewjohnson/apec_8222_2022_fall\nThere is a download Zipfile option in the green code button."
  },
  {
    "objectID": "Python_Introduction/expanding-your-toolset.html#what-did-we-do-with-mamba",
    "href": "Python_Introduction/expanding-your-toolset.html#what-did-we-do-with-mamba",
    "title": "2¬† Expanding Your Toolset",
    "section": "What did we do with Mamba?",
    "text": "What did we do with Mamba?\n\nThe mamba command downloaded the entire sum of human knowledge and placed it in this folder.\nWell‚Ä¶ not quite the entire sum, but a surprisingly large chunk of it.\n\nThis is where all the required Python packages were installed.\nYou could spend a career learning the details of each package, but for now, we‚Äôll just test that it works"
  },
  {
    "objectID": "Python_Introduction/expanding-your-toolset.html#select-our-interpreter",
    "href": "Python_Introduction/expanding-your-toolset.html#select-our-interpreter",
    "title": "2¬† Expanding Your Toolset",
    "section": "Select our interpreter",
    "text": "Select our interpreter\n\nEasiest way: Run the python file\n\nClick the Play triangle. \n\nThis will prompt you to Select a Python Interpreter.\n\nClick that, and then find the 8222env1 environment you created with Mamba  \n\n\n\n\nThere are multiple (potentially confusing) ways to set the interpreter.\n\nIf you have a python (.py) file open, you can click the bottom-right yellow bar \nIf you have a Jupyter notebook open, you can select it in the upper-right of the notebook \nIf nothing is open, you can use the command-palate (ctrl-shift-p) and search for ‚Äúpython select interpreter‚Äù"
  },
  {
    "objectID": "Python_Introduction/expanding-your-toolset.html#jupyter-notebooks",
    "href": "Python_Introduction/expanding-your-toolset.html#jupyter-notebooks",
    "title": "2¬† Expanding Your Toolset",
    "section": "Jupyter notebooks",
    "text": "Jupyter notebooks\n\nOpen up VS Code and use the explorer tab (on the left) to open 01_jupyter_intro.ipynb in the lecture_notebooks folder\nFor the rest of this lecture, we‚Äôll be using only VS Code"
  },
  {
    "objectID": "Python_Introduction/expanding-your-toolset.html#about-vs-workspaces-and-folders",
    "href": "Python_Introduction/expanding-your-toolset.html#about-vs-workspaces-and-folders",
    "title": "2¬† Expanding Your Toolset",
    "section": "About VS ‚ÄúWorkspaces‚Äù and ‚ÄúFolders‚Äù",
    "text": "About VS ‚ÄúWorkspaces‚Äù and ‚ÄúFolders‚Äù\n\nBy starting VS Code in a folder, that folder becomes your ‚Äúworkspace‚Äù. VS Code stores settings that are specific to that workspace in .vscode/settings.json, which are separate from user settings that are stored globally.\nI have created some versions of these settings files in the class repository for you to use.\nIf you set the class repository as your VS Code folder, it will automatically use these settings. Alternately, you can run VS Code through the operating system UI, then use File &gt; Open Folder to open the project folder"
  },
  {
    "objectID": "Python_Introduction/expanding-your-toolset.html#open-up-settings",
    "href": "Python_Introduction/expanding-your-toolset.html#open-up-settings",
    "title": "2¬† Expanding Your Toolset",
    "section": "Open up settings",
    "text": "Open up settings\n\nFile menu &gt; Preferences &gt; Settings\nOr &lt;CTRL-,&gt;"
  },
  {
    "objectID": "Python_Introduction/expanding-your-toolset.html#settings-i-like-to-play-with",
    "href": "Python_Introduction/expanding-your-toolset.html#settings-i-like-to-play-with",
    "title": "2¬† Expanding Your Toolset",
    "section": "Settings I like to play with",
    "text": "Settings I like to play with\nWhen I debug (we‚Äôll talk about this), I like to have the values of variables printed onto the screen as I step through my code.\n\nThat annoying lightbulb‚Ä¶\n\nGet rid of it in settings if it annoys you.\nHere I didn‚Äôt even disguise the fact that this is from StackOverflow.\n\nI refuse to devote memory to such things and have outsourced this type of storage to external sources.\nReal programmers Copy-Paste.\n\nThe lightbulb was an icon to the ‚ÄúQuick Fix‚Äù option for code mistakes. You can do this instead by putting your cursor in a word and typing &lt;ctrl-.&gt;"
  },
  {
    "objectID": "Python_Introduction/expanding-your-toolset.html#important-note-file-organization-matters",
    "href": "Python_Introduction/expanding-your-toolset.html#important-note-file-organization-matters",
    "title": "2¬† Expanding Your Toolset",
    "section": "Important note: File organization matters",
    "text": "Important note: File organization matters\nHere‚Äôs how I set things up.\n\n\nIt may be useful for you to set things up similarly, but it‚Äôs up to you.\nFirst off, if I‚Äôm working on a some arbitrary computer (or cloud service, or Docker image, or on my Steam Deck, etc), I choose to save everything in my ‚ÄúUser directory‚Äù. This helps it be similar across operating systems.\nBut I hate all the clutter in the User dir put here automatically by other programs, so I put everything I care about in a ‚ÄúFiles‚Äù directory."
  },
  {
    "objectID": "Python_Introduction/expanding-your-toolset.html#cloud-integration",
    "href": "Python_Introduction/expanding-your-toolset.html#cloud-integration",
    "title": "2¬† Expanding Your Toolset",
    "section": "Cloud integration",
    "text": "Cloud integration\n\n\nIf I‚Äôm on my own machine, I have an identically organized ‚ÄúFile‚Äù directory in my google drive.\nHere too it‚Äôs always full of clutter automatically put there by others.\nThis setup lets you work seamlessly across many computers\n\n\nHowever you choose to set up your folders, please configure your directory for this course as follows\n\n\nYou can download the default configuration of this in the courses‚Äô google drive page.\nImportant notes:\n\nWe will use Git to put our code in the Repos directory.\nThe Data directory will have data used in assignments and such.\nIf you have exactly this file setup, it will be easy to create correct ‚Äúrelative paths‚Äù later on."
  },
  {
    "objectID": "Python_Introduction/expanding-your-toolset.html#here-is-where-mamba-installed-everything",
    "href": "Python_Introduction/expanding-your-toolset.html#here-is-where-mamba-installed-everything",
    "title": "2¬† Expanding Your Toolset",
    "section": "Here is where Mamba installed everything",
    "text": "Here is where Mamba installed everything\nLook we‚Äôve got Python!\n\nAnd here‚Äôs our new environment!\n\nWhen we selected the interpreter in VS Code, all we really did it point it to that folder"
  },
  {
    "objectID": "Python_Introduction/expanding-your-toolset.html#optional-path-step",
    "href": "Python_Introduction/expanding-your-toolset.html#optional-path-step",
    "title": "2¬† Expanding Your Toolset",
    "section": "Optional PATH step",
    "text": "Optional PATH step\n\nIf you didn‚Äôt select to ‚ÄúAdd Mambaforge to my PATH environment Variable‚Äù, you can set it with the following (WINDOWS ONLY) command.‚Äô\nWhere you replace User with your own user directory.\n\nSETX PATH \"%PATH%;C:\\Users\\User\\mambaforge;C:\\Users\\User\\mambaforge;\""
  },
  {
    "objectID": "Python_Introduction/expanding-your-toolset.html#git-details",
    "href": "Python_Introduction/expanding-your-toolset.html#git-details",
    "title": "2¬† Expanding Your Toolset",
    "section": "Git Details",
    "text": "Git Details"
  },
  {
    "objectID": "Python_Introduction/expanding-your-toolset.html#creating-new-repos",
    "href": "Python_Introduction/expanding-your-toolset.html#creating-new-repos",
    "title": "2¬† Expanding Your Toolset",
    "section": "Creating new repos",
    "text": "Creating new repos\n\n\nVSCode has the ‚ÄúPublish to Github‚Äù option to create a repository on Github when there is no existing git repository.\nCreate your project folder locally and add your README.md file."
  },
  {
    "objectID": "Python_Introduction/expanding-your-toolset.html#git-from-the-ground-up-using-the-command-line",
    "href": "Python_Introduction/expanding-your-toolset.html#git-from-the-ground-up-using-the-command-line",
    "title": "2¬† Expanding Your Toolset",
    "section": "Git from the ground up (using the command line)",
    "text": "Git from the ground up (using the command line)\n\n\n\n\n\nFigure¬†10.1: Git Folder with your Code\n\n\n\n\nInitial state:\n\nIf your hard drive dies, you‚Äôre in major trouble\nPerhaps you‚Äôve backed up your files, but how long ago was that?"
  },
  {
    "objectID": "Python_Introduction/expanding-your-toolset.html#git-from-the-ground-up",
    "href": "Python_Introduction/expanding-your-toolset.html#git-from-the-ground-up",
    "title": "2¬† Expanding Your Toolset",
    "section": "Git from the ground up",
    "text": "Git from the ground up\n\n\n\n\n\nFigure¬†10.2: Git from the ground up (Step 1)\n\n\n\n\nStep 1: Initialize your folder as a Git Repository\n\nAt this point, your files are now ‚Äúversion controlled‚Äù and have a full history saved, but only for you and only on your local machine (still quite useful)\n\nAt this point, you might go ahead and make a new file.\n\n\n\n\n\n\n\n\n\nFigure¬†10.3: Git from the ground up (Step 2)\n\n\n\n\nStep 2: Add files to the repo\n\nFirst ‚Äúadd‚Äù files to the repository\nThen ‚Äúcommit‚Äù them to the repository\n\nCommand:\n\ngit add new_file.py\ngit commit ‚Äìm ‚ÄúI added the initial files‚Äù\n\n\n\n\n\n\n\n\n\n\nFigure¬†10.4: Git from the ground up (Step 3)\n\n\n\n\nStep 3: Create a ‚Äúremote‚Äù repository\n\nOn github.com, create a new repository\n\n\n\n\n\n\n\nFigure¬†10.5: GitHub Repository Demo\n\n\n\n\n\n\n\n\nFigure¬†10.6: Git from the ground up (Step 4)\n\n\n\n\nStep 4: Connect your local repository to the remote\nCommand:\n\ngit remote add origin https://github.com/user/repo.git\n\nOrigin is the standard name that Git gives to your primary remote repository.\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†10.7: Git from the ground up (Step 5)\n\n\n\n\nStep 5: Push your changed local code to the remote.\nCommand:\n\ngit push\n\n\n\n\n\n\n\n\n\n\nFigure¬†10.8: Git from the ground up (Step 6)\n\n\n\n\nStep 6: Add new contributor via GitHub.com\n\nTell them to clone your repo\n\nCommand:\n\ngit clone https://github.com/user/repo.git\n\n\n\n\n\n\n\n\n\n\nFigure¬†10.9: Git from the ground up (Step 7)\n\n\n\n\nStep 7: Suppose they change a file.\n\nThey would then add, commit and push the new file.\nBut now your files are different!\n\n\n\n\n\n\n\n\n\n\nFigure¬†10.10: Git from the ground up (Step 8)\n\n\n\n\nStep 8: BEFORE YOU START CODING, you must pull their changes.\n\nIf you didn‚Äôt change the file, you‚Äôre all good.\nIf you did, you will have to ‚Äúmerge‚Äù the files, choosing line-by-line which one to accept.\n\nCommand:\n\ngit pull"
  },
  {
    "objectID": "Python_Introduction/expanding-your-toolset.html#using-git-from-the-command-line",
    "href": "Python_Introduction/expanding-your-toolset.html#using-git-from-the-command-line",
    "title": "2¬† Expanding Your Toolset",
    "section": "Using Git from the command line",
    "text": "Using Git from the command line\n\nNavigate to the folder‚Äôs repository, right click on the directory, select Git Bash Here\n\n\n\nFor a first command, just type ‚Äúgit‚Äù\nNow you‚Äôre ready to implement the commands above. For a reminder, they were:\n\n\n\nCode\ngit init\ngit add new_file.py\ngit commit ‚Äìm ‚ÄúI added the initial files‚Äù\ngit remote add origin https://github.com/user/repo.git\ngit push\ngit pull"
  },
  {
    "objectID": "Python_Introduction/expanding-your-toolset.html#optional-version-control-graphical-user-interface-installation",
    "href": "Python_Introduction/expanding-your-toolset.html#optional-version-control-graphical-user-interface-installation",
    "title": "2¬† Expanding Your Toolset",
    "section": "Optional: Version Control Graphical User Interface Installation",
    "text": "Optional: Version Control Graphical User Interface Installation\n\nDownload GitHub Desktop from desktop.github.com\nWhen you are installing this, it will prompt you to sign in to GitHub or create a new account. This is a VERY important account. If you do not have an account, create one, though I suggest thinking carefully about the username/email (because if you‚Äôre like me, inertia and network effects guarantee this will be your GitHub name for decades)"
  },
  {
    "objectID": "Python_Introduction/expanding-your-toolset.html#breakout-group-activity",
    "href": "Python_Introduction/expanding-your-toolset.html#breakout-group-activity",
    "title": "2¬† Expanding Your Toolset",
    "section": "BREAKOUT GROUP ACTIVITY",
    "text": "BREAKOUT GROUP ACTIVITY\n\nGoal: have 1 person create a new repository on github and have everybody clone the repository to their local computer\nBonus goal, if you finish the above, have someone make a change, push it, then have everyone pull it."
  },
  {
    "objectID": "Python_Introduction/expanding-your-toolset.html#setting-the-interpreter-with-pycharms",
    "href": "Python_Introduction/expanding-your-toolset.html#setting-the-interpreter-with-pycharms",
    "title": "2¬† Expanding Your Toolset",
    "section": "Setting the interpreter with PyCharms",
    "text": "Setting the interpreter with PyCharms\n\nPycharms has you set your project root directory\nSet this to the class repository \nThis will create a new Python installation specific (may take a while to create all the files)"
  },
  {
    "objectID": "Python_Introduction/expanding-your-toolset.html#create-a-new-environment-using-conda",
    "href": "Python_Introduction/expanding-your-toolset.html#create-a-new-environment-using-conda",
    "title": "2¬† Expanding Your Toolset",
    "section": "Create a new environment using Conda",
    "text": "Create a new environment using Conda\n\n\nIf you haven‚Äôt created a project before, this box will pop up where you can select New Environment Using Conda\nHopefully it will find where you installed Anaconda automatically, but if not, you will have to put this in.\n\n\nNow we have working IDE capable of software development\n\nNavigate to your 03_01_hello_world.py\nIn the run menu, select Run, or Ctrl-Shift-F10.\nThe new window below is a command prompt calling python on your script file."
  },
  {
    "objectID": "Python_Introduction/expanding-your-toolset.html#installing-packages",
    "href": "Python_Introduction/expanding-your-toolset.html#installing-packages",
    "title": "2¬† Expanding Your Toolset",
    "section": "Installing Packages",
    "text": "Installing Packages\n\nYou can see your new environment in Anaconda Navigator\nSelect the environment you created in your Project directory\nSelect it, then click channels and add ‚Äúconda-forge‚Äù\nInstall Numpy and then Matplotlib by searching for them (be sure to select uninstalled when you‚Äôre searching)"
  },
  {
    "objectID": "Python_Introduction/expanding-your-toolset.html#running-python-on-the-command-line",
    "href": "Python_Introduction/expanding-your-toolset.html#running-python-on-the-command-line",
    "title": "2¬† Expanding Your Toolset",
    "section": "Running Python on the command line",
    "text": "Running Python on the command line\n\nAfter you‚Äôve launched theCMD prompt from Anaconda Navigator, just type python. You‚Äôll now have a &gt;&gt;&gt; on the command line, indicating you are entering python code\nNow we have basic calculator functionality. You can get back to a normal CMD prompt using the python function quit()"
  },
  {
    "objectID": "Python_Introduction/expanding-your-toolset.html#spatial-data-viewer-installation",
    "href": "Python_Introduction/expanding-your-toolset.html#spatial-data-viewer-installation",
    "title": "2¬† Expanding Your Toolset",
    "section": "Spatial Data Viewer Installation",
    "text": "Spatial Data Viewer Installation\n\nInstall QGIS from: https://qgis.org/en/site/forusers/download.html\n\nNOTE: I recommend getting QGIS Standalone Installer 64bit\n\nFun tidbit: QGIS used to be an inferior free version of ArcGIS but now challenges (and in my opinion surpasses) it in most key metrics."
  },
  {
    "objectID": "Python_Introduction/expanding-your-toolset.html#colab",
    "href": "Python_Introduction/expanding-your-toolset.html#colab",
    "title": "2¬† Expanding Your Toolset",
    "section": "Colab?",
    "text": "Colab?\n\nhttps://colab.research.google.com/\nGuide\n\nhttps://web.eecs.umich.edu/~justincj/teaching/eecs442/WI2021/colab.html"
  },
  {
    "objectID": "Python_Introduction/expanding-your-toolset.html#popularity-matters",
    "href": "Python_Introduction/expanding-your-toolset.html#popularity-matters",
    "title": "2¬† Expanding Your Toolset",
    "section": "Popularity matters",
    "text": "Popularity matters\n\n\nIDE usage over time: VS Code is winning.\n\n\n\nLanguage usage over time: Python is winning."
  },
  {
    "objectID": "Python_Introduction/expanding-your-toolset.html#the-original-tools-the-command-line",
    "href": "Python_Introduction/expanding-your-toolset.html#the-original-tools-the-command-line",
    "title": "2¬† Expanding Your Toolset",
    "section": "The original tool(s): The Command Line",
    "text": "The original tool(s): The Command Line"
  },
  {
    "objectID": "Python_Introduction/expanding-your-toolset.html#different-ways-of-using-python",
    "href": "Python_Introduction/expanding-your-toolset.html#different-ways-of-using-python",
    "title": "2¬† Expanding Your Toolset",
    "section": "Different ways of using Python",
    "text": "Different ways of using Python\n\nType Python code directly in the command line (what we just did)\nUse the command line to tell Python to run a .py file Common stumbling block: you have to make sure your current working directory in folder where your code is In the Command Prompt pictured, I ran a .py file and it outputted straight back\n\n\nRun in a local Integrated Development Environment\n\nRun on a remote computer you control (via SSH or some other protocol)\nRun a cloud compute server (this assumes you could do the Mambaforge steps, or something similar, on the cloud Amazon AWS, Google Cloud, and Microsoft Azure are the three big ones\nRun in a ‚Äúcontainerized‚Äù version using Docker, Kubernetes, etc.\nLaunch a python script on a high-performance computing (HPC) cluster Sometimes, they will even give you the ability to run a jupyter notebook!\nBrowser/Git-based IDE envioronments (Google Colab)"
  },
  {
    "objectID": "Python_Introduction/jupyter_intro.html#jupyter-has-two-kinds-of-cells",
    "href": "Python_Introduction/jupyter_intro.html#jupyter-has-two-kinds-of-cells",
    "title": "4¬† Jupyter Intro",
    "section": "4.1 Jupyter has two kinds of cells",
    "text": "4.1 Jupyter has two kinds of cells\n\nMarkdown cells (like this one)\nCode cells (like the next one)\n\nAbove the editor window here, you can click +Code or +Markdown to add new ones.\nAlternatively, right-click on a cell for more options (like splitting a cell into two)\nMarkdown cells are meant for formatted content, pictures, lecture notes etc. and follows the same notation as R-markdown.\nIf you want to edit a markdown cell, double left-click it. To finalize the edits, click the checkmark above (or type ctrl-enter)."
  },
  {
    "objectID": "Python_Introduction/jupyter_intro.html#code-runs-in-the-notebook.",
    "href": "Python_Introduction/jupyter_intro.html#code-runs-in-the-notebook.",
    "title": "4¬† Jupyter Intro",
    "section": "4.2 Code runs IN the notebook.",
    "text": "4.2 Code runs IN the notebook.\nSelect the python cell below. You can edit it freely. To run it, you can click the triangle button (‚Äúplay button‚Äù) to the upper right of that cell. Alternatively, you can ctrl-enter.\n\n\nCode\na = 5\nb = 4\nsimple_summation = a + b\n\n\nYou know the cell has run successfully if it gets a green check at the bottom. Also notice that there is a number now in the [ ] box at the bottom left. This indicates which cell this was run in order of all the cells run.\nNotice that it doesn‚Äôt output anything, but note that the values are now stored in the Python Kernel (Jupyter Server) and are available to other parts of this notebook.\nIf you want to see a variable outputted, you can just type the variable name.\n\n\nCode\nsimple_summation\n\n\n9\n\n\nYou can also use the print command, but this will supress non-printed variables.\n\n\nCode\nprint('Rounded: ', simple_summation)\n\n\nRounded:  9"
  },
  {
    "objectID": "Python_Introduction/jupyter_intro.html#order-matters",
    "href": "Python_Introduction/jupyter_intro.html#order-matters",
    "title": "4¬† Jupyter Intro",
    "section": "4.3 Order matters",
    "text": "4.3 Order matters\nThe second python cell would fail if the first one wasn‚Äôt run.\nYou can run the whole program via the double-play Run icon above."
  },
  {
    "objectID": "Python_Introduction/jupyter_intro.html#in-class-exercise-1.1",
    "href": "Python_Introduction/jupyter_intro.html#in-class-exercise-1.1",
    "title": "4¬† Jupyter Intro",
    "section": "4.4 In-class exercise 1.1",
    "text": "4.4 In-class exercise 1.1\nBelow, add two cells to this notebook.\nFirst, create a markdown cell where you have a header and some paragraph text. To make something a header in Markdown language, just use a hashtag and a space before the title. To make it a paragraph, just separate it with a blank line in between. Finally, add a bulletted list with a few entries. To do this, just have a dash at the beginning of each new bullet.\nSecond, create a python cell. Save a variable that is the sum of all primes between 3 and 10. Print that sum."
  },
  {
    "objectID": "Python_Introduction/python_assignment_0.html#optional-more",
    "href": "Python_Introduction/python_assignment_0.html#optional-more",
    "title": "4¬† Exercise 1",
    "section": "4.1 Optional More",
    "text": "4.1 Optional More\nIf you‚Äôre looking for more, here is one extra task: Run this notebook in VS Code! We‚Äôll talk more about VS Code in the next class, so don‚Äôt worry if this doesn‚Äôt make sense. If you do want to run this, open it and then select ‚ÄúRun All‚Äù at the top of the editor. It will automatically output how well you did (worth no class credit).\n\n\nCode\nimport numpy as np\nimport pandas as pd\nimport sklearn\nfrom osgeo import gdal\nimport hazelbean as hb\n\ndef autograde():\n    \n    if len(str(hb.__name__)) == 9:\n        return '100%'\n    else:\n        return \"0%. Keep trying! Feel free to email me in advance of the first Python lecture. Don't save this for teh last day!\"\n\nprint('Hazelbean library imported successfully:', hb)\n\nprint('Autograding output:', autograde())\n\n\nHazelbean library imported successfully: &lt;module 'hazelbean' from 'c:\\\\Users\\\\jajohns\\\\AppData\\\\Local\\\\mambaforge\\\\envs\\\\8222env1\\\\lib\\\\site-packages\\\\hazelbean\\\\__init__.py'&gt;\nAutograding output: 100%"
  },
  {
    "objectID": "Python_Introduction/python_basics.html#python-basics",
    "href": "Python_Introduction/python_basics.html#python-basics",
    "title": "5¬† Python Basics",
    "section": "5.1 Python Basics",
    "text": "5.1 Python Basics\n\n\nCode\n# Comments: The hashtag makes the rest of the line a comment. The more programming you do, the more you focus on making good comments.\n# Jupyter lets you write formatted text, but you'll still want to put comments in the raw python.\n\n# Assign some text (a string) to a variable\nsome_text = 'This is the text.'\n\n# Assign some numbers to variables\na = 5  # Here, we implicitly told python that a is an integer\nb = 4.6  # Here, we told python that b is a floating point number (a decimal)\n\n\n\nEven though nothing is outputted above, our Python ‚ÄúKernel‚Äù has the values to each variable stored for later use.\n\n\n5.1.1 Important note: Python is not a ‚Äútyped‚Äù language\n\nNotice that above, we added an integer and the float (a floating point number, i.e., one with a decimal point). Python ‚Äúsmartly‚Äù redefines variables so that they work together.\nThis is different from other languages which require you to manually manage the ‚Äútypes‚Äù of your variables.\n\n\n\nCode\n# Python as a calculator. \nsum_of_two_numbers = a + b\n\n# Printing output to the console\nprint('Our output was', sum_of_two_numbers)\n\n\nOur output was 9.6\n\n\n\nIn the above, you‚Äôll notice the result was a float.\nIf needed, you can demand that python specify something as a certain type, as below.\n\n\n\nCode\nsum_as_int = int(sum_of_two_numbers)\nsum_as_int_back_to_float = float(sum_as_int)\n\nprint('We lost some precision in this operation:', sum_as_int_back_to_float)\n\n\nWe lost some precision in this operation: 9.0"
  },
  {
    "objectID": "Python_Introduction/python_basics.html#other-python-types",
    "href": "Python_Introduction/python_basics.html#other-python-types",
    "title": "5¬† Python Basics",
    "section": "5.2 Other python types",
    "text": "5.2 Other python types\n\n\nCode\n# Reminder, this assumes you have setup an envioronment with conda using:\nlist_1 = [4, 5, 6]\nprint('list_1', list_1)\n\n\nlist_1 [4, 5, 6]\n\n\n\n\nCode\n# You can embed lists in lists in lists, etc.\nlist_2 = [[5, 3, 5], [6, 6, 5]]\nprint(list_2)\n\n\n[[5, 3, 5], [6, 6, 5]]\n\n\n\n\nCode\n# Dictionaries\ndictionary_1 = {23: \"Favorite number\", 24: \"Second favorite number\"}\nprint('dictionary_1', dictionary_1)\n\n# Here is a multi line string: (also discusses improved capabilities of an IDE editor)\n\nthings_you_can_do_in_vs_code_that_you_cant_do_without_an_ide = \"\"\"\n1.) Move back and forth in your history of cursor positions (using your mouse forward and back buttons)\n2.) Edit on multiple lines at the same time (hold alt and click new spots)\n3.) Smartly paste DIFFERENT values\n4.) Duplicate lines (ctrl-d)\n5.) Introspection (e.g., jump between function definition and usages)\n6.) Debugging (Interactively walk through your code one line at a time)\n7.) Profiling your code (see which lines take the most time to compute.)\n8.) Keep track of a history of copy-paste items and paste from past copies. (ctrl-shift-v)\n\"\"\"\n\n\ndictionary_1 {23: 'Favorite number', 24: 'Second favorite number'}"
  },
  {
    "objectID": "Python_Introduction/python_basics.html#looping",
    "href": "Python_Introduction/python_basics.html#looping",
    "title": "5¬† Python Basics",
    "section": "5.3 Looping",
    "text": "5.3 Looping\n\n\nCode\n\n\nsmall_range = range(0, 10)\nprint('small_range:', small_range)\n\nsmall_range_as_list = list(range(0, 10))\nprint('small_range_as_list:', small_range_as_list)\n\n# Here is a for loop. Also note that python EXPLICITLY USES TAB-LEVEL to denote nested things.\n# I.e., the inner part of the loop is tabbed 1 level up. Python does not use { like  R.\n# I LOVE this notation and it's a big part of why python is so pretty and readable.\nsum = 0 # Set the initial variable values\nnum = 0\nsum_with_some = 0\nfor i in range(100, 136, 3):\n    sum = sum + i\n    num = num + 1\n\n    # loop within a loop\n    for j in range(200, 205):\n        sum_with_some = sum + j\n\nmean = sum / num\nprint('mean', mean)\n\n\nsmall_range: range(0, 10)\nsmall_range_as_list: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\nmean 116.5"
  },
  {
    "objectID": "Python_Introduction/python_basics.html#defining-functions",
    "href": "Python_Introduction/python_basics.html#defining-functions",
    "title": "5¬† Python Basics",
    "section": "5.4 Defining functions",
    "text": "5.4 Defining functions\n\n\nCode\n# Functions\ndef my_function(input_parameter_1, input_parameter_2):\n    product = input_parameter_1 * input_parameter_2\n    return product\n\n# Use the function\nvalue_returned = my_function(2, 7)\nprint(value_returned)\n\n\n14\n\n\n\n\nCode\n# In-class exercise workspace"
  },
  {
    "objectID": "Python_Introduction/python_basics.html#importing-packages",
    "href": "Python_Introduction/python_basics.html#importing-packages",
    "title": "5¬† Python Basics",
    "section": "5.5 Importing packages",
    "text": "5.5 Importing packages\n\n\nCode\n\n# Built-in packages via the Python Standard Library\nimport math\nimport os, sys, time, random\n\n# Using imported modules\nnumber_rounded_down = math.floor(sum_of_two_numbers)\nprint(number_rounded_down)\n\n\n9"
  },
  {
    "objectID": "Python_Introduction/python_basics.html#using-packages-from-elsewhere",
    "href": "Python_Introduction/python_basics.html#using-packages-from-elsewhere",
    "title": "5¬† Python Basics",
    "section": "5.6 Using packages from elsewhere",
    "text": "5.6 Using packages from elsewhere\nWhen we used Mambaforge, we installed a ton of packages. These were not ‚Äúbuilt-in‚Äù to python like the ones above. Here we will import them into our notebook to use.\nThis will also illustrate the use of numpy. We‚Äôll use it so much we us the as code to name it something shorter.\n\n\nCode\nimport numpy as np # The as just defines a shorter name\n\n# Create an 2 by 3 array of integers\nsmall_array = np.array([[5, 3, 5], [6, 6, 5]])\n\nprint('Here\\'s a small numpy array\\n', small_array)\n\n# Sidenote: from above backspace \\ put in front of a character is the\n# \"escapce character,\" which makes python interpret the next thing as a string or special text operator. \\n makes a line break\n\n\nHere's a small numpy array\n [[5 3 5]\n [6 6 5]]"
  },
  {
    "objectID": "Python_Introduction/python_basics.html#discussion-point",
    "href": "Python_Introduction/python_basics.html#discussion-point",
    "title": "5¬† Python Basics",
    "section": "5.7 Discussion point",
    "text": "5.7 Discussion point\nThe array above looks identical to the nested lists we made. It IS NOT! It is a numpy array that is ridiculously fast and can scale up to massive, massive data questions. The optional reading for today (Harris et al.¬†2020, Nature) discusses how these arrays have formed the backbone of modern scientific computing.\n\n\nCode\nlow = 3\nhigh = 8\nshape = (1000, 1000)\n\nsmallish_random_array = np.random.randint(low, high, shape)\n\nprint('Here\\'s a slightly larger numpy array\\n', smallish_random_array)\n\n\nHere's a slightly larger numpy array\n [[4 7 3 ... 3 6 6]\n [7 6 4 ... 4 7 7]\n [5 6 3 ... 5 6 3]\n ...\n [4 3 7 ... 4 3 3]\n [7 4 4 ... 3 6 6]\n [4 5 4 ... 4 3 5]]"
  },
  {
    "objectID": "Python_Introduction/python_basics.html#in-class-exercise-2.1",
    "href": "Python_Introduction/python_basics.html#in-class-exercise-2.1",
    "title": "5¬† Python Basics",
    "section": "5.8 In-class exercise 2.1",
    "text": "5.8 In-class exercise 2.1\nParticipation points note! I will call on a random table to show me their answer via their table‚Äôs monitor.\nMake a function that returns the square of a number. Combine the function with a loop to calculate the Sum of Squared Numbers from 1 to 100.\nHINT, ** is the exponent operator in python.\nBONUS: Make sure you‚Äôre actually right by inserting a print statement in each step.\nBONUS-bonus: Store each stage of the results in a list using your_list = [] and your_list.append(thing_to_add_to_your_list)"
  },
  {
    "objectID": "Python_Introduction/numpy_arrays.html#arrays-as-objects",
    "href": "Python_Introduction/numpy_arrays.html#arrays-as-objects",
    "title": "6¬† Numpy Arrays",
    "section": "6.1 Arrays as objects",
    "text": "6.1 Arrays as objects\nThe a variable we defined holds much more information than jsut the raw values. It also gives us useful information necessary for working with really big data.\n\n\nCode\n\n# a is an OBJECT, which has lots of useful attributes, such as:\narray_shape = a.shape\nprint(array_shape)\nprint(a.ndim)\nprint(a.dtype.name)\nprint(a.size)\nprint(a.itemsize) #8 Pro-level question. Why does this return 8? Hint 8 * 8 = 64.\nprint(type(a)) \n\n\n\n(3, 5)\n2\nint32\n15\n4\n&lt;class 'numpy.ndarray'&gt;"
  },
  {
    "objectID": "Python_Introduction/numpy_arrays.html#creating-an-array-from-values",
    "href": "Python_Introduction/numpy_arrays.html#creating-an-array-from-values",
    "title": "6¬† Numpy Arrays",
    "section": "6.2 Creating an array from values",
    "text": "6.2 Creating an array from values\n\n\nCode\n\na = np.array([1,2,3,4])  # RIGHT\n# a = np.array(1,2,3,4)    # WRONG: TypeError: array() takes from 1 to 2 positional arguments but 4 were given. Uncomment this to see what happens with error handling.\n\n# 2d version\nb = np.array([(1.5,2,3), (4,5,6)])\n\nprint('b\\n', b)\n\n\nb\n [[1.5 2.  3. ]\n [4.  5.  6. ]]\n\n\n\n\nCode\n\n# Creating an empty array of zeros # NOTICE the extra paranetheses.\nnp.zeros((3, 4))\n\n# or ones.\nnp.ones((2, 3), dtype=np.int16)                # dtype can also be specified\n\n# or ones.\nr = np.random.random((3, 4))                # dtype can also be specified\n# print('r', r)\n\n# Or even faster, just \"allocate the memory\" with an empty matrix.\nc = np.empty((2,3))"
  },
  {
    "objectID": "Python_Introduction/numpy_arrays.html#array-math",
    "href": "Python_Introduction/numpy_arrays.html#array-math",
    "title": "6¬† Numpy Arrays",
    "section": "6.3 Array math",
    "text": "6.3 Array math\nNumpy is super smart about doing matrix math across multiple dimensions. Note how in the below, it correctly guesses we wanted to add things element-wise.\n\n\nCode\n# Array math\na = np.array([20, 30, 40, 50.])\nb = np.arange(4)\n\nc = a-b\n\nprint('a', a)\nprint('b', b)\nprint('c', c)\n\n\na [20. 30. 40. 50.]\nb [0 1 2 3]\nc [20. 29. 38. 47.]\n\n\n\n\nCode\n\n# ** is the exponent operator in python\nd = b**2\nprint('d', d)\n\n# Numpy also has handy array-enabled math operators\ne = 10*np.sin(a)\nprint('e', e)\n\n# Con also create conditional arrays\nf = a&lt;35\nprint('f', f)\n\n\n\nd [0 1 4 9]\ne [ 9.12945251 -9.88031624  7.4511316  -2.62374854]\nf [ True  True False False]"
  },
  {
    "objectID": "Python_Introduction/numpy_arrays.html#slicing-arrays",
    "href": "Python_Introduction/numpy_arrays.html#slicing-arrays",
    "title": "6¬† Numpy Arrays",
    "section": "6.4 Slicing Arrays",
    "text": "6.4 Slicing Arrays\nSometimes you want to operate on a subset of an array. Slicing provies a high-performance way of doing this.\n\n\nCode\n\na = np.arange(10)\nb = np.arange(12).reshape(3, 4)\nprint(a)\nprint(b)\n\n# Can access items directly, but need as many indices as there are dimensions\nfirst_value = a[2]\nsecond_value = b[2, 3]\nprint(first_value)\nprint(second_value)\n\n\n[0 1 2 3 4 5 6 7 8 9]\n[[ 0  1  2  3]\n [ 4  5  6  7]\n [ 8  9 10 11]]\n2\n11\n\n\n\n\nCode\n# Can also access \"slices\", which are denoted Start, Stop, Stepsize\nr = a[1: 9: 2]\nprint(r)\n\n\n[1 3 5 7]\n\n\n\n\nCode\n# If you leave out the number and just have the colon, that means you want to use the default. So below, the\n# ::2 is interpretted as Start:End:Every-Other.\n# 3:: would be Start at the third:End:All.\n# :: would just be all the values.\nr = a[::2]\nprint(r)\n\n\n[0 2 4 6 8]\n\n\n\n\nCode\n\n# A single colon means also means use the full thing.\nr = a[:]\nprint(r)\n\n\n[0 1 2 3 4 5 6 7 8 9]\n\n\n\n\nCode\n\n# Using slices, you can also set individual elements in the array:\nr[0] = 33\nr[3:5] = 44\nprint('r', r)\n\n\nr [33  1  2 44 44  5  6  7  8  9]\n\n\n\n\nCode\n\n# Setting in this way also can be done according to a condition:\nr[r &lt;= 6] = 5\nprint('r', r)\n\n\nr [33  5  5 44 44  5  5  7  8  9]\n\n\n\n\nCode\n\n# Finally, an alternate and possibly more powerful way of setting conditional values is the np.where function\n# This function sets anywhere greater than 10 to be 12, otherwise it keeps it at whatever value was already in r\nr = np.where(r &gt; 10, 12, r)\nprint('r', r)\n\n\nr [12  5  5 12 12  5  5  7  8  9]\n\n\nFinally, if you want to combine conditionals, when you‚Äôre working inside an array you need to use Parentheses, & for and and | for or, as below.\n\n\nCode\nd[(d &gt; 200) & (d &lt; 10000)] = 33"
  },
  {
    "objectID": "Python_Introduction/numpy_arrays.html#lets-talk-about-performance",
    "href": "Python_Introduction/numpy_arrays.html#lets-talk-about-performance",
    "title": "6¬† Numpy Arrays",
    "section": "6.5 Let‚Äôs talk about performance",
    "text": "6.5 Let‚Äôs talk about performance\nBig data requires fast algorithms. Let‚Äôs introduce a slow way of applying an algorithm, and then the fast way.\n\n\nCode\n\n# Slowly looping over arrays\nfor i in a:\n    r = i**(1/3.)\nprint('r', r)\n\n\nr 2.080083823051904\n\n\n\n\nCode\n\n# Slowly loop to get the sum of the array\nr = 0\nfor row in b:\n    print('row', row)\n    for value in row:\n        print('value', value)\n        r += value\n\nprint('slow sum', r)\n\n\nrow [0 1 2 3]\nvalue 0\nvalue 1\nvalue 2\nvalue 3\nrow [4 5 6 7]\nvalue 4\nvalue 5\nvalue 6\nvalue 7\nrow [ 8  9 10 11]\nvalue 8\nvalue 9\nvalue 10\nvalue 11\nslow sum 66\n\n\nNOTE: Iterating over arrays here is just for illustration as it is VERY VERY SLOW and loses the magic of numpy speed. We‚Äôll learn how to bet around this later by ‚Äúvectorizing‚Äù functions, which basically means batch calculating everything in a vector all in one call. For now, here‚Äôs an example of the much faster version\n\n\nCode\n\nr = b.sum()\nprint('fast sum', r)\n\n\nfast sum 66"
  },
  {
    "objectID": "Python_Introduction/numpy_arrays.html#diving-into-vectorized-computation.",
    "href": "Python_Introduction/numpy_arrays.html#diving-into-vectorized-computation.",
    "title": "6¬† Numpy Arrays",
    "section": "6.6 Diving into vectorized computation.",
    "text": "6.6 Diving into vectorized computation.\nHere we are going to do matrix math, but using the fast numpy methods.\n\n\nCode\n\n# Vectorized multiplication (and broadcasting):\n\n# First lets make two arrays. This is the cannonical way of making example arrays\na = np.arange(20).reshape(5, 4)\nb = np.arange(20).reshape(5, 4)\nprint(a)\nprint(b)\n\n\n[[ 0  1  2  3]\n [ 4  5  6  7]\n [ 8  9 10 11]\n [12 13 14 15]\n [16 17 18 19]]\n[[ 0  1  2  3]\n [ 4  5  6  7]\n [ 8  9 10 11]\n [12 13 14 15]\n [16 17 18 19]]\n\n\n\n\nCode\nc = a * b # NOTE: this does element-wise multiplication, not the matrix multiplication you learned in 7-th? grade.\nprint(c)\n\n\n[[  0   1   4   9]\n [ 16  25  36  49]\n [ 64  81 100 121]\n [144 169 196 225]\n [256 289 324 361]]"
  },
  {
    "objectID": "Python_Introduction/numpy_arrays.html#numpy-broadcasting",
    "href": "Python_Introduction/numpy_arrays.html#numpy-broadcasting",
    "title": "6¬† Numpy Arrays",
    "section": "6.7 Numpy broadcasting",
    "text": "6.7 Numpy broadcasting\nNumpy will smartly ‚Äúbroadcast‚Äù two matrices of different sizes or dimensions so that it works:\n\n\nCode\nd = np.arange(4)\ne = a * d # WAIT! Aren't you multiplying two different matrices with different sizes? Yes!\nprint(e)\n\n\n[[ 0  1  4  9]\n [ 0  5 12 21]\n [ 0  9 20 33]\n [ 0 13 28 45]\n [ 0 17 36 57]]\n\n\nAbove, Numpy smartly figured out how the two dimensions could be repeatedly broadcast to each other so the math was ‚Äúwell defined.‚Äù\n\n\nCode\n\n# Also means you can use the same notation to multiply an array (2dim) against a scalar (0dim):\nf = a * 6.0\nprint('f\\n', f)\n\n\nf\n [[  0.   6.  12.  18.]\n [ 24.  30.  36.  42.]\n [ 48.  54.  60.  66.]\n [ 72.  78.  84.  90.]\n [ 96. 102. 108. 114.]]\n\n\nLet‚Äôs plot the results of this new f matrix we created.\n\n\nCode\n\nax = plt.imshow(b)\nplt.show()\n\n\n\n\n\nLooks very similar to above, so let‚Äôs add a colorbar to make clear this value is not just our starting array.\n\n\nCode\nax = plt.imshow(b)\nplt.colorbar(ax)\nplt.show()"
  },
  {
    "objectID": "Python_Introduction/numpy_arrays.html#plot-mandlebrot",
    "href": "Python_Introduction/numpy_arrays.html#plot-mandlebrot",
    "title": "6¬† Numpy Arrays",
    "section": "6.8 Plot Mandlebrot",
    "text": "6.8 Plot Mandlebrot\nWith these tools, we can do all sorts of things. Just for fun, let‚Äôs end this section by defining a function for the Mandlebrot set and then plotting the function.\n\n\nCode\n\ndef mandelbrot(h, w, maxit=20 ):\n    \"\"\"Returns an image of the Mandelbrot fractal of size (h,w).\"\"\"\n    y,x = np.ogrid[ -1.4:1.4:h*1j, -2:0.8:w*1j ]\n    c = x+y*1j\n    z = c\n    divtime = maxit + np.zeros(z.shape, dtype=int)\n\n    for i in range(maxit):\n        z = z**2 + c\n        diverge = z*np.conj(z) &gt; 2**2            # who is diverging\n        div_now = diverge & (divtime==maxit)  # who is diverging now\n        divtime[div_now] = i                  # note when\n        z[diverge] = 2                        # avoid diverging too much\n\n    return divtime\n\n\nplt.imshow(mandelbrot(400, 400))\nplt.show()"
  },
  {
    "objectID": "Python_Introduction/numpy_arrays.html#in-class-exercise-3.1",
    "href": "Python_Introduction/numpy_arrays.html#in-class-exercise-3.1",
    "title": "6¬† Numpy Arrays",
    "section": "6.9 In-class exercise 3.1:",
    "text": "6.9 In-class exercise 3.1:\nAgain, I‚Äôll call on a random table to showcase their results.\n\nCreate a 20 by 40 matrix of random values 0-1 (Hint: use the np.random.random function. Use VS Code‚Äôs built-in help to see what you should enteR).\nSet the upper left quadrant to 1. (Hint: use slices)\nNext, set the last COLUMN to 2.\nFinally, change all values less than .5 to be 3. (Use np.where)\n\n\n\nCode\n# Class activity workspace"
  },
  {
    "objectID": "Python_Introduction/pandas_tables.html",
    "href": "Python_Introduction/pandas_tables.html",
    "title": "7¬† Pandas Tables",
    "section": "",
    "text": "Code\n\nimport pandas\nimport os\n\ndata_directory = '../data'\nfood_prices_filename = 'world_monthly_food_prices.csv'\nfood_prices_path = os.path.join(data_directory, food_prices_filename)\nfood_prices = pandas.read_csv(food_prices_path)\n\nprint('Whole dataframe:', food_prices)\n\n\nWhole dataframe:                                      Domain Code                  Domain  \\\n0                                             CP  Consumer Price Indices   \n1                                             CP  Consumer Price Indices   \n2                                             CP  Consumer Price Indices   \n3                                             CP  Consumer Price Indices   \n4                                             CP  Consumer Price Indices   \n..                                           ...                     ...   \n176                                           CP  Consumer Price Indices   \n177                                           CP  Consumer Price Indices   \n178                                           CP  Consumer Price Indices   \n179                                           CP  Consumer Price Indices   \n180  FAOSTAT Date: Wed Aug 17 16:12:56 CEST 2016                     NaN   \n\n     AreaCode AreaName  ElementCode ElementName  ItemCode  \\\n0      5000.0    World       7001.0     January   23013.0   \n1      5000.0    World       7001.0     January   23013.0   \n2      5000.0    World       7001.0     January   23013.0   \n3      5000.0    World       7001.0     January   23013.0   \n4      5000.0    World       7001.0     January   23013.0   \n..        ...      ...          ...         ...       ...   \n176    5000.0    World       7012.0    December   23013.0   \n177    5000.0    World       7012.0    December   23013.0   \n178    5000.0    World       7012.0    December   23013.0   \n179    5000.0    World       7012.0    December   23013.0   \n180       NaN      NaN          NaN         NaN       NaN   \n\n                                       ItemName    Year  Value  Flag  \\\n0    Consumer Prices, Food Indices (2000 = 100)  2000.0   99.5   NaN   \n1    Consumer Prices, Food Indices (2000 = 100)  2001.0  101.5   NaN   \n2    Consumer Prices, Food Indices (2000 = 100)  2002.0  106.5   NaN   \n3    Consumer Prices, Food Indices (2000 = 100)  2003.0  112.5   NaN   \n4    Consumer Prices, Food Indices (2000 = 100)  2004.0  119.2   NaN   \n..                                          ...     ...    ...   ...   \n176  Consumer Prices, Food Indices (2000 = 100)  2011.0  210.3   NaN   \n177  Consumer Prices, Food Indices (2000 = 100)  2012.0  224.2   NaN   \n178  Consumer Prices, Food Indices (2000 = 100)  2013.0  239.0   NaN   \n179  Consumer Prices, Food Indices (2000 = 100)  2014.0  250.9   NaN   \n180                                         NaN     NaN    NaN   NaN   \n\n             FlagD  \n0    Official data  \n1    Official data  \n2    Official data  \n3    Official data  \n4    Official data  \n..             ...  \n176  Official data  \n177  Official data  \n178  Official data  \n179  Official data  \n180            NaN  \n\n[181 rows x 12 columns]\n\n\n\n\nCode\nprint('List of column names:', food_prices.columns)\n\n\nList of column names: Index(['Domain Code', 'Domain', 'AreaCode', 'AreaName', 'ElementCode',\n       'ElementName', 'ItemCode', 'ItemName', 'Year', 'Value', 'Flag',\n       'FlagD'],\n      dtype='object')\n\n\n\n\nCode\nprint('Specific column:', food_prices['Value'])\n\n\nSpecific column: 0       99.5\n1      101.5\n2      106.5\n3      112.5\n4      119.2\n       ...  \n176    210.3\n177    224.2\n178    239.0\n179    250.9\n180      NaN\nName: Value, Length: 181, dtype: float64\n\n\n\n\nCode\nprint('Specific value in that column:', food_prices['Value'][6])\n\n\nSpecific value in that column: 132.2\n\n\n\n\nCode\n\nimport matplotlib\nfrom matplotlib import pyplot as plt\nplt.plot(food_prices['Value'])\nplt.show() \n\n\n\n\n\n\n\nCode\n\nimport os\n\nimport numpy as np\nimport pandas as pd\n\n# Set a seed value for the random generator\nnp.random.seed(48151623)\n\n# Creating a Series by passing a list of values, letting pandas create a default integer index:\ns = pd.Series([1, 3, 5, np.nan, 6, 8])\n\n# Pandas is very detailed in dealing with dates and all the quirks (leap year?) that this leads to.\ndates = pd.date_range('20130101', periods=6)\n\n# Creating a DataFrame by passing a NumPy array, with a datetime index and labeled columns:\ndf = pd.DataFrame(np.random.randn(6, 4), columns=list('ABCD'))\nprint('df:\\n', df)\n\n\ndf2 = pd.DataFrame({'A': 1.,\n                    'B': pd.Timestamp('20130102'),\n                    'C': pd.Series(1, index=list(range(4)), dtype='float32'),\n                    'D': np.array([3] * 4, dtype='int32'),\n                    'E': pd.Categorical([\"test\", \"train\", \"test\", \"train\"]),\n                    'F': 'foo'})\n\n# df.head()\n# print(df.index)\n# print(df.columns)\ndf.describe()\n\n# Also note that a dataframe is really just a numpy array dressed up with extra trappings. If you want you\n# can get back the raw array (though this might lose a lot of functionality).\na = df.to_numpy()\nprint('a\\n', a)\n\n# Sorting Values:\n\n# Also, I want to illustrate THE MOST COMMON MISTAKE people make with Pandas.\n\n# The sort_values method (a method is just a function attached to an object) returns a NEW modified dataframe.\n# Thus, in the line below, if you just printed df, it would not be sorted because we didn't use the returned value.\ndf.sort_values(by='B')\n# print('Not sorted:\\n', df)\n\n# Easy way to get around this is just to assign the returned dataframe to a variable (even the input variable)\ndf = df.sort_values(by='B')\n# print('Sorted with return:\\n', df)\n\n# Alternatively, if you hate returning things, there is the inplace=True command, which will modify the df ... inplace.\ndf.sort_values(by='B', inplace=True)\n# print('Sorted inplace:\\n', df)\n\n## Selection/subsetting of data\n\n# Selecting a single column, which yields a Series, equivalent to df.A\ndf['A']\ndf.A\n\n# Selecting via [], which slices the rows.\ndf[0:3] # CAN BE SLOW\n\n# Note, slicing above, which uses the\n# standard Python / Numpy expressions for selecting and setting are intuitiveits best to use\n# the optimized pandas data access methods, .at, .iat, .loc and .iloc.\n\n## Selecting by LABELS, loc and iloc\n\nr = df.loc[0] # 0-th row.\n\n# print('r', r)\n\n# Discuss difference between df['A'] and df.loc[0]\nr = df.loc[0, 'A']\n\nr = df.loc[:, 'A'] # Colon is a slice, an empty colon means ALL the values.\n\n# OPTIMIZATION:\n# for faster single point access, use:\nr = df.at[0, 'A']\n\n# SELECTING BY POSITION\nr = df.iloc[3]\n\n# Selecting with slices\nr = df.iloc[3:5, 0:2]\n\n# Slices again with an empty slice.\nr = df.iloc[1:3, :]\n\nr = df.iloc[:, 1:3]\n\n# SIMILAR OPTIMIZATION:\nr = df.iat[1, 1]\n\n# Boolean indexing\n# Using a single column‚Äôs values to select data.\nr = df[df['A'] &gt; 0]\n\n# Make a copy (why?) and add a column\ndf2 = df.copy()\ndf2['E'] = ['one', 'one', 'two', 'three', 'four', 'three']\nr = df2[df2['E'].isin(['two', 'four'])]\n\n\n# Setting by assigning with a NumPy array:\ndf.loc[:, 'D'] = np.array([5] * len(df))\n\n# Missing data\n\n# First we're going to create a new df by \"reindexing\" the old one, which will shuffle the data into a new\n# order according to the index provided. At the same time, we're going to add on a new, empty column\n# EE, which we set as 1 for the first two obs.\n\ndf1 = df.reindex(index=[2, 0, 1, 3], columns=list(df.columns) + ['EE'])\ndf1.loc[0:1, 'EE'] = 1\n# print(df1)\n\n# Apply: Similar to R. Applies a function across many cells (fast because it's vectorized)\ndf.apply(np.cumsum)\ndf.apply(lambda x: x.max() - x.min())\n\n# Concat\ns = pd.Series(range(0, 6))\n# print('s', s)\n\nr = pd.concat([df, s]) # Concatenate it, default is by row, which just puts it on the bottom.\n\nr = pd.concat([df, s], axis=1) # Concatenate as a new column\n\n# print(r) # Result when concatenating a series of the same size.\n\ns = pd.Series(range(0, 7))\nr = pd.concat([df, s], axis=1) # Concatenate as a new column\n\ns = pd.Series(range(0, 2))\nr = pd.concat([df, s], axis=1) # Concatenate as a new column\n\n# Join\n# SQL style merges. See the Database style joining section.\n\nleft = pd.DataFrame({'key': ['foo', 'bar'], 'lval': [1, 2]})\nright = pd.DataFrame({'key': ['foo', 'bar'], 'rval': [4, 5]})\n\n# print(left)\n# print(right)\n\ndf = pd.merge(left, right, on='key')\n\n# print('df:\\n', df)\n\n# Stacking\nstacked = df.stack()\n# print('stacked:\\n', stacked)\n\n\n# Pivot Tables\ndf = pd.DataFrame({'A': ['one', 'one', 'two', 'three'] * 3,\n                   'B': ['A', 'B', 'C'] * 4,\n                   'C': ['foo', 'foo', 'foo', 'bar', 'bar', 'bar'] * 2,\n                   'D': np.random.randn(12),\n                   'E': np.random.randn(12)})\n\n# print(df) # SPREADSHEET VIEW\ndf = pd.pivot_table(df, values='D', index=['A', 'B'], columns=['C'])\n# print(df) # Multiindexed (Pivot table) view.\n\n# NOTICE that a pivot table is just the above date but where specific things have been made into multi-level\n# indices.\n\n# PLOTTING\nts = pd.Series(np.random.randn(1000),\n            index=pd.date_range('1/1/2000', periods=1000))\n\nts = ts.cumsum()\nts.plot()\nimport matplotlib.pyplot as plt\n# plt.show()\n\n\n# Writing to files\n\ndf.to_csv('foo.csv')\n\n# Reading files:\n\n# FIRST NOTE, here we are using relative paths (which you should almost always do too). the ../ means go up one level.\n# this path works if you organized your data into the folder structure I suggested.\nwdi_filename = \"WDI_CO2_data.csv\"\nwdi_path = os.path.join(data_directory, wdi_filename)\ndf = pd.read_csv(wdi_path)\n\nprint('csv read as a df\\n', df)\n\n# For reference, here's the Excel version\n# df = pd.read_excel('foo.xlsx', 'Sheet1', index_col=None, na_values=['NA'])\n\ncols = list(df.columns)\n\n# Make a subset of only 2 cols\nr = df[['Country Code', '1970 [YR1970]']]\n# print(r)\n\nr = df.loc[df['Country Code'] == 'CAN']\n# print('r', r)\n\nrr = r.loc[df['Series Name'] == 'Total greenhouse gas emissions (kt of CO2 equivalent)']\nprint(rr)\n\n# Class exercise: Plot the emissions of CO2 for Canada (or whereever I don't care).\n\n\ndf:\n           A         B         C         D\n0  0.336581  0.923754 -0.277124  0.388604\n1 -1.295428  3.296657 -0.698246  0.245552\n2 -1.086536  1.187113 -0.153344 -1.264476\n3  0.798694  1.330577 -0.113975 -0.060949\n4 -0.076321 -0.240310  0.728421 -0.384309\n5  0.634212  1.605129  1.415844  0.385849\na\n [[ 0.33658127  0.92375415 -0.27712413  0.38860406]\n [-1.2954285   3.29665716 -0.69824576  0.24555238]\n [-1.0865361   1.18711302 -0.15334368 -1.26447611]\n [ 0.79869365  1.33057707 -0.11397528 -0.06094946]\n [-0.07632075 -0.24031028  0.72842085 -0.38430931]\n [ 0.6342119   1.60512881  1.41584369  0.38584939]]\ncsv read as a df\n                                           Country Name Country Code  \\\n0                                          Afghanistan          AFG   \n1                                          Afghanistan          AFG   \n2                                          Afghanistan          AFG   \n3                                          Afghanistan          AFG   \n4                                          Afghanistan          AFG   \n...                                                ...          ...   \n2640                                               NaN          NaN   \n2641                                               NaN          NaN   \n2642                                               NaN          NaN   \n2643  Data from database: World Development Indicators          NaN   \n2644                          Last Updated: 10/15/2020          NaN   \n\n                                            Series Name           Series Code  \\\n0     Access to clean fuels and technologies for coo...        EG.CFT.ACCS.ZS   \n1               Access to electricity (% of population)        EG.ELC.ACCS.ZS   \n2     Adjusted net enrollment rate, primary (% of pr...           SE.PRM.TENR   \n3                          Arable land (% of land area)        AG.LND.ARBL.ZS   \n4     Agricultural methane emissions (thousand metri...  EN.ATM.METH.AG.KT.CE   \n...                                                 ...                   ...   \n2640                                                NaN                   NaN   \n2641                                                NaN                   NaN   \n2642                                                NaN                   NaN   \n2643                                                NaN                   NaN   \n2644                                                NaN                   NaN   \n\n     1960 [YR1960]     1961 [YR1961]     1962 [YR1962]     1963 [YR1963]  \\\n0               ..                ..                ..                ..   \n1               ..                ..                ..                ..   \n2               ..                ..                ..                ..   \n3               ..  11.7176730079956  11.7942591060871  11.8708452041785   \n4               ..                ..                ..                ..   \n...            ...               ...               ...               ...   \n2640           NaN               NaN               NaN               NaN   \n2641           NaN               NaN               NaN               NaN   \n2642           NaN               NaN               NaN               NaN   \n2643           NaN               NaN               NaN               NaN   \n2644           NaN               NaN               NaN               NaN   \n\n       1964 [YR1964]   1965 [YR1965]  ...     2011 [YR2011]     2012 [YR2012]  \\\n0                 ..              ..  ...             22.33             24.08   \n1                 ..              ..  ...  43.2220189082037              69.1   \n2                 ..              ..  ...                ..                ..   \n3     11.94743130227  11.94743130227  ...  11.9336458046135  11.9321140826517   \n4                 ..              ..  ...                ..                ..   \n...              ...             ...  ...               ...               ...   \n2640             NaN             NaN  ...               NaN               NaN   \n2641             NaN             NaN  ...               NaN               NaN   \n2642             NaN             NaN  ...               NaN               NaN   \n2643             NaN             NaN  ...               NaN               NaN   \n2644             NaN             NaN  ...               NaN               NaN   \n\n         2013 [YR2013]    2014 [YR2014]    2015 [YR2015]     2016 [YR2016]  \\\n0                26.17            27.99             30.1             32.44   \n1     68.9332656860352             89.5             71.5              97.7   \n2                   ..               ..               ..                ..   \n3     11.9244554728426  11.903011365377  11.893821033606  11.8386790429801   \n4                   ..               ..               ..                ..   \n...                ...              ...              ...               ...   \n2640               NaN              NaN              NaN               NaN   \n2641               NaN              NaN              NaN               NaN   \n2642               NaN              NaN              NaN               NaN   \n2643               NaN              NaN              NaN               NaN   \n2644               NaN              NaN              NaN               NaN   \n\n     2017 [YR2017]     2018 [YR2018] 2019 [YR2019] 2020 [YR2020]  \n0               ..                ..            ..            ..  \n1             97.7  98.7132034301758            ..            ..  \n2               ..                ..            ..            ..  \n3               ..                ..            ..            ..  \n4               ..                ..            ..            ..  \n...            ...               ...           ...           ...  \n2640           NaN               NaN           NaN           NaN  \n2641           NaN               NaN           NaN           NaN  \n2642           NaN               NaN           NaN           NaN  \n2643           NaN               NaN           NaN           NaN  \n2644           NaN               NaN           NaN           NaN  \n\n[2645 rows x 65 columns]\n    Country Name Country Code  \\\n369       Canada          CAN   \n\n                                           Series Name        Series Code  \\\n369  Total greenhouse gas emissions (kt of CO2 equi...  EN.ATM.GHGT.KT.CE   \n\n    1960 [YR1960] 1961 [YR1961] 1962 [YR1962] 1963 [YR1963] 1964 [YR1964]  \\\n369            ..            ..            ..            ..            ..   \n\n    1965 [YR1965]  ...     2011 [YR2011]     2012 [YR2012] 2013 [YR2013]  \\\n369            ..  ...  1033481.98200961  1027063.85487082            ..   \n\n    2014 [YR2014] 2015 [YR2015] 2016 [YR2016] 2017 [YR2017] 2018 [YR2018]  \\\n369            ..            ..            ..            ..            ..   \n\n    2019 [YR2019] 2020 [YR2020]  \n369            ..            ..  \n\n[1 rows x 65 columns]\n\n\nC:\\Users\\jajohns\\AppData\\Local\\Temp\\ipykernel_26860\\2184278581.py:107: FutureWarning:\n\nIn a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`"
  },
  {
    "objectID": "Python_Introduction/gdal_rasters.html",
    "href": "Python_Introduction/gdal_rasters.html",
    "title": "8¬† GDAL Rasters",
    "section": "",
    "text": "9 Saving a raster to your harddrive\nNow that you‚Äôve created an amazing raster of total maize production, you might want to save it to your harddrive.\nTo do this, we‚Äôre first going to define a new filename for our output file. In the code below, + concatenates things. Str() makes the number a string.\nCode\noutput_filename = 'gdal_created_array_' + str(random.randint(1, 1000000)) + '.tif'\noutput_file_path = os.path.join(data_directory, output_filename)\nCreate a new file at that filename location using the attributes we used above. Notice that we flipped n_cols and n_rows from how numpy would have wanted it. For extra BONUS value, replace the d array with the one you created in the in-class exercise.\nCode\noutput_dataset = gdal.GetDriverByName('GTiff').Create(output_file_path, d.shape[1], d.shape[0], 1, 6)\nSet dataset-level information. Here we‚Äôre just using what we got from the input raster, defined above.\nCode\noutput_dataset.SetGeoTransform(geotransform)\noutput_dataset.SetProjection(projection)\n\n\n0\nNow get a band from our new dataset on which we‚Äôll write our array.\nCode\noutput_band = output_dataset.GetRasterBand(1)\nDo the array writing\nCode\noutput_band.WriteArray(d)\n\n\n0\nSet any final band-level information\nCode\noutput_band.SetNoDataValue(no_data_value)\n\n\n0\nFinally, and very importantly, clean up after yourself. It wont actually write until the resources in memory have been released.\nCode\nd = None\noutput_band = None\noutput_dataset = None"
  },
  {
    "objectID": "Python_Introduction/gdal_rasters.html#spatial-data-in-jupyter",
    "href": "Python_Introduction/gdal_rasters.html#spatial-data-in-jupyter",
    "title": "8¬† GDAL Rasters",
    "section": "8.1 Spatial data in Jupyter",
    "text": "8.1 Spatial data in Jupyter\nFirst we‚Äôre going to import gdal, numpy and a few other things.\n\n\nCode\nfrom osgeo import gdal\nimport numpy as np\nimport os, random\n\n\nFirst, define the path to our raster data. This is remarkably difficult (at least, as measured by how many hours I have wasted because my code pointed to the wrong place). A the superior way to manage this is with RELATIVE PATHS. Here, we define the filename, the directory in relative terms, and then join them together using the os functions.\n\n\nCode\ngeotiff_filename = 'rwanda_lulc_2015.tif'\n\n# the ../ notation means go up one level relative to your current working directory. This gets us outside of the course\n# repository and into our Data directory\ndata_directory = '../data' \n\n# Join them together (this will work across operating systems)\ngeotiff_file_path = os.path.join(data_directory, geotiff_filename)\n\n\n\n\nIt‚Äôs often easy to get confused by relative paths, accidentally missing a level or something. One way to trouble-shoot this is to view the current working direcotry:\n\n\nCode\nprint(os.getcwd())\n\n\nD:\\My Drive\\Files\\Teaching\\jupyter_based_courses\\earth_economy_textbook\\earth_economy_textbook_dev\\01_Python_Introduction\n\n\nOr to view the absolute path:\n\n\nCode\nos.path.abspath(data_directory)\n\n\n'D:\\\\My Drive\\\\Files\\\\Teaching\\\\jupyter_based_courses\\\\earth_economy_textbook\\\\earth_economy_textbook_dev\\\\data'\n\n\nSometimes it can be useful to see what is in the directory you‚Äôve specified (to help you figure out what‚Äôs going on if it can‚Äôt find the file)\n\n\nCode\ncontents = os.listdir(data_directory)\nprint(contents)\n\n\n['rwanda_lulc_2015.tif', 'world_monthly_food_prices.csv', 'rwanda_lulc_2000.tif', 'rwanda_lulc_2010.tif', 'WDI_CO2_data.csv', 'gdal_created_array_737809.tif', 'gdal_created_array_828327.tif']\n\n\n\n\nCode\n# If you want to be super clear, you can actually check if it exists\nprint(os.path.exists(geotiff_file_path))\n\n\nTrue"
  },
  {
    "objectID": "Python_Introduction/gdal_rasters.html#gdal-and-opening-the-raster",
    "href": "Python_Introduction/gdal_rasters.html#gdal-and-opening-the-raster",
    "title": "8¬† GDAL Rasters",
    "section": "8.2 GDAL and opening the raster",
    "text": "8.2 GDAL and opening the raster\nNow that we know for sure that the file is there, we can use gdal and it‚Äôs Open function using the DOT notation (technically its a ‚Äúmethod‚Äù not a function, but you can ignore that. If you‚Äôre actually curious about object-oriented programming, a method is just a function attached to an object.).\n\n\nCode\nmaize_production_tons_per_cell = gdal.Open(geotiff_file_path)\nprint(maize_production_tons_per_cell)\n\n\n&lt;osgeo.gdal.Dataset; proxy of &lt;Swig Object of type 'GDALDatasetShadow *' at 0x000001D92F0A8030&gt; &gt;\n\n\nThe dataset object holds information about the area and extent of the data, or the geotransform information\n\n\nCode\ngeotransform = maize_production_tons_per_cell.GetGeoTransform()\nprojection = maize_production_tons_per_cell.GetProjection()\n\nprint('GDAL dataset geotransform', geotransform)\nprint('GDAL dataset projection', projection)\n\n\nGDAL dataset geotransform (28.855555555555558, 0.002777777777777778, 0.0, -1.0583333333333333, 0.0, -0.002777777777777778)\nGDAL dataset projection GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]\n\n\nIMPORTANT ANNOYING NOTE: in programming, there are different conventions for identifying a place by rows, cols vs.¬†x, y vs.¬†upper-left, lower-right, etc. Numpy is denoted row, col but gdal is denoted X, Y (which flips the order). Just memorize that row = Y and col = X.\n\n\nCode\nn_rows = maize_production_tons_per_cell.RasterYSize\nprint('Number of rows in a GDAL dataset', n_rows)\n\nn_cols = maize_production_tons_per_cell.RasterXSize\nprint('Number of columns in a GDAL dataset', n_cols)\n\n\nNumber of rows in a GDAL dataset 637\nNumber of columns in a GDAL dataset 732\n\n\nNext, get the ‚Äúband‚Äù of the dataset. Many datasets have multiple layers (e.g.¬†NetCDFs). Geotiffs can have multiple bands but often have just 1. For now, grab band 1\n\n\nCode\nmaize_production_tons_per_cell_band = maize_production_tons_per_cell.GetRasterBand(1)\n\n\nThe band object has information too, like the datatype of the geotiff:\n\n\nCode\ndata_type = maize_production_tons_per_cell_band.DataType\nno_data_value = maize_production_tons_per_cell_band.GetNoDataValue()\n\nprint('data_type', data_type)\nprint('no_data_value', no_data_value)\n\n\ndata_type 1\nno_data_value 255.0\n\n\nFinally, we can get the array from the band as a numpy array:\n\n\nCode\narray = maize_production_tons_per_cell_band.ReadAsArray()\nshape = array.shape\n\nprint('Look at the array itself', array)\n\n\nLook at the array itself [[255 255 255 ... 255 255 255]\n [255 255 255 ... 255 255 255]\n [255 255 255 ... 255 255 255]\n ...\n [255 255 255 ... 255 255 255]\n [255 255 255 ... 255 255 255]\n [255 255 255 ... 255 255 255]]"
  },
  {
    "objectID": "Python_Introduction/gdal_rasters.html#plotting-a-raster",
    "href": "Python_Introduction/gdal_rasters.html#plotting-a-raster",
    "title": "8¬† GDAL Rasters",
    "section": "8.3 Plotting a raster",
    "text": "8.3 Plotting a raster\nWe are now going to use matplotlib. It is basically like ggplot and draws its inspiration from MATLAB notation. By convention, we‚Äôll import it into the variable name plt, which is an object that lets us use matplotlib plotting notation.\n\n\nCode\n\nimport matplotlib.pyplot as plt\n\nplt.imshow(array)\nplt.title('Land-use, land-cover')\nplt.colorbar(orientation='horizontal')\n\n# Uncomment this if you want to save it\n# plt.savefig('maize.png', dpi=300) \n\nplt.show()\n\n\n\n\n\nThis is super ugly for one primary reason: it has scaled the colorbar to the minimum and maximum values, which ends up coloring nearly everything close to the zero value. We‚Äôre going to crop the values it shoes to not let the outliers define the colorbar range.\n\n\nCode\nimport matplotlib.pyplot as plt\n\n# The second (and often better) plotting method is to use plt to create a figure and one ore more axes.\n# This is potentially confusing but is powerful. the Axes object we created (ax) is the plottable area (and there could be lots of axes)\n# The figure contains all the axes and is responsible for organizing stuff.\n\nfig = plt.figure(figsize=(8, 6))\nfig.set_dpi(300)\n\nax = fig.add_subplot()\n\n# Set the title of this ax object\nax.set_title('Land-use, land-cover')\n\n# Using the ax we created, we call the imshow function on our array from earlier. This create a new \"im\" object\nim = ax.imshow(array)\n\n# To fix the outlier problem from before, we use the im object to set its limits.\nim.set_clim(0, 200)\n\n# Add the colorbar to the figure. It will generate its values from the im object.\nfig.colorbar(im, orientation='horizontal')\n\nplt.show()"
  },
  {
    "objectID": "Python_Introduction/gdal_rasters.html#copies-versus-views-in-numpy",
    "href": "Python_Introduction/gdal_rasters.html#copies-versus-views-in-numpy",
    "title": "8¬† GDAL Rasters",
    "section": "8.4 Copies versus Views in Numpy",
    "text": "8.4 Copies versus Views in Numpy\nPart of how Numpy arrays are fast is the only ever load or access data when it is needed. This means that if you don‚Äôt tell numpy to make a copy of something, any new variable will point to the old array. More specifically, this only creates a new pointer to the same block of memory on your computer that holds the array. If we change c_view, c will also be changed. So in the below, c_view only points to the old data in c.¬†This is called a ‚Äúview‚Äù of the array.\n\n\nCode\nc_view = array\n\n\nThis also means that if you modify array, you will be modifying what you have in c_view.\nIf you really need a copy in memory, you can use the numpy method copy():\n\n\nCode\nd = array.copy()\n\n\nThis gives us a NEW array in a new block of memory, so changing array will not change d,"
  },
  {
    "objectID": "Python_Introduction/gdal_rasters.html#in-class-exercise-4.1",
    "href": "Python_Introduction/gdal_rasters.html#in-class-exercise-4.1",
    "title": "8¬† GDAL Rasters",
    "section": "8.5 In-class exercise 4.1",
    "text": "8.5 In-class exercise 4.1\nHere you will use GDAL and numpy to do some highly-optimized raster manipulation.\nUsing data from Earthstat, I want you to calculate the production per grid-cell of Maize, globally, at ‚Äúhigh‚Äù-resolution.\nThe two files you need are in your class data directory (obtained from google drive). On my computer they are saved in the following locations.\nD:\\My Drive\\Files\\Teaching\\APEC 8222\\Data\\maize_HarvestedAreaHectares.tif\nD:\\My Drive\\Files\\Teaching\\APEC 8222\\Data\\maize_YieldPerHectare.tif\nUse the os.path.join() approach from earlier to correcly make relative paths (NOT ABSOLUTE PATHS LIKE I PASTED ABOVE) pointing to the two files.\nOpen them up as raster using Gdal.\nMultiply the HarvestedAreaHectars by YieldPerHectare. This will give you the total production on the grid-cell.\nUse numpy to sum up the total production of Maize globally and report that to the class."
  },
  {
    "objectID": "Python_Introduction/python_assignment_1.html#question-1",
    "href": "Python_Introduction/python_assignment_1.html#question-1",
    "title": "9¬† Exercise 2",
    "section": "9.1 Question 1",
    "text": "9.1 Question 1\nThis question tests some basic python comprehension.\nThroughout, use comments to explain steps you are doing. No need to go overboard but it is always important in coding to be as descriptive as possible.\nMost things in this can be solved using methods shown in lecture. However, some cases will require you to search the internet for answers. This is intended because efficiently searching documentation or Stackoverflow is a requirement of modern programing.\nSubsequent questions will be specific to big data, statistics and economics, but for now we are just checking that you‚Äôre okay with the basics of Python.\nPart A: Write a python function that calculates the sum of all squared numbers from 1 to x. Illustrate that it works for x = 20.\nHINT, ** is the exponent operator in python. HINT syntax for a python function is:\ndef function_name(variable_name): outcome = variable_name + 1 return outcome\n\n\nCode\n# 1A Answer\n\n\nThe python library named ‚Äúos‚Äù is a built-in library for dealing with any file on your operating system. Often, research tasks involve LOTS of files and you need to iterate over them. To show you know how to do this, use the os.listdir function to answer the following questions. Note that you need to write ‚Äúimport os‚Äù to import the library into your code before you can use it:\nPart B: Print out a list of all the files in the class repository (which you have gotten from GitHub). I don‚Äôt care how many are actually there (in case you‚Äôve added some yourself) but show me how.\n\n\nCode\n#1B Answer\n\n\nPart C: Using a FOR loop, iterate over the list from above. Using the function len(), count how many letters there are in the filenames. HINT just like in real life, you may need to google the len() function and see how it work on e.g.¬†strings.\n\n\nCode\n# 1C Answer\n\n\nPart D: Write a Python program which iterates over the integers from 1 to 50. For multiples of three print ‚ÄúFizz‚Äù instead of the number and for the multiples of five print ‚ÄúBuzz‚Äù. For numbers which are multiples of both three and five print ‚ÄúFizzBuzz‚Äù. Hint: look up how to use the modulo operator (which is written as % in python) which would let you test when the remainder of a devision is exactly 0.\nSide-note, this is a hilariously over-used question that most software engineers get askedon their first interview for a job.\n\n\nCode\n# 1D Answer"
  },
  {
    "objectID": "Python_Introduction/python_assignment_1.html#question-2",
    "href": "Python_Introduction/python_assignment_1.html#question-2",
    "title": "9¬† Exercise 2",
    "section": "9.2 Question 2",
    "text": "9.2 Question 2\nPart a.) Using the gdal package and the gdal.Open() function, open up the land-use, land-cover map for Rwanda in 2000.\nIn this file, there is only 1 band in this file, so you can also access it with the GetRasterBand(1) function. Without reading the whole array, show how you can determine how many total grid-cells there are in this country.\n\n\nCode\n# 2A Answer\n\n\nPart b.) Using the results of part a, read the whole array into memory as a numpy array (the default option when using the ReadAsArray() funciton, and plot it using the matplotlib imshow command. Add a nice title to the plot describing what it is.\n\n\nCode\n#2B Answer\n\n\nPart c.) Using the legend you find at https://maps.elie.ucl.ac.be/CCI/viewer/download/ESACCI-LC-QuickUserGuide-LC-Maps_v2-0-7.pdf reclassify the LULC into a simplified map where 1 = cropland (including any mosaic types that are partially cropland) and 0 = anything else. Plot this using imshow.\n\n\nCode\n# 2C Answer\n\n\nPart d.) Repeat the process for the 2010 LULC map. Using this array with the one from part c, create a new array that records where there was cropland expansion (i.e., there is cropland in 2010 but not in 2000) and where there was cropland abandonment (cropland in 2000 but not in 2010). Save this classification in a single new array. Plot this last array. Optionally, add a legend indicating which values in the array denote expansion and abandonment using some variant of ax.legend()\n\n\nCode\n# 2D Answer\n\n\nPart e.) Overall, did Rwanada see net expansion or contraction of cropland over this period? Looking at the spatial results, do you see any patterns of where this happens? Is it in the north/south, near cities, replacing forests etc.?\n\n\nCode\n#2E Answer"
  },
  {
    "objectID": "Machine_Learning/introduction_big_data.html",
    "href": "Machine_Learning/introduction_big_data.html",
    "title": "11¬† Diving into Machine Learning on Big Data",
    "section": "",
    "text": "Syllabus check-in\nI‚Äôve also added some optional journal articles to Canvas that exemplify the approaches we‚Äôre using\nIn my opinion, this is the most important advance in all of Machine Learning from the perspective of an applied economist.\nOpen Lectures\\02_Machine_Learning\\01_Linear_Regression.ipynb\nMotivation: classify inputs into categories Approach: draw a line (hyperplane) separating observations from different categories\nIssue 1: Many lines might work. Which one should we choose?\nA: Pick the line with the largest ‚Äòmargin‚Äô ‚Äì distance to nearest points on either side\nOpen Lectures\\02_Machine_Learning\\02_SVM.ipynb"
  },
  {
    "objectID": "Machine_Learning/linear_regression.html#exercise-1",
    "href": "Machine_Learning/linear_regression.html#exercise-1",
    "title": "11¬† Linear Regression",
    "section": "11.1 Exercise 1:",
    "text": "11.1 Exercise 1:\nPrint out the mean BMI in the dataset, the sum BMI, and the sum of the squared BMI values. Explain why the sum of the squared BMI is what it is. To do this, you will need to access the right parts of the data array and slice out the right column.\nHINT: You will need to read the DESCR to understand which column the BMI is stored in.\nHINT 2: To create a new variable with just the desired column of the array, you can use Array slicing notation like a = data_array[:, n] where the : means you want ALL rows, and the n means you want just column n.\nHINT 3: You may want to use data_array.sum(), data_array.mean(), and the ** exponent operator.\n\n\nCode\nbmi = data_array[:, 2]\n\nbmi_squared = bmi ** 2\n\nprint('Sum: ', bmi.mean())\nprint('Mean: ', bmi.mean())\nprint('Sum of squared: ', bmi_squared.mean())\n\n\nSum:  -2.2455642172282577e-16\nMean:  -2.2455642172282577e-16\nSum of squared:  0.0022624434389140265\n\n\n\n\nCode\n# For conveinence, sklearn also just has an option to get the key parts for the regression ready to use.\ndiabetes_X, diabetes_y = datasets.load_diabetes(return_X_y=True)\n\n\n\n\nCode\n\n# Look at diabetes_X and notice there are lots of independent variables. Rather than printing the whole\n# Array, which would be messy, just look at the .shape attribute.l\n# print('diabetes_X', diabetes_X)\nprint('diabetes_X', diabetes_X.shape)\n\n\ndiabetes_X (442, 10)\n\n\nFor now, we‚Äôre just going to use a single variable (a single column) for simplicity. The following line extracts just the second column from the array. The colon was necessary because we access arrays using the ROW, COLUMN notation, so we sliced out all ROWS (the colon indicates all) and the second COLUMN.\n\n\nCode\ndiabetes_X = diabetes_X[:, 2]\n# diabetes_X = np.array([diabetes_X])\n# diabetes_X = diabetes_X[:, np.newaxis, 2]\n# print('diabetes_X', diabetes_X)\nprint('diabetes_X', diabetes_X.shape)\n\n# diabetes_X = diabetes_X.reshape((:, 1))\ndiabetes_X = diabetes_X.reshape((diabetes_X.shape[0], 1))\n\n# print('diabetes_X', diabetes_X)\nprint('diabetes_X', diabetes_X.shape)\n\n\ndiabetes_X (442,)\ndiabetes_X (442, 1)\n\n\n\n11.1.1 Split into training and testing arrays (the manual way)\nNext we are going to do a very rudimentary split of the data into training and testing sets using array slice notation. The following lines assigns the last all but the last 20 lines to the TRAIN set and the remaining 20 to the test set.\n\n\nCode\ndiabetes_X_train = diabetes_X[:-20]\ndiabetes_X_test = diabetes_X[-20:]\ndiabetes_y_train = diabetes_y[:-20]\ndiabetes_y_test = diabetes_y[-20:]\n\n\nCreate an empty LinearRegression object.\nIn the lines below, we will follow a relatively standardized process for running a model:\n\nCreate the model object.\nFit the model.\nPredict with the model\n\nThe basic notation for sklearn below first creates a regression model object using the linear_model that we imported above. This model is ‚Äúempty‚Äù in the sense that it has no coefficients identified. Just like othertimes we‚Äôve encountered objects (like numpy array objects), this object has many functions (called methods) and attributes which can be accessed by the dot operator.\n\n\nCode\nregression_object = linear_model.LinearRegression()\nprint('regression_object', regression_object)\n\n\nregression_object LinearRegression()\n\n\n\nUse the fit method\nUse the fit method from our regression object. It takes two inputs, the independent variables (X) and dependent variables (y).\nBelow, we will ONLY use the training subset of the data we created above.\n\n\nCode\nregression_object.fit(diabetes_X_train, diabetes_y_train)\nprint(regression_object)\n\n\nLinearRegression()\n\n\n\n\nUse the fitted model to predict values\nNow the regression_object is ‚Äútrained,‚Äù which means we can also call it‚Äôs predict() method which will take some other observations and (in the case of OLS), multiple the new observations against our trained coefficients to make a prediciton.\nThe predict method returned an array of numerical predictions, which we will look at.\n\n\nCode\ndiabetes_y_pred = regression_object.predict(diabetes_X_test)\nprint(diabetes_y_pred)\n\n\n[225.9732401  115.74763374 163.27610621 114.73638965 120.80385422\n 158.21988574 236.08568105 121.81509832  99.56772822 123.83758651\n 204.73711411  96.53399594 154.17490936 130.91629517  83.3878227\n 171.36605897 137.99500384 137.99500384 189.56845268  84.3990668 ]\n\n\n\n\nLook at the coefficients\nMore interesting might be to look at the coefficients. Once the model has been fit, it has a new attribute .coef_ which stores an array of coefficients. In this case it will only be an array of length 1 because we just have one input.\n\n\nCode\nprint('Coefficients: \\n', regression_object.coef_)\n\n\nCoefficients: \n [938.23786125]\n\n\nYou might be wondering why we are looking at the coefficients as a raw array rather than at a nicely formatted regression table. The reason is in cross-validation approaches, these coefficients might just be one step towards the final model performance check on unseen data.\n\n\nEvaluating the fit\nWe can use sklearn‚Äôs built in evaluation functions, such as for the mean squared error or other metrics.\n\n\nCode\nmse = mean_squared_error(diabetes_y_test, diabetes_y_pred)\nprint('Mean squared error on the TEST data:',  mse)\n\n\nMean squared error on the TEST data: 2548.07239872597\n\n\n\n\nCode\n\n# Or perhaps we want the r2 for the second independent variable (which is the only one we used)\nr2_score_value = r2_score(diabetes_y_test, diabetes_y_pred)\nprint('r2 calculated on TEST data: ', r2_score_value)\n\n\nr2 calculated on TEST data:  0.47257544798227147\n\n\n\n\nCode\n# Finally, to prove to ourselves that we know what we are doing, let's plot this.\nplt.scatter(diabetes_X_test, diabetes_y_test,  color='black')\nplt.plot(diabetes_X_test, diabetes_y_pred, color='blue', linewidth=3)\n\nplt.show()"
  },
  {
    "objectID": "Machine_Learning/linear_regression.html#exercise-2.1-machine-learning-ols-mashup.",
    "href": "Machine_Learning/linear_regression.html#exercise-2.1-machine-learning-ols-mashup.",
    "title": "11¬† Linear Regression",
    "section": "11.2 Exercise 2.1: Machine Learning OLS Mashup.",
    "text": "11.2 Exercise 2.1: Machine Learning OLS Mashup.\nUse loops to find which TWO variables best describe the data, as measured by R-squared. This is a hilariously brute-force approach to OLS model selection, but it is similar in some senses to Machine Learning and will be relevant to the cross-validation approaches we discuss next.\n\n\nCode\n# Exercise 2.1 workspace and starter code\n\n\nfull_dataset = datasets.load_diabetes() # Load the full dataset\ndiabetes_X, diabetes_y = datasets.load_diabetes(return_X_y=True) # Get just the data arrays\n\n# Split into training and testing\ndiabetes_X_train = diabetes_X[:-20]\ndiabetes_X_test = diabetes_X[-20:]\ndiabetes_y_train = diabetes_y[:-20]\ndiabetes_y_test = diabetes_y[-20:]\n\nhighest_score = 0\nfor i in range(len(full_dataset['feature_names'])):\n    for j in range(len(full_dataset['feature_names'])):\n        \n        diabetes_current_X_train = diabetes_X_train[:, [i, j]]\n        diabetes_current_X_test = diabetes_X_test[:, [i, j]]\n\n        # MISSING STUFF HERE.       \n        \n        if r2_score_value &gt; highest_score:\n            highest_score = r2_score_value\n            best_option = [i, j, r2_score_value]\n        \nprint('best_option', best_option)\n        \n        \n\n\nbest_option [0, 0, 0.47257544798227147]"
  },
  {
    "objectID": "Machine_Learning/linear_regression.html#just-for-completeness-lets-look-at-this-the-way-an-econometritian-would",
    "href": "Machine_Learning/linear_regression.html#just-for-completeness-lets-look-at-this-the-way-an-econometritian-would",
    "title": "11¬† Linear Regression",
    "section": "11.3 Just for completeness, let‚Äôs look at this the way an econometritian would",
    "text": "11.3 Just for completeness, let‚Äôs look at this the way an econometritian would\nSklearn doesn‚Äôt report summary statistics in the classic, econometric sense because it focuses on the train, test paradigm, which is not equivilent to a model performance report (which in the classic case is only reporting performance of the TRAINING data).\nNonetheless, Here‚Äôs how I do it, using an alternative, more econometrics-focused package. You will need to conda install statsmodel if you want to uncomment this line and have it work. Note that because we‚Äôre not splitting our data into training and testing, the r-squareds are not really comparable.\n\n\nCode\nimport statsmodels\nfrom statsmodels.api import OLS\n\ndata_with_constant = statsmodels.api.add_constant(full_dataset.data)\nresult = OLS(full_dataset.target, data_with_constant).fit().summary()\nprint(result)\n\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.518\nModel:                            OLS   Adj. R-squared:                  0.507\nMethod:                 Least Squares   F-statistic:                     46.27\nDate:                Thu, 29 Dec 2022   Prob (F-statistic):           3.83e-62\nTime:                        14:01:42   Log-Likelihood:                -2386.0\nNo. Observations:                 442   AIC:                             4794.\nDf Residuals:                     431   BIC:                             4839.\nDf Model:                          10                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst        152.1335      2.576     59.061      0.000     147.071     157.196\nx1           -10.0099     59.749     -0.168      0.867    -127.446     107.426\nx2          -239.8156     61.222     -3.917      0.000    -360.147    -119.484\nx3           519.8459     66.533      7.813      0.000     389.076     650.616\nx4           324.3846     65.422      4.958      0.000     195.799     452.970\nx5          -792.1756    416.680     -1.901      0.058   -1611.153      26.802\nx6           476.7390    339.030      1.406      0.160    -189.620    1143.098\nx7           101.0433    212.531      0.475      0.635    -316.684     518.770\nx8           177.0632    161.476      1.097      0.273    -140.315     494.441\nx9           751.2737    171.900      4.370      0.000     413.407    1089.140\nx10           67.6267     65.984      1.025      0.306     -62.064     197.318\n==============================================================================\nOmnibus:                        1.506   Durbin-Watson:                   2.029\nProb(Omnibus):                  0.471   Jarque-Bera (JB):                1.404\nSkew:                           0.017   Prob(JB):                        0.496\nKurtosis:                       2.726   Cond. No.                         227.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
  },
  {
    "objectID": "Machine_Learning/support_vector_machines.html",
    "href": "Machine_Learning/support_vector_machines.html",
    "title": "13¬† Support Vector Machines",
    "section": "",
    "text": "In this section, we will use what we learned about fitting models and apply it to a very useful machine-learning algorithm.\nFirst let‚Äôs start with imports.\n\n\nCode\nimport numpy as np\nimport scipy\nimport sklearn\nfrom sklearn import datasets\nimport pandas as pd\nimport os\n\n\n\nLoad in some digit image data\nOne of the canonical datasets in sklearn is a series of images of handwritten digits. We‚Äôve imported the datasets above, but now lets load it.\n\n\nCode\n\ndigits = datasets.load_digits()\n\n# First, take a look at the raw python object:\nprint('digits\\n', digits)\n\n\ndigits\n {'data': array([[ 0.,  0.,  5., ...,  0.,  0.,  0.],\n       [ 0.,  0.,  0., ..., 10.,  0.,  0.],\n       [ 0.,  0.,  0., ..., 16.,  9.,  0.],\n       ...,\n       [ 0.,  0.,  1., ...,  6.,  0.,  0.],\n       [ 0.,  0.,  2., ..., 12.,  0.,  0.],\n       [ 0.,  0., 10., ..., 12.,  1.,  0.]]), 'target': array([0, 1, 2, ..., 8, 9, 8]), 'frame': None, 'feature_names': ['pixel_0_0', 'pixel_0_1', 'pixel_0_2', 'pixel_0_3', 'pixel_0_4', 'pixel_0_5', 'pixel_0_6', 'pixel_0_7', 'pixel_1_0', 'pixel_1_1', 'pixel_1_2', 'pixel_1_3', 'pixel_1_4', 'pixel_1_5', 'pixel_1_6', 'pixel_1_7', 'pixel_2_0', 'pixel_2_1', 'pixel_2_2', 'pixel_2_3', 'pixel_2_4', 'pixel_2_5', 'pixel_2_6', 'pixel_2_7', 'pixel_3_0', 'pixel_3_1', 'pixel_3_2', 'pixel_3_3', 'pixel_3_4', 'pixel_3_5', 'pixel_3_6', 'pixel_3_7', 'pixel_4_0', 'pixel_4_1', 'pixel_4_2', 'pixel_4_3', 'pixel_4_4', 'pixel_4_5', 'pixel_4_6', 'pixel_4_7', 'pixel_5_0', 'pixel_5_1', 'pixel_5_2', 'pixel_5_3', 'pixel_5_4', 'pixel_5_5', 'pixel_5_6', 'pixel_5_7', 'pixel_6_0', 'pixel_6_1', 'pixel_6_2', 'pixel_6_3', 'pixel_6_4', 'pixel_6_5', 'pixel_6_6', 'pixel_6_7', 'pixel_7_0', 'pixel_7_1', 'pixel_7_2', 'pixel_7_3', 'pixel_7_4', 'pixel_7_5', 'pixel_7_6', 'pixel_7_7'], 'target_names': array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), 'images': array([[[ 0.,  0.,  5., ...,  1.,  0.,  0.],\n        [ 0.,  0., 13., ..., 15.,  5.,  0.],\n        [ 0.,  3., 15., ..., 11.,  8.,  0.],\n        ...,\n        [ 0.,  4., 11., ..., 12.,  7.,  0.],\n        [ 0.,  2., 14., ..., 12.,  0.,  0.],\n        [ 0.,  0.,  6., ...,  0.,  0.,  0.]],\n\n       [[ 0.,  0.,  0., ...,  5.,  0.,  0.],\n        [ 0.,  0.,  0., ...,  9.,  0.,  0.],\n        [ 0.,  0.,  3., ...,  6.,  0.,  0.],\n        ...,\n        [ 0.,  0.,  1., ...,  6.,  0.,  0.],\n        [ 0.,  0.,  1., ...,  6.,  0.,  0.],\n        [ 0.,  0.,  0., ..., 10.,  0.,  0.]],\n\n       [[ 0.,  0.,  0., ..., 12.,  0.,  0.],\n        [ 0.,  0.,  3., ..., 14.,  0.,  0.],\n        [ 0.,  0.,  8., ..., 16.,  0.,  0.],\n        ...,\n        [ 0.,  9., 16., ...,  0.,  0.,  0.],\n        [ 0.,  3., 13., ..., 11.,  5.,  0.],\n        [ 0.,  0.,  0., ..., 16.,  9.,  0.]],\n\n       ...,\n\n       [[ 0.,  0.,  1., ...,  1.,  0.,  0.],\n        [ 0.,  0., 13., ...,  2.,  1.,  0.],\n        [ 0.,  0., 16., ..., 16.,  5.,  0.],\n        ...,\n        [ 0.,  0., 16., ..., 15.,  0.,  0.],\n        [ 0.,  0., 15., ..., 16.,  0.,  0.],\n        [ 0.,  0.,  2., ...,  6.,  0.,  0.]],\n\n       [[ 0.,  0.,  2., ...,  0.,  0.,  0.],\n        [ 0.,  0., 14., ..., 15.,  1.,  0.],\n        [ 0.,  4., 16., ..., 16.,  7.,  0.],\n        ...,\n        [ 0.,  0.,  0., ..., 16.,  2.,  0.],\n        [ 0.,  0.,  4., ..., 16.,  2.,  0.],\n        [ 0.,  0.,  5., ..., 12.,  0.,  0.]],\n\n       [[ 0.,  0., 10., ...,  1.,  0.,  0.],\n        [ 0.,  2., 16., ...,  1.,  0.,  0.],\n        [ 0.,  0., 15., ..., 15.,  0.,  0.],\n        ...,\n        [ 0.,  4., 16., ..., 16.,  6.,  0.],\n        [ 0.,  8., 16., ..., 16.,  8.,  0.],\n        [ 0.,  1.,  8., ..., 12.,  1.,  0.]]]), 'DESCR': \".. _digits_dataset:\\n\\nOptical recognition of handwritten digits dataset\\n--------------------------------------------------\\n\\n**Data Set Characteristics:**\\n\\n    :Number of Instances: 1797\\n    :Number of Attributes: 64\\n    :Attribute Information: 8x8 image of integer pixels in the range 0..16.\\n    :Missing Attribute Values: None\\n    :Creator: E. Alpaydin (alpaydin '@' boun.edu.tr)\\n    :Date: July; 1998\\n\\nThis is a copy of the test set of the UCI ML hand-written digits datasets\\nhttps://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits\\n\\nThe data set contains images of hand-written digits: 10 classes where\\neach class refers to a digit.\\n\\nPreprocessing programs made available by NIST were used to extract\\nnormalized bitmaps of handwritten digits from a preprinted form. From a\\ntotal of 43 people, 30 contributed to the training set and different 13\\nto the test set. 32x32 bitmaps are divided into nonoverlapping blocks of\\n4x4 and the number of on pixels are counted in each block. This generates\\nan input matrix of 8x8 where each element is an integer in the range\\n0..16. This reduces dimensionality and gives invariance to small\\ndistortions.\\n\\nFor info on NIST preprocessing routines, see M. D. Garris, J. L. Blue, G.\\nT. Candela, D. L. Dimmick, J. Geist, P. J. Grother, S. A. Janet, and C.\\nL. Wilson, NIST Form-Based Handprint Recognition System, NISTIR 5469,\\n1994.\\n\\n.. topic:: References\\n\\n  - C. Kaynak (1995) Methods of Combining Multiple Classifiers and Their\\n    Applications to Handwritten Digit Recognition, MSc Thesis, Institute of\\n    Graduate Studies in Science and Engineering, Bogazici University.\\n  - E. Alpaydin, C. Kaynak (1998) Cascading Classifiers, Kybernetika.\\n  - Ken Tang and Ponnuthurai N. Suganthan and Xi Yao and A. Kai Qin.\\n    Linear dimensionalityreduction using relevance weighted LDA. School of\\n    Electrical and Electronic Engineering Nanyang Technological University.\\n    2005.\\n  - Claudio Gentile. A New Approximate Maximal Margin Classification\\n    Algorithm. NIPS. 2000.\\n\"}\n\n\nNot super helpful unless you‚Äôre very good at reading python dictionary notation. Fortunately, one of the entries in this dataset is a description. Let‚Äôs read that.\n\n\nCode\n\nprint('DESCR\\n', digits['DESCR'])\n\n\nDESCR\n .. _digits_dataset:\n\nOptical recognition of handwritten digits dataset\n--------------------------------------------------\n\n**Data Set Characteristics:**\n\n    :Number of Instances: 1797\n    :Number of Attributes: 64\n    :Attribute Information: 8x8 image of integer pixels in the range 0..16.\n    :Missing Attribute Values: None\n    :Creator: E. Alpaydin (alpaydin '@' boun.edu.tr)\n    :Date: July; 1998\n\nThis is a copy of the test set of the UCI ML hand-written digits datasets\nhttps://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits\n\nThe data set contains images of hand-written digits: 10 classes where\neach class refers to a digit.\n\nPreprocessing programs made available by NIST were used to extract\nnormalized bitmaps of handwritten digits from a preprinted form. From a\ntotal of 43 people, 30 contributed to the training set and different 13\nto the test set. 32x32 bitmaps are divided into nonoverlapping blocks of\n4x4 and the number of on pixels are counted in each block. This generates\nan input matrix of 8x8 where each element is an integer in the range\n0..16. This reduces dimensionality and gives invariance to small\ndistortions.\n\nFor info on NIST preprocessing routines, see M. D. Garris, J. L. Blue, G.\nT. Candela, D. L. Dimmick, J. Geist, P. J. Grother, S. A. Janet, and C.\nL. Wilson, NIST Form-Based Handprint Recognition System, NISTIR 5469,\n1994.\n\n.. topic:: References\n\n  - C. Kaynak (1995) Methods of Combining Multiple Classifiers and Their\n    Applications to Handwritten Digit Recognition, MSc Thesis, Institute of\n    Graduate Studies in Science and Engineering, Bogazici University.\n  - E. Alpaydin, C. Kaynak (1998) Cascading Classifiers, Kybernetika.\n  - Ken Tang and Ponnuthurai N. Suganthan and Xi Yao and A. Kai Qin.\n    Linear dimensionalityreduction using relevance weighted LDA. School of\n    Electrical and Electronic Engineering Nanyang Technological University.\n    2005.\n  - Claudio Gentile. A New Approximate Maximal Margin Classification\n    Algorithm. NIPS. 2000.\n\n\n\n\n\nExtract one of the digits to inspect\nNow that we‚Äôre oriented, also look at one particular image of a digit, just so you know what it actually looks like. Below, we print just the first (index = 0) numeral of the 5620 they provide.\n\n\nCode\n\nprint('digits.images[0]\\n', digits.images[0])\n\n\ndigits.images[0]\n [[ 0.  0.  5. 13.  9.  1.  0.  0.]\n [ 0.  0. 13. 15. 10. 15.  5.  0.]\n [ 0.  3. 15.  2.  0. 11.  8.  0.]\n [ 0.  4. 12.  0.  0.  8.  8.  0.]\n [ 0.  5.  8.  0.  0.  9.  8.  0.]\n [ 0.  4. 11.  0.  1. 12.  7.  0.]\n [ 0.  2. 14.  5. 10. 12.  0.  0.]\n [ 0.  0.  6. 13. 10.  0.  0.  0.]]\n\n\n\n\nCode\n\n# If you squint, maybe you can tel what image it is, but let's plot it to be sure.\nimport matplotlib\nfrom matplotlib import pyplot as plt\nplt.imshow(digits.images[0])\nplt.show()\n\n\n\n\n\nNotice also in the dataset that there is a ‚Äòtargets‚Äô attribute in the dataset. This is the correct numeral that we are trying to make the model predict.\n\n\nCode\nprint('target', digits.target)\n\n\ntarget [0 1 2 ... 8 9 8]\n\n\nOur task now is to train a model that inputs the digit images and predicts the digit numeral. For this, we‚Äôre going to use SVM, as discussed in lecture.\n\n\nImport SVM and create a new (unfitted) model with it.\nFor now, the parameters are going to be manually set (gamme) but we‚Äôll address how to choose them later. Here, I want to illustrate the basic approach used in sklearn to Load, train, fit and predict the model\n\n\nCode\nfrom sklearn import svm\n\n# Create the model object\nclassifier = svm.SVC(gamma=0.001)\n\n\nAt this point, classifier is not yet ‚Äútrained‚Äù, ie. not yet fit to the model. All ML algorithms in SKLEARN have a .fit() method, which we will use here, passing it the images and the targets.\nBefore we train it, we want to split the data into testing and training splits.\nClass question: Remind me WHY are we splitting it here? What is the bad thing that would happen if we just trained it on all of them?\nBefore we can even split the data, however, we need to reshape it to be in the way the regression model expects.\nIn particular, the SVM model needs a 1-dimensional, 64 element array. BUT, the input digits we saw were 2-dimensional, 8 by 8 arrays.\nThis actually leads to a somewhat mind-blown example of how computers ‚Äúthink‚Äù differently than we do. We clearly think about a numeral in 2 dimensional space, but here we see that the computer doesn‚Äôt are about the spatial relationship ship at all. It sees each individual pixel as it‚Äôs own ‚ÄúFeature‚Äù to use the classification parlance. You could even reshuffle the order of those 64 digits and as long as you kept it consistent across the data, it would result in identical predictions.\nLater on, we will talk about machine learning techniques that leverage rather than ignore this 2 dimensional, spatial nature of the data.\nFor now, let‚Äôs just look at the data again. Rather than print it out, I really just want the shape so that i don‚Äôt get inundated with text.\n\n\nCode\nprint('digits.images shape', digits.images.shape)\n\n\ndigits.images shape (1797, 8, 8)\n\n\n\n\nCode\n\nn_samples = len(digits.images)\nn_features = digits.images[0].size\n\nprint('n_samples', n_samples)\nprint('n_features', n_features)\n\ndata = digits.images.reshape((n_samples, n_features))\n\n# Now check the shame again to see that it's right.\nprint('data shape', data.shape)\n\n\nn_samples 1797\nn_features 64\ndata shape (1797, 64)\n\n\n\n\nCode\n\n# Now that we've arranged our data in this shape, we can split it into training and testing sets\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(data, digits.target, test_size=0.5, shuffle=False)\n\nprint('X_train', X_train)\nprint('y_train', y_train)\n\n\nX_train [[ 0.  0.  5. ...  0.  0.  0.]\n [ 0.  0.  0. ... 10.  0.  0.]\n [ 0.  0.  0. ... 16.  9.  0.]\n ...\n [ 0.  0.  2. ... 14.  0.  0.]\n [ 0.  1. 12. ...  0.  0.  0.]\n [ 0.  0.  0. ...  3.  0.  0.]]\ny_train [0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 9 5 5 6 5 0\n 9 8 9 8 4 1 7 7 3 5 1 0 0 2 2 7 8 2 0 1 2 6 3 3 7 3 3 4 6 6 6 4 9 1 5 0 9\n 5 2 8 2 0 0 1 7 6 3 2 1 7 4 6 3 1 3 9 1 7 6 8 4 3 1 4 0 5 3 6 9 6 1 7 5 4\n 4 7 2 8 2 2 5 7 9 5 4 8 8 4 9 0 8 9 8 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7\n 8 9 0 1 2 3 4 5 6 7 8 9 0 9 5 5 6 5 0 9 8 9 8 4 1 7 7 3 5 1 0 0 2 2 7 8 2\n 0 1 2 6 3 3 7 3 3 4 6 6 6 4 9 1 5 0 9 5 2 8 2 0 0 1 7 6 3 2 1 7 3 1 3 9 1\n 7 6 8 4 3 1 4 0 5 3 6 9 6 1 7 5 4 4 7 2 8 2 2 5 5 4 8 8 4 9 0 8 9 8 0 1 2\n 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 9 5 5 6 5 0 9 8 9\n 8 4 1 7 7 3 5 1 0 0 2 2 7 8 2 0 1 2 6 3 3 7 3 3 4 6 6 6 4 9 1 5 0 9 5 2 8\n 2 0 0 1 7 6 3 2 1 7 4 6 3 1 3 9 1 7 6 8 4 3 1 4 0 5 3 6 9 6 1 7 5 4 4 7 2\n 8 2 2 5 7 9 5 4 8 8 4 9 0 8 9 3 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0\n 1 2 3 4 5 6 7 8 9 0 9 5 5 6 5 0 9 8 9 8 4 1 7 7 3 5 1 0 0 2 2 7 8 2 0 1 2\n 6 3 3 7 3 3 4 6 6 6 4 9 1 5 0 9 5 2 8 2 0 0 1 7 6 3 2 1 7 4 6 3 1 3 9 1 7\n 6 8 4 3 1 4 0 5 3 6 9 6 1 7 5 4 4 7 2 8 2 2 5 7 9 5 4 8 8 4 9 0 8 9 8 0 1\n 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 9 5 5 6 5 0 9 8\n 9 8 4 1 7 7 3 5 1 0 0 2 2 7 8 2 0 1 2 6 3 3 7 3 3 4 6 6 6 4 9 1 5 0 9 5 2\n 8 2 0 0 1 7 6 3 2 1 7 4 6 3 1 3 9 1 7 6 8 4 3 1 4 0 5 3 6 9 6 1 7 5 4 4 7\n 2 8 2 2 5 7 9 5 4 8 8 4 9 0 8 9 8 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9\n 0 1 2 3 4 5 6 7 8 9 0 9 5 5 6 5 0 9 8 9 8 4 1 7 7 3 5 1 0 0 2 2 7 8 2 0 1\n 2 6 3 3 7 3 3 4 6 6 6 4 9 1 5 0 9 5 2 8 2 0 0 1 7 6 3 2 1 7 4 6 3 1 3 9 1\n 7 6 8 4 3 1 4 0 5 3 6 9 6 1 7 5 4 4 7 2 8 2 2 5 7 9 5 4 8 8 4 9 0 8 9 8 0\n 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 9 5 5 6 5 0 9\n 8 9 8 4 1 7 7 3 5 1 0 0 2 2 7 8 2 0 1 2 6 3 3 7 3 3 4 6 6 6 4 9 1 5 0 9 5\n 2 8 2 0 0 1 7 6 3 2 1 7 4 6 3 1 3 9 1 7 6 8 4 3 1 4 0 5 3 6 9 6 1 7 5 4 4\n 7 2 8 2 2 5 7 9 5 4]\n\n\n\n\nFit the model\nFinally, now that we‚Äôve split it, we can call the classifier‚Äôs fit method which takes the TRAINING data as input.\n\n\nCode\nclassifier.fit(X_train, y_train)\n\n\nSVC(gamma=0.001)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.SVCSVC(gamma=0.001)\n\n\nNow, our classifier object has it‚Äôs internal parameters fit so that when we give it new input, it predicts what it thinks the correct classification is.\n\n\nCode\npredicted = classifier.predict(X_test)\n\n# Looking at the predicted won't be very intuitive, but you could glance.\nprint('predicted', predicted)\n\n\npredicted [8 8 4 9 0 8 9 8 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 9 6 7 8 9\n 0 9 5 5 6 5 0 9 8 9 8 4 1 7 7 3 9 1 2 7 8 2 0 1 2 6 3 3 7 3 3 4 6 6 6 4 9\n 1 5 0 9 5 2 8 2 0 0 1 7 6 3 2 1 4 6 3 1 3 9 1 7 6 8 4 3 1 4 0 5 3 6 9 6 1\n 7 5 4 4 7 2 8 2 2 5 7 9 5 4 4 9 0 8 9 8 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6\n 7 8 9 0 1 2 3 4 5 6 7 8 9 0 9 5 5 6 5 0 9 8 9 8 4 1 7 7 3 5 1 0 0 7 8 2 0\n 1 2 6 3 3 7 3 3 4 6 6 6 9 9 1 5 0 9 5 2 8 2 0 0 1 7 6 3 2 1 5 4 6 3 1 7 9\n 1 7 6 8 4 3 1 4 0 5 3 6 9 6 1 7 5 4 4 7 2 8 2 2 5 7 9 5 4 8 8 4 9 0 8 9 8\n 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 9 5 5 6 5 0\n 9 8 9 8 4 1 7 7 3 5 1 0 0 2 2 7 8 2 0 1 2 6 3 3 7 3 3 4 6 6 6 4 9 1 5 0 9\n 5 2 8 2 0 0 1 7 6 3 2 2 7 4 6 3 1 3 9 1 7 6 8 4 3 1 4 0 5 3 6 9 6 8 7 5 4\n 4 7 2 8 2 2 5 7 9 5 4 8 8 4 9 0 8 9 8 0 9 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7\n 8 9 0 1 2 3 4 5 6 7 8 9 0 9 5 5 6 5 0 9 8 9 8 4 1 7 7 3 5 1 0 0 2 2 7 8 2\n 0 1 2 6 3 3 7 3 3 4 6 6 6 4 9 1 5 0 9 6 2 8 3 0 0 1 7 6 3 2 1 7 4 6 3 1 3\n 9 1 7 6 8 4 3 1 4 0 5 3 6 9 6 1 7 5 4 4 7 2 8 2 2 5 7 9 5 4 8 8 4 9 0 8 0\n 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 9 5 5 6 5 0 9\n 8 9 8 4 1 7 7 3 5 1 0 0 2 2 7 8 2 0 1 2 6 3 3 7 3 3 4 6 6 6 4 9 1 5 0 9 5\n 2 8 2 0 0 1 7 6 3 2 1 7 4 6 3 1 3 9 1 7 6 8 4 3 1 4 0 5 3 6 9 6 1 7 5 4 4\n 7 2 8 2 2 5 7 9 5 4 8 8 4 9 0 8 9 8 0 1 2 3 4 5 1 7 8 9 0 1 2 3 4 5 6 9 0\n 1 2 3 4 5 6 7 8 9 4 9 5 5 6 5 0 9 8 9 8 4 1 7 7 3 5 1 0 0 2 2 7 8 2 0 1 2\n 6 8 7 7 7 3 4 6 6 6 9 9 1 5 0 9 5 2 8 0 1 7 6 3 2 1 7 9 6 3 1 3 9 1 7 6 8\n 4 3 1 4 0 5 3 6 9 6 1 7 5 4 4 7 2 2 5 7 3 5 9 4 5 0 8 9 8 0 1 2 3 4 5 6 7\n 8 9 0 1 2 8 4 5 6 7 8 9 0 1 2 5 4 5 6 7 8 9 0 9 5 5 6 5 0 9 8 9 8 4 1 7 7\n 7 5 1 0 0 2 2 7 8 2 0 1 2 6 8 8 7 5 8 4 6 6 6 4 9 1 5 0 9 5 2 8 2 0 0 1 7\n 6 3 2 1 7 4 6 3 1 3 9 1 7 6 8 4 5 1 4 0 5 3 6 9 6 1 7 5 4 4 7 2 8 2 2 5 7\n 9 5 4 8 8 4 9 0 8 9 8]\n\n\n\n\nPlot some results\nLet‚Äôs plot a few of them in nicer format. Don‚Äôt worry about learning the plotting code but it‚Äôs a useful example to show the power.\n\n\nCode\n\n_, axes = plt.subplots(2, 4)\nimages_and_labels = list(zip(digits.images, digits.target))\nfor ax, (image, label) in zip(axes[0, :], images_and_labels[:4]):\n    ax.set_axis_off()\n    ax.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')\n    ax.set_title('Training: %i' % label)\n\nimages_and_predictions = list(zip(digits.images[n_samples // 2:], predicted))\nfor ax, (image, prediction) in zip(axes[1, :], images_and_predictions[:4]):\n    ax.set_axis_off()\n    ax.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')\n    ax.set_title('Prediction: %i' % prediction)\nplt.show()\n\n\n\n\n\n\n\nCode\n\nfrom sklearn import metrics\n\nprint(\"Classification report:\\n\", metrics.classification_report(y_test, predicted))\n\n\nClassification report:\n               precision    recall  f1-score   support\n\n           0       1.00      0.99      0.99        88\n           1       0.99      0.97      0.98        91\n           2       0.99      0.99      0.99        86\n           3       0.98      0.87      0.92        91\n           4       0.99      0.96      0.97        92\n           5       0.95      0.97      0.96        91\n           6       0.99      0.99      0.99        91\n           7       0.96      0.99      0.97        89\n           8       0.94      1.00      0.97        88\n           9       0.93      0.98      0.95        92\n\n    accuracy                           0.97       899\n   macro avg       0.97      0.97      0.97       899\nweighted avg       0.97      0.97      0.97       899\n\n\n\n\n\nConfusion matrix\nA more convenient way of looking at the results is t the confusion matrix. This is a built in metric for sklearn. It plots the predicted labels vs.¬†the true labels.\n\n\nCode\ndisp = metrics.plot_confusion_matrix(classifier, X_test, y_test)\ndisp.figure_.suptitle(\"Confusion Matrix\")\n\n# print(\"Confusion matrix:\\n\", disp.confusion_matrix)\n\n# Finally, show it so that you can look at it and see how good we did.\nplt.show()\n\n\nC:\\Users\\jajohns\\AppData\\Local\\mambaforge\\envs\\8222env1\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning:\n\nFunction plot_confusion_matrix is deprecated; Function `plot_confusion_matrix` is deprecated in 1.0 and will be removed in 1.2. Use one of the class methods: ConfusionMatrixDisplay.from_predictions or ConfusionMatrixDisplay.from_estimator.\n\n\n\n\n\n\nQUESTION: Which digit was hardest to categorize and what was it most frequently confused as?"
  },
  {
    "objectID": "Machine_Learning/regularization_intro.html",
    "href": "Machine_Learning/regularization_intro.html",
    "title": "13¬† Regularization Intro",
    "section": "",
    "text": "14 Remember to pull the latest code!\nReminder: HW4 due by midnight tonight.\n\n\nFun trick of the day: You can use Markdown in Google Docs! (have to enable it first)\n\n\n15 Agenda\n\nComment on our hilariously ‚Äúeffective‚Äù p-hacking routine from last lecture.\n\nWe‚Äôll address how can we do this in a legitimate way.\n\nIntroduce regularization and shrinkage\n\nDiscuss why we should do it.\n\nLearn Ridge and LASSO ML algorithms\nApply them in Code\n\n\n\n16 Econometrics versus CV\n\nEconometrics: X, Y not split into testing and training.\n\nY_hat is generated from OLS operating on All Data.\n\nIn the linear_regression notebook, we used Sklearn to automatically solve for the best set of (two) coefficients.\n\nIn this, we took our first baby step to CV by splitting into Training and Testing.\nWe learned SVM applied to this training/testing paradigm.\n\nThis is still not ‚ÄúReal CV‚Äù because we used the Test Data in finding the best set of coefficients.\n\n\n\n\n17 Real CV\n\n\nTherefor, cross-validation creates a second split of the training data.\n\nIteratively solves on this second-split.\n\n\n\n\n\n18 But that process can be extrapolated up to better utilize our (possibly scarce) data\n\n\nKeeping track of splits and folds is hard\n\nWe‚Äôre going to use GridSearchCV to do it all for us.\n\n\n\n\n19 Reminder on the bias-variance tradeoff\n\nReminder: ML lingo uses the word ‚ÄúVariance‚Äù in a slightly different way than we‚Äôre used to:\n\nHere, variance measures the difference in performance between on the training vs testing data\n\nThis is almost unrelated to the more familiar (to us) use of variance to describe a distribution.\n\n\n\n\n\n\n\n20 One way to reduce variance: Shrinkage\n\nShrinkage estimators offer lower variance by construction\n\nIt‚Äôs literally just lowering your beta values.\nThe more we shrink coefficient estimates, the lower their variance across samples.\n\nTo build intuition on this, start with the extreme case: shrink all slope estimates to zero, make constant predictions.\n\nPerformance would be constant!\n\n\n\n\n21 Quick example and implications for bias\n\n\n22 Starting with OLS\n\nWhen we have many observations, OLS does a good job\n\nWhy? Because with tons of observations, it is more likely the training data reflect other possible data.\n\nE.g., suppose we have measurements of weight and size of mice. Fits pretty well.\n\n\n\n\n\n\n23 But what if we have fewer data?\n\nSuppose we only get to fit the data on a small subset (red)\n\nThe resulting estimate is not similar to the full data in green\n\n\n\n\n\n24 Variance on out-of-sample observations is huge\n\n\nOur predicted line will have minimum bias on the training data\n\nIt‚Äôs the ‚ÄúBest Linear Unbiased Estimate‚Äù (BLUE)\n\nBut, there is huge variance on green.\nMain idea of regularization:\n\nWhat if we shrink the COEFFICIENT value in the OLS regression\nThis will obviously introduce more bias\n\nBut, as we will see, this almost always reduces out-of-sample variance\n\nThis is also referred to as ‚Äúshrinkage‚Äù\n\n\n\n\n25 \n\n\n\nWhy did we just do this?\nIn general (VERY VERY OFTEN) shrinking our coefficients will improve out-of-sample performance\n\nIt is so important that in big data and ML applications, almost everything is regularized in some way.\n\n\n\n\n26 Shrinkage estimators: general form\n\n\n27 Shrinkage estimators: Lagrangian form\n\n\n28 Ridge regression\n\n\n29 Ridge Regression intuition\n\n\n30 Solution to Ridge\n\n\n31 Note about coefficient scaling\n\n\n32 LASSO: Least Absolute Shrinkage and Selection Operator\n\n\n\n33 LASSO: Not just another penalty.\n\n\n34 LASSO and variable selection\n\nWhen solved, LASSO may result in estimates of exactly zero for some parameters.\n\nIn other words, LASSO shrinks parameter estimates, and shrinks some to zero\nThis results in automated variable selection .\n\nSwitch to VS Code, notebook lectures/03_Regulariation/01_ridge_and_lasso.ipynb\n\n\n\n35 So what did we just do?\n\n\n36 Solution: reduce bias by using OLS after\n\n‚Äúpost-LASSO regression‚Äù\n\nUse LASSO to estimate the main model, letting it do variable selection\nEstimate the main model with selected variables from step 1. This time use OLS.\n\nSwitch back to our notebook where we will implement this.\n\n\n\n37 Appendix\n\n\n38 Elastic Net: LASSO with a ridge\n\n\n39 IV and LASSO\n\n\n40 Motivation\n\n\n41 Many possible instruments in big data contexts\n\n\n42 How many instruments?\n\nAsymptotic theory:\n\nIF all instruments are exogenous and relevant, then they contain useful information.\nFor the most efficient (low variance) estimator, we should use all available information. Question is just how to combine it.\n2SLS can help recover optimal combination of those instruments.\n\nBut, we often have finite samples\nAnd, having too many instruments can introduce bias!\n\nHence, we need some well-thought-out model selection approach.\n\n\n\n\n43 Correct for overfitting-bias with LASSO\n\nIf bias arises from overfitting the first stage, we could try to limit overfitting.\n\nBelloni, Chen, Chernozhukov, Hansen (2012 Econometrica): LASSO!\n\nRough idea:\n\nEstimate first stage via LASSO (with specific penalization scheme)\nUse instruments selected via LASSO in first stage estimated via OLS\nUse fitted values in second stage\n\n\n\n\n44 Many instruments, many controls\n\n\n45 IV and not LASSO\nThe world isn‚Äôt always sparse\n\n\n46 Other options for the first stage\n\n\n47 Deep IV: setup\n\n\n48 Deep IV: idea"
  },
  {
    "objectID": "Machine_Learning/regularization_with_lasso.html#automatic-feature-selection-with-lasso-regression",
    "href": "Machine_Learning/regularization_with_lasso.html#automatic-feature-selection-with-lasso-regression",
    "title": "14¬† Regularization with Lasso",
    "section": "14.1 Automatic feature selection with LASSO regression",
    "text": "14.1 Automatic feature selection with LASSO regression\nIn this notebook we will learn how LASSO (Least Absolute Shrinkage and Selection Operator) regression works and how it can assist in automatically selecting which variables should be included using a Cross-Validation perspective.\n\nStart by importing packages\n\n\nCode\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom sklearn.datasets import load_diabetes\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn import datasets, linear_model\nfrom sklearn.linear_model import LassoCV, Lasso\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import mean_squared_error, r2_score\n\nimport statsmodels\nfrom statsmodels.api import OLS\n\n\n\n\nLoad dataset and inspect it\nAgain we‚Äôre going to use our diabetes dataset. Inspect it again just to remind yourself what is in it.\n\n\nCode\ndiabetes = load_diabetes()\n\nX = diabetes.data\ny = diabetes.target\n\nfeature_names = diabetes.feature_names\n\nprint(diabetes['DESCR'])\nprint(feature_names)\n\n\n.. _diabetes_dataset:\n\nDiabetes dataset\n----------------\n\nTen baseline variables, age, sex, body mass index, average blood\npressure, and six blood serum measurements were obtained for each of n =\n442 diabetes patients, as well as the response of interest, a\nquantitative measure of disease progression one year after baseline.\n\n**Data Set Characteristics:**\n\n  :Number of Instances: 442\n\n  :Number of Attributes: First 10 columns are numeric predictive values\n\n  :Target: Column 11 is a quantitative measure of disease progression one year after baseline\n\n  :Attribute Information:\n      - age     age in years\n      - sex\n      - bmi     body mass index\n      - bp      average blood pressure\n      - s1      tc, total serum cholesterol\n      - s2      ldl, low-density lipoproteins\n      - s3      hdl, high-density lipoproteins\n      - s4      tch, total cholesterol / HDL\n      - s5      ltg, possibly log of serum triglycerides level\n      - s6      glu, blood sugar level\n\nNote: Each of these 10 feature variables have been mean centered and scaled by the standard deviation times the square root of `n_samples` (i.e. the sum of squares of each column totals 1).\n\nSource URL:\nhttps://www4.stat.ncsu.edu/~boos/var.select/diabetes.html\n\nFor more information see:\nBradley Efron, Trevor Hastie, Iain Johnstone and Robert Tibshirani (2004) \"Least Angle Regression,\" Annals of Statistics (with discussion), 407-499.\n(https://web.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf)\n\n['age', 'sex', 'bmi', 'bp', 's1', 's2', 's3', 's4', 's5', 's6']\n\n\n\n\nSelect subset of data\nTo speed up calculation, we‚Äôre going to just use the first 150 observations using numpy slice notation to grab them out of the X, y\n\n\nCode\nX = X[:150]\ny = y[:150]\n\nprint(X)\n\n\n[[ 0.03807591  0.05068012  0.06169621 ... -0.00259226  0.01990749\n  -0.01764613]\n [-0.00188202 -0.04464164 -0.05147406 ... -0.03949338 -0.06833155\n  -0.09220405]\n [ 0.08529891  0.05068012  0.04445121 ... -0.00259226  0.00286131\n  -0.02593034]\n ...\n [-0.05637009 -0.04464164  0.09295276 ...  0.02545259  0.02606052\n   0.04034337]\n [-0.06000263  0.05068012  0.01535029 ... -0.00259226 -0.03074792\n  -0.0010777 ]\n [-0.04910502  0.05068012 -0.00512814 ...  0.07120998  0.06123763\n  -0.03835666]]\n\n\n\n\nRun OLS first (for comparison)\nRemember the standard Sklearn model steps:\n\ncreate the model object\ncall the object‚Äôs fit method.\nuse the fitted model to predict something.\nassess the predictions.\n\n\n\nCode\n# Create linear regression object\nmodel_ols = linear_model.LinearRegression()\n\n# Train the model using the training sets\nmodel_ols.fit(X, y)\n\n# Make predictions using the testing set\ny_hat = model_ols.predict(X)\n\n# The coefficients\nprint(\"Coefficients: \\n\", model_ols.coef_)\n\n# The mean squared error\nprint(\"Mean squared error:\", mean_squared_error(y, y_hat))\n\n# The coefficient of determination: 1 is perfect prediction\nprint(\"Coefficient of determination:\", r2_score(y, y_hat))\n\n\nCoefficients: \n [ -67.3322587  -369.98803486  445.91969019  324.49756622   89.12828579\n -370.37260059 -263.56792004  123.19006966  579.0388831    89.90418524]\nMean squared error: 2662.075876125911\nCoefficient of determination: 0.5298596601593836\n\n\n\n\nDo it again in the econometrics style\nRecall that the package statsmodels is closer to the econometrician‚Äôs way of doing things. We‚Äôre going to quickly repeat the steps above but with Statsmodels so we can view it in a nice table form.\n\n\nCode\nx_with_constant = statsmodels.api.add_constant(X)\nresult = OLS(y, x_with_constant).fit().summary()\n\nprint(result)\n\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.530\nModel:                            OLS   Adj. R-squared:                  0.496\nMethod:                 Least Squares   F-statistic:                     15.67\nDate:                Thu, 29 Dec 2022   Prob (F-statistic):           1.54e-18\nTime:                        14:05:48   Log-Likelihood:                -804.36\nNo. Observations:                 150   AIC:                             1631.\nDf Residuals:                     139   BIC:                             1664.\nDf Model:                          10                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst        152.9381      4.568     33.483      0.000     143.907     161.969\nx1           -67.3323    106.167     -0.634      0.527    -277.243     142.579\nx2          -369.9880    112.515     -3.288      0.001    -592.451    -147.525\nx3           445.9197    119.643      3.727      0.000     209.365     682.475\nx4           324.4976    117.440      2.763      0.007      92.298     556.697\nx5            89.1283    833.489      0.107      0.915   -1558.827    1737.084\nx6          -370.3726    694.067     -0.534      0.594   -1742.667    1001.922\nx7          -263.5679    398.014     -0.662      0.509   -1050.513     523.377\nx8           123.1901    274.716      0.448      0.655    -419.972     666.352\nx9           579.0389    303.265      1.909      0.058     -20.570    1178.648\nx10           89.9042    105.344      0.853      0.395    -118.380     298.189\n==============================================================================\nOmnibus:                        0.798   Durbin-Watson:                   1.871\nProb(Omnibus):                  0.671   Jarque-Bera (JB):                0.431\nSkew:                          -0.042   Prob(JB):                        0.806\nKurtosis:                       3.248   Cond. No.                         266.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\nPlot y and y_hat\nLet‚Äôs also plot y and y_hat compared to one of the most important variables, BMI. We‚Äôll see both y and y_hat resemble each other.\n\n\nCode\n# Plot outputs (comparing 1 variable (BMI in column 3) to y and y_hat\nplt.scatter(X[:, 3], y, color=\"black\")\nplt.scatter(X[:, 3], y_hat, color=\"blue\")\n\n\n&lt;matplotlib.collections.PathCollection at 0x1a2e0cf3b50&gt;"
  },
  {
    "objectID": "Machine_Learning/regularization_with_lasso.html#switch-to-lasso",
    "href": "Machine_Learning/regularization_with_lasso.html#switch-to-lasso",
    "title": "14¬† Regularization with Lasso",
    "section": "14.2 Switch to LASSO",
    "text": "14.2 Switch to LASSO\nNow that we‚Äôve spent all this time setting up our python environment and getting sklearn, it‚Äôs almost a trivial step in many cases to try out the latest-and-greatest model.\n\nCreate a LASSO model object\nToday‚Äôs goal, however, is to do Lasso on this same dataset. To start, lets create a Lasso object. Notice that we are not setting the alpha/gamma value when we create it.\n\n\nCode\nmodel_lasso = Lasso(alpha=1.0, random_state=0, max_iter=10000) # Note, alpha is set by default to 1.0 so we could have omitted it here (though I kept it in to make it clear)\nprint(model_lasso)\n\n\nLasso(max_iter=10000, random_state=0)\n\n\n\n\nFit the LASSO\nCall the lasso.fit() method.\n\n\nCode\nmodel_lasso.fit(X, y)\nprint(model_lasso)\n\n\nLasso(max_iter=10000, random_state=0)\n\n\n\n\nCode\ny_hat_lasso = model_lasso.predict(X)\nprint('y_hat_lasso', y_hat_lasso)\n\n\ny_hat_lasso [172.63694312 112.56436625 162.13985803 156.08973773 129.74383298\n 125.28140937 115.61884338 136.59052179 159.6287421  185.05063898\n 106.82661307 118.62966678 132.01651185 164.27673474 132.32978307\n 159.52719536 180.05860468 163.52345725 141.12618691 142.73726374\n 132.32597226 118.12828252 126.6122036  214.79358554 149.32105261\n 154.5286222  115.96885102 158.72385667 145.06984361 171.08808536\n 150.95110473 120.84369573 180.56864605 131.55817157 113.02543458\n 150.9215012  176.31531879 159.41171407 194.18582819 160.99349743\n 153.08379216 115.57213577 144.18353926 128.89860148 178.86126303\n 136.35519126 144.6493225  126.12767637 118.19733477 167.06059041\n 140.67767067 153.45410491 141.38478192 134.18782266 141.05570494\n 113.76852531 172.85209994 114.2633392  134.72604324 158.58764454\n 130.25790072 165.17534412 117.92607201 129.94775178 137.1011572\n 160.31880191 146.17550981 142.41117762 138.4133546  120.17269536\n 110.18834819 168.45389802 180.95757677 143.6382227  152.37093049\n 145.2863145  145.72204735 114.03536748 152.50980767 115.92610137\n 162.78887649 142.77074648 115.74271899 146.82848624 115.40891135\n 152.17529034  96.39049597 164.92236812 127.76402029 129.58254816\n 117.3524975  176.77457893 156.18693227 122.55104724 130.97049958\n 136.93397481 175.47713603 172.74225341 146.90826663 139.35118138\n 168.49953863 128.13758671 155.27499811 156.22880933 143.1622842\n 138.64434869 109.92229447 142.60610808 178.34497712 138.46006222\n  95.66797317 142.68220906 146.5611342  171.24021892 213.87120181\n 181.99208822 172.64335057 174.81696371 155.57294193 149.30073174\n 136.6525253  162.03454774 181.96440315 164.55326954 150.19022137\n 178.46763855 100.6840604  151.64942745 124.00006657 168.88710262\n 181.94923866 116.17042024 140.48211467 110.62902725 150.69250972\n 182.86260255 106.01067503 168.7211344  187.55905836 185.62473912\n 141.42848293 178.66151776 170.32853183 136.40099477 163.90515518\n 175.27393727 170.22208613 182.43176328 142.61890724 172.02344691]\n\n\n\n\nPlot it too to compare it with the OLS plot from above\nWhat do you see. Is this expected?\n\n\nCode\n# Plot outputs\nplt.scatter(X[:, 3], y, color=\"black\")\nplt.scatter(X[:, 3], y_hat_lasso, color=\"blue\")\n\nplt.show()\n\n\n\n\n\n\n\nCompare the actual coefficients created\nClass question: How are they different? And how are they similar?\n\n\nCode\nprint(model_lasso.coef_)\nprint(model_ols.coef_)\n\n\n[  0.          -0.         239.9258791    0.          -0.\n  -0.          -0.           0.         373.07866685   0.        ]\n[ -67.3322587  -369.98803486  445.91969019  324.49756622   89.12828579\n -370.37260059 -263.56792004  123.19006966  579.0388831    89.90418524]"
  },
  {
    "objectID": "Machine_Learning/regularization_with_lasso.html#exercise-1",
    "href": "Machine_Learning/regularization_with_lasso.html#exercise-1",
    "title": "14¬† Regularization with Lasso",
    "section": "14.3 Exercise 1",
    "text": "14.3 Exercise 1\nUse a loop to identify the best value of alpha, as measured by r-squared.\nWrite all of the alphas and associated r2 into a dictionary\nDiscussion question for once you‚Äôre done: what was the optimal alpha and why does this make sense? How does this compare to OLS? Why is it that way?\n# Starter code: keyt parts omitted.\nscores = {}\nalphas = np.logspace(-5, -0.05, 30)\nfor SOMETHING in SOMETHING_ELSE:\n    model_lasso = Lasso(alpha=alpha, random_state=0, max_iter=10000)\n    # LINE OMIITTED\n    # LINE OMIITTED\n    r2 = r2_score(y, y_hat_lasso)\n    print('R2 for alpha ' + str(alpha) + ': ' + str(r2))\n    scores.append(r2)\n\n# Quick way to get the value from the highest-valued dictionary entry\nbest_alpha = max(scores, key=scores.get)"
  },
  {
    "objectID": "Machine_Learning/regularization_with_lasso.html#exercise-1-answer",
    "href": "Machine_Learning/regularization_with_lasso.html#exercise-1-answer",
    "title": "14¬† Regularization with Lasso",
    "section": "14.4 Exercise 1 Answer",
    "text": "14.4 Exercise 1 Answer\n\n\nCode\n# Exercise 1 Answer Code\nscores = {}\nalphas = np.logspace(-5, -0.05, 30)\nfor alpha in alphas:\n    model_lasso = Lasso(alpha=alpha, random_state=0, max_iter=10000)\n    model_lasso.fit(X, y)\n    y_hat_lasso = model_lasso.predict(X)\n    r2 = r2_score(y, y_hat_lasso)\n    scores[alpha] = r2\n\n# Quick way to get the value from the highest-valued dictionary entry\nbest_alpha = max(scores, key=scores.get)\n\nprint('best_alpha', best_alpha)\n\n\nbest_alpha 1e-05"
  },
  {
    "objectID": "Machine_Learning/regularization_with_lasso.html#operationalizing-cv-with-gridsearch",
    "href": "Machine_Learning/regularization_with_lasso.html#operationalizing-cv-with-gridsearch",
    "title": "14¬† Regularization with Lasso",
    "section": "14.5 Operationalizing CV with GridSearch",
    "text": "14.5 Operationalizing CV with GridSearch\nIt seems a little weird to be automatically finding the best model. If we were just applying this to the dataset a single time, this would indeed be p-hacking to the extreme. However, showing its performance on UNSEEN data is quite the opposite of p-hacking.\nHere, we‚Äôre going to operationalize our method for finding th ebest model by using GridSearch. We are going to test a variety of different alphas, similar to above. Define them here using numpy logspace:\n\n\nCode\nalphas = np.logspace(-3, -0.5, 30)\nalphas\n\n\narray([0.001     , 0.00121957, 0.00148735, 0.00181393, 0.00221222,\n       0.00269795, 0.00329034, 0.00401281, 0.0048939 , 0.00596846,\n       0.00727895, 0.0088772 , 0.01082637, 0.01320352, 0.01610262,\n       0.01963828, 0.02395027, 0.02920904, 0.03562248, 0.04344412,\n       0.05298317, 0.06461671, 0.07880463, 0.0961078 , 0.11721023,\n       0.14294613, 0.17433288, 0.21261123, 0.25929438, 0.31622777])\n\n\nWe are going to be passing this range of tuning parameters to a GridSearch function that will test which works best when cross-validation methods are applied. First though, we have to put the alphas into the form the GridSearchCV funciton Expects, which is a list of dictionaries.\n\n\nCode\ntuning_parameters = [{'alpha': alphas}]\n\n\nRecall that CV works by calculating the fit quality of different folds of the training data. Here we will just use 5 folds. GridSearchCV will automatically implement the folding and testing logic.\n\n\nCode\nn_folds = 5\n\n\n\nCreate the lasso_cv object from the lasso object\nFinally, we have all our objects ready to pass to the GridSearchVC function which will Give us back a classifier object. Notice that we‚Äôre reusing that model_lasso objectg we created above. The difference is that we will be systematically handing different parameters from the tuning_parameters list into the model_lasso object.\n\n\nCode\nmodel_lasso_cv = GridSearchCV(model_lasso, tuning_parameters, cv=n_folds, refit=False)\n\n\n\n\nFit the lasso_cv object\nWhen we call the model_lasso_cv.fit() method, we will iteratively be calling the Lasso.fit() with different permutations of tuned parameters and then will return the classifier with the best CV fit.\n\n\nCode\nmodel_lasso_cv.fit(X, y)\n\n\nGridSearchCV(cv=5,\n             estimator=Lasso(alpha=0.8912509381337456, max_iter=10000,\n                             random_state=0),\n             param_grid=[{'alpha': array([0.001     , 0.00121957, 0.00148735, 0.00181393, 0.00221222,\n       0.00269795, 0.00329034, 0.00401281, 0.0048939 , 0.00596846,\n       0.00727895, 0.0088772 , 0.01082637, 0.01320352, 0.01610262,\n       0.01963828, 0.02395027, 0.02920904, 0.03562248, 0.04344412,\n       0.05298317, 0.06461671, 0.07880463, 0.0961078 , 0.11721023,\n       0.14294613, 0.17433288, 0.21261123, 0.25929438, 0.31622777])}],\n             refit=False)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCVGridSearchCV(cv=5,\n             estimator=Lasso(alpha=0.8912509381337456, max_iter=10000,\n                             random_state=0),\n             param_grid=[{'alpha': array([0.001     , 0.00121957, 0.00148735, 0.00181393, 0.00221222,\n       0.00269795, 0.00329034, 0.00401281, 0.0048939 , 0.00596846,\n       0.00727895, 0.0088772 , 0.01082637, 0.01320352, 0.01610262,\n       0.01963828, 0.02395027, 0.02920904, 0.03562248, 0.04344412,\n       0.05298317, 0.06461671, 0.07880463, 0.0961078 , 0.11721023,\n       0.14294613, 0.17433288, 0.21261123, 0.25929438, 0.31622777])}],\n             refit=False)estimator: LassoLasso(alpha=0.8912509381337456, max_iter=10000, random_state=0)LassoLasso(alpha=0.8912509381337456, max_iter=10000, random_state=0)\n\n\nThe classifier object now has a variety of diagnostic metrics, reporting back on different folds within the Cross Validation. Take a look at them below.\n\n\nCode\nprint('model_lasso_cv keys returned:', model_lasso_cv.cv_results_.keys())\n\n\nmodel_lasso_cv keys returned: dict_keys(['mean_fit_time', 'std_fit_time', 'mean_score_time', 'std_score_time', 'param_alpha', 'params', 'split0_test_score', 'split1_test_score', 'split2_test_score', 'split3_test_score', 'split4_test_score', 'mean_test_score', 'std_test_score', 'rank_test_score'])\n\n\nSome relevant results are as below, which we‚Äôll extract and assign to lists.\n\n\nCode\nscores = model_lasso_cv.cv_results_['mean_test_score']\nscores_std = model_lasso_cv.cv_results_['std_test_score']\n\nprint('scores', scores)\nprint('scores_std', scores_std)\n\n\nscores [0.39984741 0.40067546 0.40167546 0.40287999 0.40432758 0.4056638\n 0.40723916 0.40908292 0.41123606 0.41372907 0.41687352 0.41965693\n 0.42092208 0.42175014 0.42260226 0.42355228 0.42456133 0.42570452\n 0.42696662 0.42907498 0.4315905  0.43258464 0.43298746 0.43209091\n 0.42958333 0.42560295 0.41929547 0.40929219 0.39589098 0.37593936]\nscores_std [0.11754727 0.11729368 0.11699041 0.11662935 0.11620183 0.11559979\n 0.11487707 0.11400269 0.11298083 0.11180212 0.11051429 0.10929125\n 0.10826799 0.10702882 0.10566315 0.10406309 0.10222666 0.10004269\n 0.0974447  0.09440138 0.09082211 0.08770956 0.08378049 0.07835987\n 0.07219072 0.06557553 0.05779547 0.0494723  0.04611854 0.04981942]"
  },
  {
    "objectID": "Machine_Learning/regularization_with_lasso.html#exercise-2",
    "href": "Machine_Learning/regularization_with_lasso.html#exercise-2",
    "title": "14¬† Regularization with Lasso",
    "section": "14.6 Exercise 2:",
    "text": "14.6 Exercise 2:\nWith your table, explore the scores and alphas lists we‚Äôve created. Identify which alpha is the best, based on the MSE score returned. A challenge here is that sklearn gave us the scores as a list rather than a dictionary (as we built above), so you will need to use the list to create the dictionary.\nOne way to consider doing this would be to create a for loop to iterate through a range(len(scores)): object, saving the alphas and scores to a new dictionary, as in the starter code below.\nSave the optimal alpha as a new variable called chosen_alpha.\noutput_dict = {}\nfor i in OMITTED_CODE:\n    output_dict[alphas[i]] = scores[i]\n    \nbest_alpha = max(output_dict, key=output_dict.get)\n\nprint('best_alpha', best_alpha)"
  },
  {
    "objectID": "Machine_Learning/regularization_with_lasso.html#exercise-2-answer",
    "href": "Machine_Learning/regularization_with_lasso.html#exercise-2-answer",
    "title": "14¬† Regularization with Lasso",
    "section": "14.7 Exercise 2 Answer",
    "text": "14.7 Exercise 2 Answer\n\n\nCode\n# Exercise 2 Code\n\noutput_dict = {}\nfor i in range(len(scores)):\n    output_dict[alphas[i]] = scores[i]\n    \nbest_alpha = max(output_dict, key=output_dict.get)\n\nprint('best_alpha', best_alpha)\n\n\n\nbest_alpha 0.07880462815669913\n\n\n\nUse the built-in attributes to get the best alpha\nFortunately, the authors provide a useful best_params_ attribute.\n\n\nCode\nprint('best_parameters:', model_lasso_cv.best_params_)\n\n\nbest_parameters: {'alpha': 0.07880462815669913}\n\n\nExtract the best alpha, which we will use later.\n\n\nCode\nchosen_alpha = model_lasso_cv.best_params_['alpha']\nprint('chosen_alpha', chosen_alpha)\n\n\nchosen_alpha 0.07880462815669913\n\n\n\n\nRerun LASSO with the best alpha\nNow we can rerun a vanilla (no CV) version of Lasso with that specific alpha. This will return, for instance, a .coef_ list.\n\n\nCode\nmodel_lasso_cv_2 = Lasso(alpha=chosen_alpha, random_state=0, max_iter=10000).fit(X, y)\n\nprint(\"coefficients\", model_lasso_cv_2.coef_)\n\n\ncoefficients [ -26.87410362 -318.07808453  427.47324303  272.29570713   -0.\n -181.31265355 -262.37184106    0.          613.14932629   71.09561387]\n\n\nSimply looking at the coefficients tells us which are to be included. Question: How will we know just by looking?\n\n\nExtract the feature names and colum indices of the features that Lasso has selected.\n\n\nCode\nselected_coefficient_labels = []\nselected_coefficient_indices = []\nfor i in range(len(model_lasso_cv_2.coef_)):\n    print('Coefficient', feature_names[i], 'was', model_lasso_cv_2.coef_[i])\n    if abs(model_lasso_cv_2.coef_[i]) &gt; 0:\n        selected_coefficient_labels.append(feature_names[i])\n        selected_coefficient_indices.append(i)\n\n\nCoefficient age was -26.874103620938673\nCoefficient sex was -318.0780845318727\nCoefficient bmi was 427.4732430327188\nCoefficient bp was 272.2957071277454\nCoefficient s1 was -0.0\nCoefficient s2 was -181.31265355198863\nCoefficient s3 was -262.371841059376\nCoefficient s4 was 0.0\nCoefficient s5 was 613.1493262893765\nCoefficient s6 was 71.09561386767885\n\n\nThis process led us to the following selected_coefficient_labels:\n\n\nCode\nprint('selected_coefficient_labels', selected_coefficient_labels)\n\n\nselected_coefficient_labels ['age', 'sex', 'bmi', 'bp', 's2', 's3', 's5', 's6']\n\n\n\n\nPlot the scores versus the alphas\nFor fun, let‚Äôs plot the alphas, scores and a confidence range. What does this show us about the optimal alpha and how it varies with score?\n\n\nCode\nplt.figure().set_size_inches(8, 6)\nplt.semilogx(alphas, scores)\n\n\n\n\n\nA fun aspect of the k-fold approach is you can get a measure of the std_errors involved. Plot those below.\n\n\nCode\nstd_error = scores_std / np.sqrt(n_folds)\n\nplt.semilogx(alphas, scores + std_error, 'b--')\nplt.semilogx(alphas, scores - std_error, 'b--')\n\n\n\n\n\n\n\nPlot the confidence band and the maximum score\nalpha=0.2 controls the translucency of the fill color\n\n\nCode\nplt.fill_between(alphas, scores + std_error, scores - std_error, alpha=0.2)\n\nplt.ylabel('CV score +/- std error')\nplt.xlabel('alpha')\nplt.axhline(np.max(scores), linestyle='--', color='.5')\nplt.xlim([alphas[0], alphas[-1]])\n\nplt.show()"
  },
  {
    "objectID": "Machine_Learning/regularization_with_lasso.html#switch-back-to-slides",
    "href": "Machine_Learning/regularization_with_lasso.html#switch-back-to-slides",
    "title": "14¬† Regularization with Lasso",
    "section": "14.8 Switch back to slides",
    "text": "14.8 Switch back to slides"
  },
  {
    "objectID": "Machine_Learning/regularization_with_lasso.html#post-lasso",
    "href": "Machine_Learning/regularization_with_lasso.html#post-lasso",
    "title": "14¬† Regularization with Lasso",
    "section": "14.9 Post-LASSO",
    "text": "14.9 Post-LASSO\nFinally, now that we have our selected labels, we can use them to select the numpy array columns that we want to use for a post-LASSO run.\n\n\nCode\nnew_x = X[:, selected_coefficient_indices]\nnew_x = statsmodels.api.add_constant(new_x)\nprint('new_x', new_x)\n\n\nnew_x [[ 1.          0.03807591  0.05068012 ... -0.04340085  0.01990749\n  -0.01764613]\n [ 1.         -0.00188202 -0.04464164 ...  0.07441156 -0.06833155\n  -0.09220405]\n [ 1.          0.08529891  0.05068012 ... -0.03235593  0.00286131\n  -0.02593034]\n ...\n [ 1.         -0.05637009 -0.04464164 ... -0.02867429  0.02606052\n   0.04034337]\n [ 1.         -0.06000263  0.05068012 ...  0.019187   -0.03074792\n  -0.0010777 ]\n [ 1.         -0.04910502  0.05068012 ... -0.06917231  0.06123763\n  -0.03835666]]\n\n\nPlug this new x matrix into our statsmodels OLS function and print that out.\n\n\nCode\nresult = OLS(y, new_x).fit().summary()\nprint(result)\n\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.529\nModel:                            OLS   Adj. R-squared:                  0.502\nMethod:                 Least Squares   F-statistic:                     19.79\nDate:                Thu, 29 Dec 2022   Prob (F-statistic):           8.67e-20\nTime:                        14:05:51   Log-Likelihood:                -804.49\nNo. Observations:                 150   AIC:                             1627.\nDf Residuals:                     141   BIC:                             1654.\nDf Model:                           8                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst        152.8679      4.503     33.944      0.000     143.965     161.771\nx1           -66.9996    105.470     -0.635      0.526    -275.506     141.507\nx2          -366.0229    111.491     -3.283      0.001    -586.432    -145.614\nx3           444.0490    118.802      3.738      0.000     209.186     678.912\nx4           318.1611    115.915      2.745      0.007      89.005     547.317\nx5          -231.1543    104.191     -2.219      0.028    -437.132     -25.177\nx6          -296.7442    110.058     -2.696      0.008    -514.321     -79.168\nx7           635.5421    123.804      5.133      0.000     390.790     880.294\nx8            98.0388    103.296      0.949      0.344    -106.171     302.248\n==============================================================================\nOmnibus:                        0.803   Durbin-Watson:                   1.880\nProb(Omnibus):                  0.669   Jarque-Bera (JB):                0.437\nSkew:                          -0.051   Prob(JB):                        0.804\nKurtosis:                       3.244   Cond. No.                         35.8\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
  },
  {
    "objectID": "Machine_Learning/regularization_with_lasso.html#class-discussion",
    "href": "Machine_Learning/regularization_with_lasso.html#class-discussion",
    "title": "14¬† Regularization with Lasso",
    "section": "14.10 Class discussion",
    "text": "14.10 Class discussion\nHow does the r-squared of this model compare to the one we did at the start of the lecture?\nGiven the above, how is the LASSO approach better than a vanilla OLS?\nLook at the adjusted R-squared. How does that compare across models. In what ways is the adjusted R-squared similar the CV approach?"
  },
  {
    "objectID": "Machine_Learning/neural_networks_intro.html",
    "href": "Machine_Learning/neural_networks_intro.html",
    "title": "15¬† Neural networks",
    "section": "",
    "text": "Multi-layer perceptrons and convolutional neural nets.\n\n\n16 Logistical comments and agenda\n\nAfter today, Ali will resume lectures for 2 more days\nAgenda:\n\nNeural-nets example\nNN theory\nNN in python\n\n\n\n\n17 \nhttps://www.youtube.com/watch?v=aQwqD5cB2ck\n\n\n18 What is this magic?\n\nBut first, a historical comic.\nThis comic is &gt; 5 years old and the research team was successful.\n\n\n19 Image analysis: from SVMs to Neural Nets to CONVOLUTIONAL Neural Nets\n\n\nRecall that we used Support Vector Machines to categorize these digits.\n\nThese were 8x8 images, but we flattened them into 64 independent features.\nWe‚Äôve also made great progress using Neural Nets to expand beyond just SVM.\n\n\nConvolutional Neural Net, conceptual diagram:\n\n\n\n20 Neurons - Introduction\n\nOur brain makes models of problems through networks of neurons.\n\nNeuroscience in 6 words: ‚ÄúNeurons that fire together wire together.‚Äù\n\nEach network takes a set of inputs and fires under certain conditions.\n\nWhat if we mimic that process in ML?\n\n\n\n\n21 Neural networks for regression\nConnect multiple layers of neurons. Often the covariates are considered an input ‚Äòlayer.‚Äô\nDeterministic function\n\n\n22 Neural networks: learning\n\n\n23 Graphical mapping of the functions\n\nLet‚Äôs start with an example of relating dosage to efficacy\nWe define a simple NN with 1 hidden layer and two perceptrons.\n\nEach perceptron is defined as:\n\n\n\n\n\n\n24 How is this fit? Start with some random choices and iterate via CV to find the best coefficients\n\n\n\n\n25 More specifically how is this fit? Back propagation (sec 11.4 in text)\n\n\n26 Intuitively, what is gradient descent?\n\nWe have a multidimensional parameter space. Calculate where the slope downwards is the steepest.\n\nTraverse part of the way down that slope, then recalculate derivatives.\nIteratively do this until you find the bottom.\n\n\n\n\n\n27 Simplified example of a neural net\n\n\n\nOnce trained, the neural net basically ‚Äúcombines squiggles‚Äù by adding up the fitted curves\n\n\n28 More complexity can be added to improve predictions\n\n\n\n29 Code Example: Analyzing imagery features\nBased on 500+ images of potentially malignant tumors, extracted variables such as radius, concavity, spacing, etc.\nCompared this to a medical test that identified if the tumor was in fact malignant.\nCODE TIME: Switch to VS Code.\n\n\n\n30 What did we just do?\n\nThe image below is the coefficients of the neurons in our network for different variables (vertical) and network layers (horizontal)\n\n\n\n31 Neural networks: challenges\n\nThrowing a bunch of nodes in layers won‚Äôt necessarily work well:\n\nIf you have lots of layers, it will take a long time to train\nSimply adding more nodes doesn‚Äôt always increase predictive accuracy\n\nUse problem knowledge to intelligently construct structure of network\n\nWhat about problems that require very large data?\n\nMaybe we want to impose the same weights in different parts e.g. ‚ÄúConvolutional Neural Networks‚Äù\n\n\n\n\n\n32 Convolutional Neural Nets\n\n\n33 In our SVM, we were ‚Äúthrowing away‚Äù the spatial information by flattening it.\n\nWhat if we try to keep that spatial information? Neural Nets apply can do this!\nHowever, traditional Neural Nets like we‚Äôve just discussed in Code may break down under the huge information of images.\n\n\n34 Image analysis and CNNs.\nImagine an input image, where we‚Äôre trying to classify the image (rather than a pixel).\nSay we have 3 bands for that image, and the image is 100x100 pixels. That‚Äôs 30,000 inputs.\nSuppose we have one hidden layer with 10 neurons.\nIf we use a fully connected network (like in the code we just made), we would have 300,000 weights and 10 offsets to learn for that hidden layer. Too many.\n\n\n\n35 What are convolutions?\n\n\nhttps://miro.medium.com/max/2400/1*ciDgQEjViWLnCbmX-EeSrA.gif\n\n\n36 Layer types in CNNs\n\nConvolution layer:\n\nConstruct one or more ‚Äúfilters‚Äù that get passed over the pixel\nEach filter has a window of say 5x5 pixels. It moves with some ‚Äústride‚Äù length over the image.\nNeed to chose activation function (sign, linear, logistic)\n\n\n\n\n37 Fitting convolutions into neural nets\nNow perceptrons have convolution kernels as their definition\nSame back-propagation gradient descent method works to solve this\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n38 Layer types in CNNs\n\nThe convolutions can also be added to the rest of a fully-connected Neural Net\n\n\n39 Research advance I‚Äôm currently working on: Creating ‚Äúreduced-form‚Äù spatial regression that can work on billion+ observations.\n\nGridded data preserve spatial structure\n\nNearby cells are highly correlated\nThe actual pattern may be a good predictor\n\nCan use 2-dimensional convolutions to express this structure\n\nE.g., identify what is the relationship between two variables as their distance increases via a flexible parametric form\n\n\nB.  Expression in 2- and 3-dimensions\nA.  Example 1-dimensional adjacency relationships\n\nThis is one of the parametric relationships we solve for\n\n\n\n40 Gaussian kernel\nSize = 21, Sigma 4\nApply this definition of spatial effect to each of the different land-use classes.\n\n\n\n41 Preliminary results\n\nGreatly outperforms the look-up table approach (IPCC method)\nEdge effects definitely exist\n\nBut still working to identify exact structure\n\n\n\n\n42 Appendix\nAdditional slides on own research\n\n\n\n43 Spatial results\n\nIncreases precision over linear model\nCan test specific edge-type hypotheses\n\nNote that marginal impact analysis of edge type is tricky\nNote also these use the LassoLarsCV method\n\nPreliminary conclusions:\n\nIntensive crops (class 10) have negative impact on carbon.\nNon-fragmented forests (class 50) has positive impact.\nCities (190) have negative impact.\n\n\n\n\n44 Linear model\n\n\n\n45 Spatial model"
  },
  {
    "objectID": "Machine_Learning/nn_multilayer_perceptron.html#load-the-data",
    "href": "Machine_Learning/nn_multilayer_perceptron.html#load-the-data",
    "title": "16¬† Neural Nets Code",
    "section": "16.1 Load the data",
    "text": "16.1 Load the data\nAgain we will use a dataset built-in to Sklearn that includes data related to diagnosing breast cancer.\n\n\nCode\n\ncancer = load_breast_cancer()\n\n# print('Dataset raw object', cancer)\nprint('Dataset description', cancer['DESCR'])\n\n\nDataset description .. _breast_cancer_dataset:\n\nBreast cancer wisconsin (diagnostic) dataset\n--------------------------------------------\n\n**Data Set Characteristics:**\n\n    :Number of Instances: 569\n\n    :Number of Attributes: 30 numeric, predictive attributes and the class\n\n    :Attribute Information:\n        - radius (mean of distances from center to points on the perimeter)\n        - texture (standard deviation of gray-scale values)\n        - perimeter\n        - area\n        - smoothness (local variation in radius lengths)\n        - compactness (perimeter^2 / area - 1.0)\n        - concavity (severity of concave portions of the contour)\n        - concave points (number of concave portions of the contour)\n        - symmetry\n        - fractal dimension (\"coastline approximation\" - 1)\n\n        The mean, standard error, and \"worst\" or largest (mean of the three\n        worst/largest values) of these features were computed for each image,\n        resulting in 30 features.  For instance, field 0 is Mean Radius, field\n        10 is Radius SE, field 20 is Worst Radius.\n\n        - class:\n                - WDBC-Malignant\n                - WDBC-Benign\n\n    :Summary Statistics:\n\n    ===================================== ====== ======\n                                           Min    Max\n    ===================================== ====== ======\n    radius (mean):                        6.981  28.11\n    texture (mean):                       9.71   39.28\n    perimeter (mean):                     43.79  188.5\n    area (mean):                          143.5  2501.0\n    smoothness (mean):                    0.053  0.163\n    compactness (mean):                   0.019  0.345\n    concavity (mean):                     0.0    0.427\n    concave points (mean):                0.0    0.201\n    symmetry (mean):                      0.106  0.304\n    fractal dimension (mean):             0.05   0.097\n    radius (standard error):              0.112  2.873\n    texture (standard error):             0.36   4.885\n    perimeter (standard error):           0.757  21.98\n    area (standard error):                6.802  542.2\n    smoothness (standard error):          0.002  0.031\n    compactness (standard error):         0.002  0.135\n    concavity (standard error):           0.0    0.396\n    concave points (standard error):      0.0    0.053\n    symmetry (standard error):            0.008  0.079\n    fractal dimension (standard error):   0.001  0.03\n    radius (worst):                       7.93   36.04\n    texture (worst):                      12.02  49.54\n    perimeter (worst):                    50.41  251.2\n    area (worst):                         185.2  4254.0\n    smoothness (worst):                   0.071  0.223\n    compactness (worst):                  0.027  1.058\n    concavity (worst):                    0.0    1.252\n    concave points (worst):               0.0    0.291\n    symmetry (worst):                     0.156  0.664\n    fractal dimension (worst):            0.055  0.208\n    ===================================== ====== ======\n\n    :Missing Attribute Values: None\n\n    :Class Distribution: 212 - Malignant, 357 - Benign\n\n    :Creator:  Dr. William H. Wolberg, W. Nick Street, Olvi L. Mangasarian\n\n    :Donor: Nick Street\n\n    :Date: November, 1995\n\nThis is a copy of UCI ML Breast Cancer Wisconsin (Diagnostic) datasets.\nhttps://goo.gl/U2Uwz2\n\nFeatures are computed from a digitized image of a fine needle\naspirate (FNA) of a breast mass.  They describe\ncharacteristics of the cell nuclei present in the image.\n\nSeparating plane described above was obtained using\nMultisurface Method-Tree (MSM-T) [K. P. Bennett, \"Decision Tree\nConstruction Via Linear Programming.\" Proceedings of the 4th\nMidwest Artificial Intelligence and Cognitive Science Society,\npp. 97-101, 1992], a classification method which uses linear\nprogramming to construct a decision tree.  Relevant features\nwere selected using an exhaustive search in the space of 1-4\nfeatures and 1-3 separating planes.\n\nThe actual linear program used to obtain the separating plane\nin the 3-dimensional space is that described in:\n[K. P. Bennett and O. L. Mangasarian: \"Robust Linear\nProgramming Discrimination of Two Linearly Inseparable Sets\",\nOptimization Methods and Software 1, 1992, 23-34].\n\nThis database is also available through the UW CS ftp server:\n\nftp ftp.cs.wisc.edu\ncd math-prog/cpo-dataset/machine-learn/WDBC/\n\n.. topic:: References\n\n   - W.N. Street, W.H. Wolberg and O.L. Mangasarian. Nuclear feature extraction \n     for breast tumor diagnosis. IS&T/SPIE 1993 International Symposium on \n     Electronic Imaging: Science and Technology, volume 1905, pages 861-870,\n     San Jose, CA, 1993.\n   - O.L. Mangasarian, W.N. Street and W.H. Wolberg. Breast cancer diagnosis and \n     prognosis via linear programming. Operations Research, 43(4), pages 570-577, \n     July-August 1995.\n   - W.H. Wolberg, W.N. Street, and O.L. Mangasarian. Machine learning techniques\n     to diagnose breast cancer from fine-needle aspirates. Cancer Letters 77 (1994) \n     163-171."
  },
  {
    "objectID": "Machine_Learning/nn_multilayer_perceptron.html#split-into-our-training-and-testing-xy-sets",
    "href": "Machine_Learning/nn_multilayer_perceptron.html#split-into-our-training-and-testing-xy-sets",
    "title": "16¬† Neural Nets Code",
    "section": "16.2 Split into our training and testing XY sets",
    "text": "16.2 Split into our training and testing XY sets\n\n\nCode\nX_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, random_state=0)"
  },
  {
    "objectID": "Machine_Learning/nn_multilayer_perceptron.html#exercise-1-scale-the-data",
    "href": "Machine_Learning/nn_multilayer_perceptron.html#exercise-1-scale-the-data",
    "title": "16¬† Neural Nets Code",
    "section": "16.3 Exercise 1: Scale the data",
    "text": "16.3 Exercise 1: Scale the data\nThe Multilayer Perceptron (MLP) approach is one of the few that doesn‚Äôt automatically scale the data, so let‚Äôs do that. Here we will use Numpy to do it manually, though there are alternative built-in methods within scikit-learn.\nIn the code block below, use X_train.mean(axis=0) and similar functions to scale ALL of the X variables so that they have mean 0 and standard deviation 1. HINT: X_train and others are numpy arrays and so you can use fast raster math, e.g., X_train - mean_on_train."
  },
  {
    "objectID": "Machine_Learning/nn_multilayer_perceptron.html#exercise-1-answer",
    "href": "Machine_Learning/nn_multilayer_perceptron.html#exercise-1-answer",
    "title": "16¬† Neural Nets Code",
    "section": "16.4 Exercise 1 Answer",
    "text": "16.4 Exercise 1 Answer\n\n\nCode\n# Exercies 1 Code\n\n# Using numpy functions, compute the mean value per feature on the training set and the STD.\n# May want to remind ourselves what the X_train looks like.\nprint('X_train', X_train)\n\n# Numpy arrays have a .mean() method attached to each array. \n# Below we use that, though note that we have to specify which axis we should calculate the mean on.\n# `axis=0` specifies that we want the mean of each column (which is how the separate variables are stored)\n\nmean_on_train = X_train.mean(axis=0)\n# print('mean_on_train', mean_on_train)\n\n\n# the .std() function is similarily powerful/fast.\nstd_on_train = X_train.std(axis=0)\n# print('std_on_train', std_on_train)\n\n\n# Still using the Numpy awesomeness,\n# subtract the mean, and scale by inverse standard deviation,\n# making it  mean=0 and std=1\nX_train_scaled = (X_train - mean_on_train) / std_on_train\nX_test_scaled = (X_test - mean_on_train) / std_on_train\n\n\n\nX_train [[1.185e+01 1.746e+01 7.554e+01 ... 9.140e-02 3.101e-01 7.007e-02]\n [1.122e+01 1.986e+01 7.194e+01 ... 2.022e-02 3.292e-01 6.522e-02]\n [2.013e+01 2.825e+01 1.312e+02 ... 1.628e-01 2.572e-01 6.637e-02]\n ...\n [9.436e+00 1.832e+01 5.982e+01 ... 5.052e-02 2.454e-01 8.136e-02]\n [9.720e+00 1.822e+01 6.073e+01 ... 0.000e+00 1.909e-01 6.559e-02]\n [1.151e+01 2.393e+01 7.452e+01 ... 9.653e-02 2.112e-01 8.732e-02]]"
  },
  {
    "objectID": "Machine_Learning/nn_multilayer_perceptron.html#create-the-mlp-model-object-and-fit-it",
    "href": "Machine_Learning/nn_multilayer_perceptron.html#create-the-mlp-model-object-and-fit-it",
    "title": "16¬† Neural Nets Code",
    "section": "16.5 Create the MLP model object and fit it",
    "text": "16.5 Create the MLP model object and fit it\nUsing this new scaled training data, we are ready to define a Neural Net, Known here as a Multi-Layer-Perceptron (MLP) classifier. Because this next line hides away millions of other lines of code, you may want to explore it. In VS Code, you can navigate to a function‚Äôs definition by placing your cursor in the function and press f-12. Try it in the cell below on the MLPClassifier code! The best documentation is often the code itself.\n\n\nCode\nmlp = MLPClassifier(random_state=0)\n\n# Now fit it with the scaled X and y TRAINING data.\nmlp.fit(X_train_scaled, y_train)\n\nprint(mlp)\n\n\nMLPClassifier(random_state=0)\n\n\nc:\\Users\\jajohns\\AppData\\Local\\mambaforge\\envs\\8222env1\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n  warnings.warn("
  },
  {
    "objectID": "Machine_Learning/nn_multilayer_perceptron.html#assess-the-fit",
    "href": "Machine_Learning/nn_multilayer_perceptron.html#assess-the-fit",
    "title": "16¬† Neural Nets Code",
    "section": "16.6 Assess the fit",
    "text": "16.6 Assess the fit\nNow we assess MLP‚Äôs accuracy on the TRAINING and the TESTING data.\nNotice here also I‚Äôm introducing another convenient way of combining strings and numbers. The {:.2f} specifies a placeholder for a 2-digit representation of a floating point number. The Format method then places that floating point value into that placeholder.\n\n\nCode\nscore_train = mlp.score(X_train_scaled, y_train)\nscore_test = mlp.score(X_test_scaled, y_test)\n\nprint(\"Accuracy on training set: {:.3f}\".format(score_train))\nprint(\"Accuracy on test set: {:.3f}\".format(mlp.score(X_test_scaled, y_test)))\n\n\nAccuracy on training set: 0.991\nAccuracy on test set: 0.965"
  },
  {
    "objectID": "Machine_Learning/nn_multilayer_perceptron.html#plot-the-inputs-and-hidden-layers-of-the-neural-net",
    "href": "Machine_Learning/nn_multilayer_perceptron.html#plot-the-inputs-and-hidden-layers-of-the-neural-net",
    "title": "16¬† Neural Nets Code",
    "section": "16.7 Plot the inputs and hidden layers of the neural net",
    "text": "16.7 Plot the inputs and hidden layers of the neural net\nIt can be hard perhaps to visualize what exaclty the neural net looks like (there is no coefficients table to simply look at). But here, it is small enough to actually visualize the coefficients within the network.\nBelow, we plot the coeffs_ array to see it.\n\n\nCode\n\nplt.figure(figsize=(20, 5))\nplt.imshow(mlp.coefs_[0], interpolation='none', cmap='viridis')\nplt.yticks(range(30), cancer.feature_names)\nplt.xlabel(\"Columns in weight matrix\")\nplt.ylabel(\"Input feature\")\nplt.colorbar()\nplt.show()"
  },
  {
    "objectID": "Machine_Learning/nn_multilayer_perceptron.html#exercies-5.1.2-understanding-which-features-matter-most",
    "href": "Machine_Learning/nn_multilayer_perceptron.html#exercies-5.1.2-understanding-which-features-matter-most",
    "title": "16¬† Neural Nets Code",
    "section": "16.8 Exercies 5.1.2: Understanding which features matter most",
    "text": "16.8 Exercies 5.1.2: Understanding which features matter most\nOne of the massive challenges in Neural Nets is understanding why exactly it makes the predictions it does. Can you identify which input feature shows the largest positive effect on on cancer diagnosis?\nYou probably can‚Äôt make heads or tails of it. Let‚Äôs create a greatly simplified version of our neural network to try to see if we can understand it.\nSpecifically create a new MLPClassifier but this time make it have only a single hidden layer. Hint: use f-12 on the MLPClassifier code to see it‚Äôs documentation and figure out what new input variable you sohuld specify when calling mlp = MLPClassifier( .......  ). Plot the output coefficients just like above. With only a single layer, the variables become somewhat more interpretable.\nWhich variable now seems to have the largest positive impact?\n\n\nCode\n# Excercise 5.1.2 workspace"
  },
  {
    "objectID": "Spatial_Analysis/spatial_data_intro.html",
    "href": "Spatial_Analysis/spatial_data_intro.html",
    "title": "17¬† Python for Big, Spatial Data",
    "section": "",
    "text": "Random python-related tweet\n\n\n18 Lecture 3 (of Justin‚Äôs lectures)\nWhile we‚Äôre waiting, go ahead and get the lecture slides which I‚Äôve just uploaded to canvas. Also pull the latest code from our repository.\n\n\n19 Overview for today\n\nYesterday, we connected to GitHub and gave a brutally quick introduction to Python.\nToday we‚Äôre going to:\n\nPickup where we left off\nLearn more about Numpy\nLearn about spatial data in Python\nDo raster-math ‚Äúat scale‚Äù\nIf there‚Äôs time, start into our first Machine Learning model\n\n\n\n\n20 Introduction to big and spatial data\n\n\nExample from recent publication in Ecological Economics\n\nCombined both econometrics and ‚Äúbig-data array manipulation‚Äù"
  },
  {
    "objectID": "Spatial_Analysis/classification_intro.html",
    "href": "Spatial_Analysis/classification_intro.html",
    "title": "18¬† Last day of class!",
    "section": "",
    "text": "19 Before the storm\n\n\n20 Agenda\n\nLogistical note: scores for all submitted assignments should be up to date (check Canvas to see if everything looks as expected)\nDiscuss two current events that are shaking the world\nTwo final ML Big-data models\n\nRegression trees\nNaÔøΩve Bayes classification\n\nIntegrating R and Python in one environment\n\n\n\n\n21 AI/ML hits the mainstream\n\n\n22 ChatGPT\n\nThe chatbot is a large language model that can ‚Äúpredict‚Äù the best words to respond to any prompt.\n\nBased on OpenAI‚Äôs GPT-3.5 model\nThe model includes 175 billion parameters (requiring 800 GB of storage).\n\nUses reinforcement learning on a recurrent neural network\nWhat does this change for us?\n\n\n\n\n\n23 Example using Assignment 1\n\n\n24 What have we done?!\n\nIn the very least, this course has prepped you with the tools to understand and leverage this.\n\nFor example, here‚Äôs a tutorial that uses almost exactly our Python setup to create their own Chatbot:\n\nhttps://www.youtube.com/watch?v=C-8sF81k7cY\n\nBased on this tutorial:\n\nhttps://jman4190.medium.com/how-to-build-a-gpt-3-chatbot-with-python-7b83e55805e6\n\n\n\n\n\n25 Classification of land-use, land-cover\n\n\n26 From raw sentinel reflectance data\n\n\n27 Some machine learning approaches retain ‚Äúunderstandability‚Äù\n\nThroughout this course we‚Äôve seen applications of satellite data\n\nOften, we don‚Äôt use the raw reflectance data\nInstead, we use a ‚Äúremote sensing product‚Äù like land-use, land-cover (LULC) maps.\n\nMost LULC maps are created by some form of a ‚Äúregression tree‚Äù based approach\n\nWe‚Äôll talk through those in a moment\nFor now, though, I want to keep it simple with a highly ‚Äúunderstandable‚Äù model: NaÔøΩve Bayes.\n\n\n\n\n28 NaÔøΩve Bayes\n\n\n\n29 Recommended reading\nJohnson, B. A., & Iizuka, K. (2016). Integrating OpenStreetMap crowdsourced data and Landsat time-series imagery for rapid land use/land cover (LULC) mapping: Case study of the Laguna de Bay area of the Philippines. Applied Geography, 67, 140-149.\n Breiman   , Leo. 2001. Random Forests. Machine Learning 45-1: 5-32. \n\n\n30 LULC classification example\nOpen up 04_Random_Forests/01_lulc_classification.ipynb\n\n\n31 Regression trees and Random forests!\n\n\n32 But first, a quick break for Student Evaluation of Instructor.\n\n\n33 What are these trees?\n\n\n34 Regression trees\nView as decision tree\nView as partition\n\n\n35 Building trees: approach\nIn the most basic implementation, pretty much a brute force approach:\nStart with a tree with one leaf, and compute ‚Äúloss‚Äù (prediction error or impurity)\nFor each variable and each possible split point for that variable, try splitting, compute loss again.\nPick the best possible split from the previous step. Compare to current loss. If the improvement outweighs a complexity penalty, split. Otherwise stop.\nRepeat 1-3 for each leaf in the new tree\nView as decision tree\n\n\n36 Moving beyond a tree\n\nTrees are very flexible and intuitive, but not without their problems:\n\nCan be sensitive to input data ÔøΩ changing a point can give different splits, etc\nMost functions we‚Äôre trying to learn aren‚Äôt really step functions, are they?\nHow do we quantify uncertainty?\n\nMuch of this can be addressed with bootstrapping.\n\n\n\n37 Bootstrap reminder\n\nFor a sample with n observations:\n\nCreate a bootstrap sample b by sampling n data points with replacement _ _ from our data\nEstimate the model on b\nAverage the estimates over B bootstrap samples, and use that for SE estimation too\n\n\n\n\n38 Bootstrap aggregating (Bagging)\n\nApply the bootstrap procedure to tree-building:\n\nCreate a bootstrap training set b from the original training set\nGrow a tree from b\nRepeat for all b in the set B of bootstrap samples\nTo predict an outcome for a new data point x, predict for each tree, then average (For classification trees, each of the trees vote)\n\n\n\n\n39 From bagging to random forests\n\nWe can go further with the randomization idea in bagging.\nIdea behind both: lower variance in predictions by averaging over different trees.\n\nBagging: Get different trees by randomizing sample\nRandom forests: Bagging plus randomizing which variables you can split on.\n\nEach time you go to split, only consider a randomly selected subset of variables, and choose the best split among those variables.\n\n\n\n40 R and Python together\n\n\n41 Related to a topic very close to me: How do we glue multiple models together?\n\n\n\n42 Let‚Äôs see!\nOpen up 06_R_and_Python_together/01_combining_languages.ipynb\n\n\n43 \n\n\n\n44 From python for economists book\nIncorporate  https://aeturrell.github.io/coding-for-economists/coming-from-r.html\nFor those coming from the ‚Äô tidyverse ‚Äô set of packages produced by RStudio, there are very direct Python equivalents. For example, Python has   plotnine   which has the same syntax to R‚Äôs   ggplot2  . There‚Äôs also   plydata  , which has the same syntax as R‚Äôs   dplyr   package. In Python,   matplotlib   and   pandas   are more popular packages for plotting and data analysis, respectively, but those R-style packages are absolutely there if you prefer them. More on other similar packages below."
  },
  {
    "objectID": "Spatial_Analysis/lulc_classification.html#introduction",
    "href": "Spatial_Analysis/lulc_classification.html#introduction",
    "title": "19¬† Classification of Land Cover",
    "section": "19.1 Introduction",
    "text": "19.1 Introduction\nIn this chapter we will classify data from the Sentinel-2 satellite using a supervised classification approach which incorporates the training data represented as a vector (shapefile). Specifically, we will be using Naive Bayes. Naive Bayes predicts the probabilities that a data point belongs to a particular class and the class with the highest probability is considered as the most likely class. The way they get these probabilities is by using Bayes‚Äô Theorem, which describes the probability of a feature, based on prior knowledge of conditions that might be related to that feature. Naive Bayes is quite fast when compared to some other machine learning approaches (e.g., SVM can be quite computationally intensive). This isn‚Äôt to say that it is the best per se; rather it is a great first step into the world of machine learning for classification and regression."
  },
  {
    "objectID": "Spatial_Analysis/lulc_classification.html#preparing-the-dataset",
    "href": "Spatial_Analysis/lulc_classification.html#preparing-the-dataset",
    "title": "19¬† Classification of Land Cover",
    "section": "19.2 Preparing the dataset",
    "text": "19.2 Preparing the dataset\n\nOpening the images\nOur first step is to import the relevant packages. Of note are a couple new ones, namely rasterio and shapely. These are excellent libraries that simplify working with rasters and vectors (respectively). They wrap around GDAL so you don‚Äôt have to do it the hard way. They also provide some very useful helper functions, like mask and mapping, which we explicitly import below.\n\n\nCode\nimport rasterio\nfrom rasterio.mask import mask\nimport geopandas as gpd\nimport numpy as np\nfrom shapely.geometry import mapping\n\n\nNow we need to collect all the Sentinal-2 bands because they come as individual images one per band. Ultimately, we‚Äôre going to rewrite them into a multi-band (8 bands) geotiff for later use in regression.\n\n\nCode\nimport os # we need os to do some basic file operations\n\ndata_dir = \"../data/lulc_classification_example/\"\n\nsentinal_fp = os.path.join(data_dir, \"sentinel-2/\")\n\n# If this isn't working it's probably because you have the file saved somewhere else than where sentinal_fp points. \n# Examine the absolute path to investigate.\n\n\na = os.path.abspath(sentinal_fp)\n# find every file in the sentinal_fp directory\nsentinal_band_paths = [os.path.join(sentinal_fp, f) for f in os.listdir(sentinal_fp) if os.path.isfile(os.path.join(sentinal_fp, f))]\nsentinal_band_paths.sort()\nsentinal_band_paths\n\n\n['../data/lulc_classification_example/sentinel-2/2018-10-13, Sentinel-2B L1C, B01.tiff',\n '../data/lulc_classification_example/sentinel-2/2018-10-13, Sentinel-2B L1C, B02.tiff',\n '../data/lulc_classification_example/sentinel-2/2018-10-13, Sentinel-2B L1C, B03.tiff',\n '../data/lulc_classification_example/sentinel-2/2018-10-13, Sentinel-2B L1C, B04.tiff',\n '../data/lulc_classification_example/sentinel-2/2018-10-13, Sentinel-2B L1C, B05.tiff',\n '../data/lulc_classification_example/sentinel-2/2018-10-13, Sentinel-2B L1C, B06.tiff',\n '../data/lulc_classification_example/sentinel-2/2018-10-13, Sentinel-2B L1C, B07.tiff',\n '../data/lulc_classification_example/sentinel-2/2018-10-13, Sentinel-2B L1C, B08.tiff']\n\n\nBelow we will create a rasterio dataset object containing all bands in order to use the mask() function and extract pixel values using geospatial polygons.\nWe‚Äôll do this by creating a new raster dataset and saving it for future uses.\n\n\nCode\n# create a products directory within the data dir which won't be uploaded to Github\nworkspace_dir = '../../textbook_workspace'\nimg_dir = os.path.join(workspace_dir, 'generated_lulc')\n\n# check to see if the dir it exists, if not, create it\nif not os.path.exists(img_dir):\n    os.makedirs(img_dir)\n\n# filepath for image we're writing out\nimg_fp = os.path.join(img_dir, 'sentinel_bands.tif')\n\n# Read metadata of first file and assume all other bands are the same\nwith rasterio.open(sentinal_band_paths[0]) as src0:\n    meta = src0.meta\n\n# Update metadata to reflect the number of layers\nmeta.update(count = len(sentinal_band_paths))\n\n# Read each layer and write it to stack\nwith rasterio.open(img_fp, 'w', **meta) as dst:\n    for id, layer in enumerate(sentinal_band_paths, start=1):\n        with rasterio.open(layer) as src1:\n            dst.write_band(id, src1.read(1))\n\n\nOkay we‚Äôve successfully written it out now let‚Äôs open it back up and make sure it meets our expectations:\n\n\nCode\nfull_dataset = rasterio.open(img_fp)\nimg_rows, img_cols = full_dataset.shape\nimg_bands = full_dataset.count\nprint(full_dataset.shape) # dimensions\nprint(full_dataset.count) # bands\n\n\n(2201, 2629)\n8"
  },
  {
    "objectID": "Spatial_Analysis/lulc_classification.html#exercise",
    "href": "Spatial_Analysis/lulc_classification.html#exercise",
    "title": "19¬† Classification of Land Cover",
    "section": "19.3 Exercise:",
    "text": "19.3 Exercise:\nLet‚Äôs clip the image and take a look at it. In the starter code below, we use the .read() method to read three different bands into a single image that we will plot. We also use numpy slice notation to clip out a smaller part of the array.\nHOWEVER, the image won‚Äôt look right on its own. You can kind of tell there is color, but it doesn‚Äôt look like it should. That‚Äôs because we‚Äôre reading in the wrong bands (our eyes are expecting Red, Green and Blue).\nAs a class race, try out different combinations of bands. When you think you‚Äôve got it, raise your hand. When someone gets it, or after 2 minutes, whoever has the best image wins! Hint: if you want to nail this, just look at the sentinel documentation https://gprivate.com/62co2.\n\n\nCode\nimport matplotlib.pyplot as plt\nfrom rasterio.plot import show\n\n\nband_1 = 1\nband_2 = 2\nband_3 = 3\n\nclipped_img = full_dataset.read([band_1, band_2, band_3])[:, 150:600, 250:1400]\nprint(clipped_img.shape)\nfig, ax = plt.subplots(figsize=(10,7))\nshow(clipped_img[:, :, :], ax=ax, transform=full_dataset.transform) # add the transform arg to get it in lat long coords\n\n\n(3, 450, 1150)\n\n\n&lt;AxesSubplot: &gt;\n\n\n\n\n\nOkay looks good! Our raster dataset is ready!\n\n19.3.1 Now our goal is to get the pixels from the raster as outlined in each shapefile.\nOur training data, the shapefile we‚Äôve worked with, contains one main field we care about: + a Classname field (String datatype)\nCombined with the innate location information of polygons in a Shapefile, we have all that we need to use for pairing labels with the information in our raster.\nHowever, in order to pair up our vector data with our raster pixels, we will need a way of co-aligning the datasets in space.\nWe‚Äôll do this using the rasterio mask function which takes in a dataset and a polygon and then outputs a numpy array with the pixels in the polygon.\nLet‚Äôs run through an example:\n\n\nCode\nfull_dataset.crs\n\n\nCRS.from_epsg(4326)\n\n\nOpen up our shapefile and check its crs\n\n\nCode\nshapefile = gpd.read_file(os.path.join(data_dir, 'rcr', 'rcr_landcover.shp'))\nshapefile.crs\n\n\n&lt;Derived Projected CRS: EPSG:32618&gt;\nName: WGS 84 / UTM zone 18N\nAxis Info [cartesian]:\n- E[east]: Easting (metre)\n- N[north]: Northing (metre)\nArea of Use:\n- name: Between 78¬∞W and 72¬∞W, northern hemisphere between equator and 84¬∞N, onshore and offshore. Bahamas. Canada - Nunavut; Ontario; Quebec. Colombia. Cuba. Ecuador. Greenland. Haiti. Jamaica. Panama. Turks and Caicos Islands. United States (USA). Venezuela.\n- bounds: (-78.0, 0.0, -72.0, 84.0)\nCoordinate Operation:\n- name: UTM zone 18N\n- method: Transverse Mercator\nDatum: World Geodetic System 1984 ensemble\n- Ellipsoid: WGS 84\n- Prime Meridian: Greenwich\n\n\nRemember the projections don‚Äôt match! Let‚Äôs use some geopandas magic to reproject all our shapefiles to lat, long.\n\n\nCode\nshapefile = shapefile.to_crs({'init': 'epsg:4326'})\n\n\nc:\\Users\\jajohns\\AppData\\Local\\mambaforge\\envs\\8222env1\\lib\\site-packages\\pyproj\\crs\\crs.py:130: FutureWarning: '+init=&lt;authority&gt;:&lt;code&gt;' syntax is deprecated. '&lt;authority&gt;:&lt;code&gt;' is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6\n  in_crs_string = _prepare_from_proj_string(in_crs_string)\n\n\n\n\nCode\nshapefile.crs\n\n\n&lt;Geographic 2D CRS: +init=epsg:4326 +type=crs&gt;\nName: WGS 84\nAxis Info [ellipsoidal]:\n- lon[east]: Longitude (degree)\n- lat[north]: Latitude (degree)\nArea of Use:\n- name: World.\n- bounds: (-180.0, -90.0, 180.0, 90.0)\nDatum: World Geodetic System 1984 ensemble\n- Ellipsoid: WGS 84\n- Prime Meridian: Greenwich\n\n\n\n\nCode\nlen(shapefile)\n\n\n23\n\n\nNow we want to extract the geometry of each feature in the shapefile in GeoJSON format:\n\n\nCode\n# this generates a list of shapely geometries\ngeoms = shapefile.geometry.values \n\n# let's grab a single shapely geometry to check\ngeometry = geoms[0] \nprint(type(geometry))\nprint(geometry)\n\n# transform to GeoJSON format\nfrom shapely.geometry import mapping\nfeature = [mapping(geometry)] # can also do this using polygon.__geo_interface__\nprint(type(feature))\nprint(feature)\n\n\n&lt;class 'shapely.geometry.polygon.Polygon'&gt;\nPOLYGON ((-76.67593927883173 34.69487548849214, -76.67573882771855 34.694513199139024, -76.6766693455509 34.69360077384821, -76.67676946161477 34.69421769352402, -76.67593927883173 34.69487548849214))\n&lt;class 'list'&gt;\n[{'type': 'Polygon', 'coordinates': (((-76.67593927883173, 34.69487548849214), (-76.67573882771855, 34.694513199139024), (-76.6766693455509, 34.69360077384821), (-76.67676946161477, 34.69421769352402), (-76.67593927883173, 34.69487548849214)),)}]\n\n\nNow let‚Äôs extract the raster values values within the polygon using the rasterio mask() function\n\n\nCode\nout_image, out_transform = mask(full_dataset, feature, crop=True)\nout_image.shape\n\n\n(8, 18, 13)\n\n\nOkay those looks like the right dimensions for our training data. 8 bands and 6x8 pixels seems reasonable given our earlier explorations.\nWe‚Äôll be doing a lot of memory intensive work so let‚Äôs clean up and close this dataset.\n\n\nCode\nfull_dataset.close()\n\n\n\n\n19.3.2 Building the Training Data for scikit-learn\nNow let‚Äôs do it for all features in the shapefile and create an array X that has all the pixels and an array y that has all the training labels.\n\n\nCode\nX = np.array([], dtype=np.int8).reshape(0,8) # pixels for training\ny = np.array([], dtype=np.string_) # labels for training\n\n# extract the raster values within the polygon \nwith rasterio.open(img_fp) as src:\n    band_count = src.count\n    for index, geom in enumerate(geoms):\n        feature = [mapping(geom)]\n\n        # the mask function returns an array of the raster pixels within this feature\n        out_image, out_transform = mask(src, feature, crop=True) \n        \n        # eliminate all the pixels with 0 values for all 8 bands - AKA not actually part of the shapefile\n        out_image_trimmed = out_image[:,~np.all(out_image == 0, axis=0)]\n        \n        # eliminate all the pixels with 255 values for all 8 bands - AKA not actually part of the shapefile\n        out_image_trimmed = out_image_trimmed[:,~np.all(out_image_trimmed == 255, axis=0)]\n        \n        # reshape the array to [pixel count, bands]\n        out_image_reshaped = out_image_trimmed.reshape(-1, band_count)\n        \n        # append the labels to the y array\n        y = np.append(y,[shapefile[\"Classname\"][index]] * out_image_reshaped.shape[0]) \n        \n        # stack the pizels onto the pixel array\n        X = np.vstack((X, out_image_reshaped))        \n\n\n\nPairing Y with X\nNow that we have the image we want to classify (our X feature inputs), and the land cover labels (our y labeled data), let‚Äôs check to make sure they match in size so we can feed them to Naive Bayes:\n\n\nCode\n# What are our classification labels?\nlabels = np.unique(shapefile[\"Classname\"])\nprint('The training data include {n} classes: {classes}\\n'.format(n=labels.size, \n                                                                classes=labels))\n\n# We will need a \"X\" matrix containing our features, and a \"y\" array containing our labels\nprint('Our X matrix is sized: {sz}'.format(sz=X.shape))\nprint('Our y array is sized: {sz}'.format(sz=y.shape))\n\n\nThe training data include 6 classes: ['Emergent Wetland' 'Forested Wetland' 'Herbaceous' 'Sand'\n 'Subtidal Haline' 'WetSand']\n\nOur X matrix is sized: (598, 8)\nOur y array is sized: (598,)\n\n\nIt all looks good! Let‚Äôs explore the spectral signatures of each class now to make sure they‚Äôre actually separable since all we‚Äôre going by in this classification is pixel values.\n\n\nCode\nfig, ax = plt.subplots(1,3, figsize=[20,8])\n\n# numbers 1-8\nband_count = np.arange(1,9)\n\nclasses = np.unique(y)\nfor class_type in classes:\n    band_intensity = np.mean(X[y==class_type, :], axis=0)\n    ax[0].plot(band_count, band_intensity, label=class_type)\n    ax[1].plot(band_count, band_intensity, label=class_type)\n    ax[2].plot(band_count, band_intensity, label=class_type)\n# plot them as lines\n\n# Add some axis labels\nax[0].set_xlabel('Band #')\nax[0].set_ylabel('Reflectance Value')\nax[1].set_ylabel('Reflectance Value')\nax[1].set_xlabel('Band #')\nax[2].set_ylabel('Reflectance Value')\nax[2].set_xlabel('Band #')\n#ax[0].set_ylim(32,38)\nax[1].set_ylim(32,38)\nax[2].set_ylim(70,140)\n#ax.set\nax[1].legend(loc=\"upper right\")\n# Add a title\nax[0].set_title('Band Intensities Full Overview')\nax[1].set_title('Band Intensities Lower Ref Subset')\nax[2].set_title('Band Intensities Higher Ref Subset')\n\n\nText(0.5, 1.0, 'Band Intensities Higher Ref Subset')\n\n\n\n\n\nThey look okay but emergent wetland and subtital haline look quite similar! They‚Äôre going to be difficult to differentiate.\nLet‚Äôs make a quick helper function, this one will convert the class labels into indicies and then assign a dictionary relating the class indices and their names.\n\n\nCode\ndef str_class_to_int(class_array):\n    class_array[class_array == 'Subtidal Haline'] = 0\n    class_array[class_array == 'WetSand'] = 1\n    class_array[class_array == 'Emergent Wetland'] = 2\n    class_array[class_array == 'Sand'] = 3\n    class_array[class_array == 'Herbaceous'] = 4\n    class_array[class_array == 'Forested Wetland'] = 5\n    return(class_array.astype(int))"
  },
  {
    "objectID": "Spatial_Analysis/lulc_classification.html#training-the-classifier",
    "href": "Spatial_Analysis/lulc_classification.html#training-the-classifier",
    "title": "19¬† Classification of Land Cover",
    "section": "19.4 Training the Classifier",
    "text": "19.4 Training the Classifier\nNow that we have our X matrix of feature inputs (the spectral bands) and our y array (the labels), we can train our model.\nVisit this web page to find the usage of GaussianNaiveBayes Classifier from scikit-learn.\n\n\nCode\nfrom sklearn.naive_bayes import GaussianNB\n\ngnb = GaussianNB()\ngnb.fit(X, y)\n\n\nGaussianNB()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GaussianNBGaussianNB()\n\n\nIt is that simple to train a classifier in scikit-learn! The hard part is often validation and interpretation."
  },
  {
    "objectID": "Spatial_Analysis/lulc_classification.html#predicting-on-the-image",
    "href": "Spatial_Analysis/lulc_classification.html#predicting-on-the-image",
    "title": "19¬† Classification of Land Cover",
    "section": "19.5 Predicting on the image",
    "text": "19.5 Predicting on the image\nWith our Naive Bayes classifier fit, we can now proceed by trying to classify the entire image:\nWe‚Äôre only going to open the subset of the image we viewed above because otherwise it is computationally too intensive for most users.\n\n\nCode\nfrom rasterio.plot import show\nfrom rasterio.plot import show_hist\nfrom rasterio.windows import Window\nfrom rasterio.plot import reshape_as_raster, reshape_as_image\n\n\n\n\nCode\nwith rasterio.open(img_fp) as src:\n    # may need to reduce this image size if your kernel crashes, takes a lot of memory\n    img = src.read()[:, 150:600, 250:1400]\n\n# Take our full image and reshape into long 2d array (nrow * ncol, nband) for classification\nprint(img.shape)\nreshaped_img = reshape_as_image(img)\nprint(reshaped_img.shape)\n\n\n(8, 450, 1150)\n(450, 1150, 8)\n\n\nNow we can predict for each pixel in our image:\n\n\nCode\nclass_prediction = gnb.predict(reshaped_img.reshape(-1, 8))\n\n# Reshape our classification map back into a 2D matrix so we can visualize it\nclass_prediction = class_prediction.reshape(reshaped_img[:, :, 0].shape)\n\n\nBecause our shapefile came with the labels as strings we want to convert them to a numpy array with ints using the helper function we made earlier.\n\n\nCode\nclass_prediction = str_class_to_int(class_prediction)\n\n\n\n19.5.1 Let‚Äôs visualize it!\nFirst we‚Äôll make a colormap so we can visualize the classes, which are just encoded as integers, in more logical colors. Don‚Äôt worry too much if this code is confusing! It can be a little clunky to specify colormaps for matplotlib.\n\n\nCode\ndef color_stretch(image, index):\n    colors = image[:, :, index].astype(np.float64)\n    for b in range(colors.shape[2]):\n        colors[:, :, b] = rasterio.plot.adjust_band(colors[:, :, b])\n    return colors\n    \n# find the highest pixel value in the prediction image\nn = int(np.max(class_prediction))\n\n# next setup a colormap for our map\ncolors = dict((\n    (0, (48, 156, 214, 255)),   # Blue - Water\n    (1, (139,69,19, 255)),      # Brown - WetSand\n    (2, (96, 19, 134, 255)),    # Purple - Emergent Wetland\n    (3, (244, 164, 96, 255)),   # Tan - Sand\n    (4, (206, 224, 196, 255)),  # Lime - Herbaceous\n    (5, (34, 139, 34, 255)),    # Forest Green - Forest \n))\n\n# Put 0 - 255 as float 0 - 1\nfor k in colors:\n    v = colors[k]\n    _v = [_v / 255.0 for _v in v]\n    colors[k] = _v\n    \nindex_colors = [colors[key] if key in colors else \n                (255, 255, 255, 0) for key in range(0, n+1)]\n\ncmap = plt.matplotlib.colors.ListedColormap(index_colors, 'Classification', n+1)\n\n\nNow show the classified map next to the RGB image!\n\n\nCode\nfig, axs = plt.subplots(2,1,figsize=(10,7))\n\nimg_stretched = color_stretch(reshaped_img, [4, 3, 2])\naxs[0].imshow(img_stretched)\n\naxs[1].imshow(class_prediction, cmap=cmap, interpolation='none')\n\nfig.show()\n\n\nC:\\Users\\jajohns\\AppData\\Local\\Temp\\ipykernel_15156\\1384158104.py:8: UserWarning: Matplotlib is currently using module://matplotlib_inline.backend_inline, which is a non-GUI backend, so cannot show the figure.\n  fig.show()\n\n\n\n\n\n\n\n19.5.2 This looks pretty good!\nLet‚Äôs generate a map of Normalized Difference Water Index (NDWI) and NDVI just to compare with out output map.\nNDWI is similar to NDVI but for identifying water.\n\n\nCode\nwith rasterio.open(img_fp) as src:\n    green_band = src.read(3)\n    red_band = src.read(4)\n    nir_band = src.read(8)\n    \nndwi = (green_band.astype(float) - nir_band.astype(float)) / (green_band.astype(float) + nir_band.astype(float))\nndvi = (nir_band.astype(float) - red_band.astype(float)) / (red_band.astype(float) + nir_band.astype(float))\n\n\nSubset them to our area of interest:\n\n\nCode\nndwi = ndwi[150:600, 250:1400]\nndvi = ndvi[150:600, 250:1400]\n\n\nDisplay all four maps:\n\n\nCode\nfig, axs = plt.subplots(2,2,figsize=(15,7))\n\nimg_stretched = color_stretch(reshaped_img, [3, 2, 1])\naxs[0,0].imshow(img_stretched)\n\naxs[0,1].imshow(class_prediction, cmap=cmap, interpolation='none')\n\nnwdi_plot = axs[1,0].imshow(ndwi, cmap=\"RdYlGn\")\naxs[1,0].set_title(\"NDWI\")\nfig.colorbar(nwdi_plot, ax=axs[1,0])\n\nndvi_plot = axs[1,1].imshow(ndvi, cmap=\"RdYlGn\")\naxs[1,1].set_title(\"NDVI\")\nfig.colorbar(ndvi_plot, ax=axs[1,1])\n\nplt.show()\n\n\n\n\n\nLooks pretty good! Areas that are high on the NDWI ratio are generally classified as water and areas high on NDVI are forest and herbaceous. It does seem like the wetland areas (e.g.¬†the bottom right island complex) aren‚Äôt being picked up so it might be worth experimenting with other algorithms!\nLet‚Äôs take a closer look at the Duke Marine Lab and the tip of the Rachel Carson Reserve.\n\n\nCode\nfig, axs = plt.subplots(1,2,figsize=(15,15))\n\nimg_stretched = color_stretch(reshaped_img, [3, 2, 1])\naxs[0].imshow(img_stretched[0:180, 160:350])\n\naxs[1].imshow(class_prediction[0:180, 160:350], cmap=cmap, interpolation='none')\n\nfig.show()\n\n\nC:\\Users\\jajohns\\AppData\\Local\\Temp\\ipykernel_15156\\1670099218.py:8: UserWarning: Matplotlib is currently using module://matplotlib_inline.backend_inline, which is a non-GUI backend, so cannot show the figure.\n  fig.show()\n\n\n\n\n\nThis actually doesn‚Äôt look half bad! Land cover mapping is a complex problem and one where there are many approaches and tools for improving a map."
  },
  {
    "objectID": "Spatial_Analysis/lulc_classification.html#testing-an-unsupervised-classification-algorithm",
    "href": "Spatial_Analysis/lulc_classification.html#testing-an-unsupervised-classification-algorithm",
    "title": "19¬† Classification of Land Cover",
    "section": "19.6 Testing an Unsupervised Classification Algorithm",
    "text": "19.6 Testing an Unsupervised Classification Algorithm\nLet‚Äôs also try a unsupervised classification algorithm, k-means clustering, in the scikit-learn library (documentation)\nK-means (wikipedia page) aims to partition n observations into k clusters in which each observation belongs to the cluster with the nearest mean, serving as a prototype of the cluster.\n\n\nCode\nfrom sklearn.cluster import KMeans\n\nbands, rows, cols = img.shape\n\nk = 10 # num of clusters\n\nkmeans_predictions = KMeans(n_clusters=k, random_state=0).fit(reshaped_img.reshape(-1, 8))\n\nkmeans_predictions_2d = kmeans_predictions.labels_.reshape(rows, cols)\n\n# Now show the classmap next to the image\nfig, axs = plt.subplots(1,2,figsize=(15,8))\n\nimg_stretched = color_stretch(reshaped_img, [3, 2, 1])\naxs[0].imshow(img_stretched)\n\naxs[1].imshow(kmeans_predictions_2d)\n\n\n&lt;matplotlib.image.AxesImage at 0x19a12736320&gt;\n\n\n\n\n\nWow this looks like it was better able to distinguish some areas like the wetland and submerged sand than our supervised classification approach! But supervised usually does better with some tuning, luckily there are lots of ways to think about improving our supervised method.\nAdapted from the wonderful tutorial series by Patrick Gray: https://github.com/patrickcgray"
  },
  {
    "objectID": "Spatial_Analysis/python_assignment_2.html",
    "href": "Spatial_Analysis/python_assignment_2.html",
    "title": "20¬† Exercise 2",
    "section": "",
    "text": "21 Step 6:\nExtract the first array row of the data_array and assign it to y. Assign the rest to X.\nCode\n# Step 6 code\nSplit the X and y into testing and training data such that the training data is the first million pixels and the testing data is the next 200,000. Do this using numpy slice notation on the X and y variables you created.\nCode\n# Step 7 code\nTo make the code run faster, we are going to use every 10th pixel. We can easily get this via numpy slicing again, using x_train[::10] to get every 10th pixel.\nCode\n# Step 8 code\nCreate a Lasso object (using the default penalty term alpha) and fit it to the training data. Create and print out a vector of predicted carbon values. Also print out the score using the lasso object‚Äôs .score() method on the TESTING data. Print out the fitted lasso score.\nCode\n# Step 9 code\nTo view how our projections LOOK, we can create a predicted matrix on the whole X, reshape it back into the original 2d shape and look at it. You can compare this to the input array to visualize how it looks. Note that this will only work if you name your objects like mine.\nCode\n# Step 10 code\nCreate a list of 30 alphas using np.logspace(-1, 3, 30).\nUsing a for loop iterate over those alphas and run the Lasso model like above, but using the alpha values in the loop. Print out the fit score at each step. Using matplotlib, plot how this value changes as alpha changes. Finally, extract the best alpha of the bunch.\nCode\n# Step 11 code\nRerun the lasso with that best value and identify all of the coefficiencts that were ‚Äúselected‚Äù ie had non-zero values. Save these coefficient indices and labels to a list.\nCode\n# Step 12 code\nUsing Statsmodels, run an OLS version on the selected variables.\nUse print to show the results table.\nWrite a description of any advantages this approach has over vanilla OLS.\nCode\n# Step 13 code"
  },
  {
    "objectID": "Spatial_Analysis/python_assignment_2.html#using-post-lasso-on-large-spatial-data",
    "href": "Spatial_Analysis/python_assignment_2.html#using-post-lasso-on-large-spatial-data",
    "title": "20¬† Exercise 2",
    "section": "20.1 Using post-LASSO on large spatial data",
    "text": "20.1 Using post-LASSO on large spatial data\nThis assignment will give you a real (active) research topic that I‚Äôve discussed a little bit in class: predicting carbon storage as a function of high-resolution gridded data. In the class google drive you will find all the data you need. I added it just recently so if you don‚Äôt have it, be sure to go get it first.\nThis assignment will have you use the automated variable selection approach within LASSO to deal with a common situation in regressions on raster-stacks: we have so much data everything is significant but will lead to massive overfitting. The basic approach used here will involve reading in 2d rasters, flattening them into a 1d column ready to add to a dataframe shaped object, which we will use as our X matrix.\nPlease turn in the completed Notebook (.ipynb) file that includes the results you generate.\nBelow is some starter code along with specific assignment questions.\n\n\nCode\n# Load libraries\nimport numpy as np\nimport os\nfrom osgeo import gdal\nfrom sklearn.linear_model import Lasso\nfrom matplotlib import pyplot as plt\nfrom statsmodels.api import OLS"
  },
  {
    "objectID": "Spatial_Analysis/python_assignment_2.html#step-1-download-the-data-and-set-paths",
    "href": "Spatial_Analysis/python_assignment_2.html#step-1-download-the-data-and-set-paths",
    "title": "20¬† Exercise 2",
    "section": "20.2 Step 1: Download the data and set paths",
    "text": "20.2 Step 1: Download the data and set paths\nDownload the latest data from the class‚Äôs google drive. In there, you will need the the files in Data/python_assignment_2 data and assign a relative path to the soyo_tile directory in that assignment directory. It is your task to ensure your script runs in the right location and the data is stored in the right location that this relative path works.\n\n\nCode\n# Step 1 code"
  },
  {
    "objectID": "Spatial_Analysis/python_assignment_2.html#step-2-set-raster-paths",
    "href": "Spatial_Analysis/python_assignment_2.html#step-2-set-raster-paths",
    "title": "20¬† Exercise 2",
    "section": "20.3 Step 2: Set raster paths",
    "text": "20.3 Step 2: Set raster paths\nAssign each of the raster paths in the directory to a dictionary for later use. I‚Äôve included most of the code (so you don‚Äôt have to waste your time typing), but add in the missing paths.\n\n\nCode\n# Step 2 code\n\nraster_paths = {}\n\n# First is the dependent varable, Above Ground Carbon (AGB) in tons, measured at 30 meters globally (here it is clipped to a smaller area)\nraster_paths['agb_observed_baccini_2000_30m'] = os.path.join(data_dir, \"agb_observed_baccini_2000_30m.tif\")\n\n# Here are some of the independent variables\nraster_paths['CRFVOL_M_sl1_250m'] = os.path.join(data_dir, \"CRFVOL_M_sl1_250m.tif\")\nraster_paths['HISTPR_250m'] = os.path.join(data_dir, \"HISTPR_250m.tif\")\nraster_paths['OCDENS_M_sl1_250m'] = os.path.join(data_dir, \"OCDENS_M_sl1_250m.tif\")\nraster_paths['PHIHOX_M_sl1_250m'] = os.path.join(data_dir, \"PHIHOX_M_sl1_250m.tif\")\nraster_paths['roughness_30s'] = os.path.join(data_dir, \"roughness_30s.tif\")\nraster_paths['SLGWRB_250m'] = os.path.join(data_dir, \"SLGWRB_250m.tif\")\nraster_paths['SLTPPT_M_sl1_250m'] = os.path.join(data_dir, \"SLTPPT_M_sl1_250m.tif\")\nraster_paths['terrain_ruggedness_index_30s'] = os.path.join(data_dir, \"terrain_ruggedness_index_30s.tif\")\nraster_paths['TEXMHT_M_sl1_250m'] = os.path.join(data_dir, \"TEXMHT_M_sl1_250m.tif\")\nraster_paths['wc2.0_bio_30s_01'] = os.path.join(data_dir, \"wc2.0_bio_30s_01.tif\")\nraster_paths['alt_30s'] = os.path.join(data_dir, \"alt_30s.tif\")\nraster_paths['AWCh1_M_sl1_250m'] = os.path.join(data_dir, \"AWCh1_M_sl1_250m.tif\")\nraster_paths['BDRICM_M_250m'] = os.path.join(data_dir, \"BDRICM_M_250m.tif\")\nraster_paths['BDRLOG_M_250m'] = os.path.join(data_dir, \"BDRLOG_M_250m.tif\")\nraster_paths['BLDFIE_M_sl1_250m'] = os.path.join(data_dir, \"BLDFIE_M_sl1_250m.tif\")"
  },
  {
    "objectID": "Spatial_Analysis/python_assignment_2.html#step-3-open-the-rasters",
    "href": "Spatial_Analysis/python_assignment_2.html#step-3-open-the-rasters",
    "title": "20¬† Exercise 2",
    "section": "20.4 Step 3: Open the rasters",
    "text": "20.4 Step 3: Open the rasters\nOur dependent variable will be 30 meter observations of carbon storage from Baccini et al.¬†(unpublished, but soon to be published) data. The label I assigned in the dictionary above was agb_observed_baccini_2000_30m for this variable. Use gdal.Open, GetRasterBand(1) and ReadAsArray() to read this geotiff as a numpy file\nSide note: If you get an error like: ‚ÄúERROR 4: This is a BigTIFF file. BigTIFF is not supported by this version of GDAL and libtiff.‚Äù make sure you have installed gdal with the mamba method from lecture 1.\n\n\nCode\n# Step 3 code"
  },
  {
    "objectID": "Spatial_Analysis/python_assignment_2.html#step-4-define-some-arrays",
    "href": "Spatial_Analysis/python_assignment_2.html#step-4-define-some-arrays",
    "title": "20¬† Exercise 2",
    "section": "20.5 Step 4: Define some arrays",
    "text": "20.5 Step 4: Define some arrays\nCreate an empty numpy array (or full of zeros) of the right shape to house all our raster data. A very CPU-efficient way of arranging a stack of 2d rasters (which would be 3d once stacked up), is to flatten each 2d raster into a longer 1d array. This will go into our X matrix. In order to create the right sized X matrix, first get the n_obs and n_vars by inspecting the dependent variable raster and the dictionary of inputs above. Note that the n_vars should be the number of independent AND dependent variables.\n\n\nCode\n# Step 4 code"
  },
  {
    "objectID": "Spatial_Analysis/python_assignment_2.html#step-5-load-all-the-independent-variables",
    "href": "Spatial_Analysis/python_assignment_2.html#step-5-load-all-the-independent-variables",
    "title": "20¬† Exercise 2",
    "section": "20.6 Step 5: Load all the independent variables",
    "text": "20.6 Step 5: Load all the independent variables\n\nIterate through the dictionary and load each raster as a 2d array\nflatten it to 1d using the .flatten() method in numpy\nAssign this 1d array to the correct column of the data array. By convention, the depvar will be the first column.\n\nHint, assuming you have arranged your X array in the correct way, it should have observations (pixels) as rows and variables as cols. Given that each flattened array is for one variable and is as long as there are rows, a convenient way of assigning it would be to use numpy slice notation, potentially similar to:\ndata_array[:, column_index_integer]\nThe first colon just denotes the whole row and the column index is an integer you could create pointing to the right row.\nSome incomplete code to get you started is below.\n\n\nCode\n# Step 5 code\n\nfor name, path in raster_paths.items():\n    print('Loading', path)\n    'MISSING STUFF'\n    flattened_raster_array = band.ReadAsArray().flatten()\n    data_array[:, col_index] = flattened_raster_array\n    feature_names.append(name)"
  },
  {
    "objectID": "Appendices/hosting_notes.html#create-a-github.io-page",
    "href": "Appendices/hosting_notes.html#create-a-github.io-page",
    "title": "21¬† Hosting notes",
    "section": "21.1 Create a github.io page",
    "text": "21.1 Create a github.io page\nCreate a new repository using your username:\n\n\n\nimage.png\n\n\nClone the newly-created repo\n\n\n\nimage.png\n\n\nThen just put the index.html file in the root directory of that repository. An example workflow is to generate the index.html from Quarto and then copy that in.\nPersonally, I use a script in the scripts dir that copies the generated Quarto directory to the public directory in ‚Äúscripts/publish_ee_book_to_github_io_site‚Äù"
  },
  {
    "objectID": "Appendices/hosting_notes.html#pasting-images",
    "href": "Appendices/hosting_notes.html#pasting-images",
    "title": "21¬† Hosting notes",
    "section": "21.2 Pasting images",
    "text": "21.2 Pasting images\nNOTE: This is easiest to do in ipynb NOT in qmd because of the built-in options for pasting images into notebooks.\nNote that there are two methods: paste (ctrl-v) directly into a markdown cell of a ipynb. This adds it as an ‚Äúattachment‚Äù where the raw binary image code is written into text in the cell‚Äôs attribute (and thus there is no external file saved). The other is the use the Paste Image VS Code plug in, which lets you actually write a file in a gnerated location for the PNG. This is called by ctrl-alt-v\nOption 1\n\n\n\nimage.png\n\n\nOption 2"
  },
  {
    "objectID": "Appendices/quarto_notes.html",
    "href": "Appendices/quarto_notes.html",
    "title": "22¬† Cross References",
    "section": "",
    "text": "For tables produced by executable code cells, include a label with a tbl- prefix to make them cross-referenceable. For example:\n\n\nCode\nfrom IPython.display import Markdown\n# from tabulate import tabulate\n# table = [[\"Sun\",696000,1989100000],\n#          [\"Earth\",6371,5973.6],\n#          [\"Moon\",1737,73.5],\n#          [\"Mars\",3390,641.85]]\n# Markdown(tabulate(\n#   table, \n#   headers=[\"Planet\",\"R (km)\", \"mass (x 10^29 kg)\"]\n# ))\n\n\n\n?(caption)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Big Data for Economists",
    "section": "",
    "text": "Preface\nThis is a a work in progress that combines code from many different aspects of my reserach, including from land-use change modeling (SEALS), ecosystem services modeling (InVEST), general equilibrium modeling (GTAP), APEC 8222 (Big Data for Economists), APEC 8601 (Natural Resource Economics) and several sources.\nThis book makes extensive use of other open-source textbooks. Acknowledgements for source material appear in each relevant file. Most have been modified to fit the topic at hand (unless they are licensed under a No-Derivative Work type of license, in which case they are just provided verbatim for easy access).\nThis will eventually grow to form the core textbook content for new iterations of theses courses as well as the documentation for the respective models."
  },
  {
    "objectID": "Python_Introduction/python_for_big_spatial_data.html",
    "href": "Python_Introduction/python_for_big_spatial_data.html",
    "title": "3¬† Python for Big, Spatial Data",
    "section": "",
    "text": "While we‚Äôre waiting, go ahead and get the lecture slides which I‚Äôve just uploaded to canvas. Also pull the latest code from our repository.\n\n\n\nFigure¬†3.1: Random python-related tweet\n\n\n\nOverview for today\n\nYesterday, we connected to GitHub and gave a brutally quick introduction to Python.\nToday we‚Äôre going to:\n\nPickup where we left off\nLearn more about Numpy\nLearn about spatial data in Python\nDo raster-math ‚Äúat scale‚Äù\nIf there‚Äôs time, start into our first Machine Learning model\n\n\n\n\nIntroduction to big and spatial data\n\nExample from recent publication in Ecological Economics \nCombined both econometrics and ‚Äúbig-data array manipulation‚Äù"
  },
  {
    "objectID": "Machine_Learning/introduction_big_data.html#source-control-tab-shows-new-code-is-available.",
    "href": "Machine_Learning/introduction_big_data.html#source-control-tab-shows-new-code-is-available.",
    "title": "11¬† Diving into Machine Learning on Big Data",
    "section": "Source control tab shows new code is available.",
    "text": "Source control tab shows new code is available.\n\nGo to the Source Control tab\nIf your files are different than the repository, you will see those files listed here."
  },
  {
    "objectID": "Machine_Learning/introduction_big_data.html#how-do-we-resolve-this-merge-conflict",
    "href": "Machine_Learning/introduction_big_data.html#how-do-we-resolve-this-merge-conflict",
    "title": "11¬† Diving into Machine Learning on Big Data",
    "section": "How do we resolve this ‚Äúmerge conflict‚Äù?",
    "text": "How do we resolve this ‚Äúmerge conflict‚Äù?\n\nIf you do not want to keep your changes (simplest case) we can discard them and then pull.\n\n\n\nTo do this, go to the Source Control tab and you will see the Change List.\n\n\n\nRight-click, and select discard changes.\nNow you can happily Git Pull."
  },
  {
    "objectID": "Machine_Learning/introduction_big_data.html#what-happens-when-i-make-a-change-and-how-does-everyone-stay-synced.",
    "href": "Machine_Learning/introduction_big_data.html#what-happens-when-i-make-a-change-and-how-does-everyone-stay-synced.",
    "title": "11¬† Diving into Machine Learning on Big Data",
    "section": "What happens when I make a change and how does everyone stay synced.",
    "text": "What happens when I make a change and how does everyone stay synced.\nSuppose I make an edit on my Workstation\nSuppose I ad some extremely important new code to our workbook from last lecture.\n\n\nGit realizes that my file now doesn‚Äôt match the repository\n\nFirst thing you‚Äôll notice is the file changes to brown/orange and has an ‚ÄúM‚Äù for modified by it.\n\n\n\n\n\nHow do I get this into the online repository?\n\nClick on the Source Control tab and you will see this file listed in the ‚ÄúChange List‚Äù\n\n\n\nBecause I‚Äôm in charge of this repository, I want to ‚Äúcommit‚Äù this file and then ‚Äúpush‚Äù it to the repository.\n\n\nI type a commit message (REQUIRED and will silently fail if not), and then select Commit and Push.\n\nNow my Source Control tab is clean. That means my local code matches the remote repository\n\n\n\n\n\nBut wait, what happens to other people who also have edited the file?\n\nGitting into a predicament.\n\n\nSuppose your instructor says ‚Äúokay, now pull the latest code from the course repository‚Ä¶‚Äù\n\nIf you do that, VS Code might scold you.\nWhat does this mean?\nIt means YOU have changes on your computer that are different from what‚Äôs on GitHub.\n\nGit can‚Äôt pull because it doesn‚Äôt know how to resolve the conflict."
  },
  {
    "objectID": "Machine_Learning/introduction_big_data.html#you-can-also-keep-both-sets-of-changes-by-merging-them",
    "href": "Machine_Learning/introduction_big_data.html#you-can-also-keep-both-sets-of-changes-by-merging-them",
    "title": "11¬† Diving into Machine Learning on Big Data",
    "section": "You can also keep both sets of changes by ‚Äúmerging‚Äù them",
    "text": "You can also keep both sets of changes by ‚Äúmerging‚Äù them\n\n\n\nFigure¬†14.1: Example: Keep and Merge\n\n\n\nHere‚Äôs an example where I might want to keep them.\nI clicked on the change list and opened it. You can see where I modified the In-class exercise.\nYou can research more about this on your own, but for now we‚Äôre just going to avoid it\n\n\nI think the best way to do this is to just move this to your own folder outside the repository.\n\nThere you can preserve all your notes.\n\n\n\nOnce you moved the file (not copied), Git Pull will succeed at getting the newest course code."
  },
  {
    "objectID": "Machine_Learning/introduction_big_data.html#cross-validation-vs-model-performance",
    "href": "Machine_Learning/introduction_big_data.html#cross-validation-vs-model-performance",
    "title": "11¬† Diving into Machine Learning on Big Data",
    "section": "Cross-validation vs model performance",
    "text": "Cross-validation vs model performance\n\nIn either econometrics or ML, there are two tasks in building a model\n\nEstimating parameters of the model\nEvaluating how well that model does\n\nDifferent approaches for these steps between Econometrics and ML:\n\nEconometrics the above steps involve:\n\nt- and p-statistics, hypothesis testing, analyzing specific coefficients\nR-values, AIC/BIC, etc\n\nIn ML, the emphasis is different. The above steps in ML are:\n\nMostly absent in isolation, but included in step 2.\nDetermined by cross-validation of the model."
  },
  {
    "objectID": "Machine_Learning/introduction_big_data.html#returning-to-the-complexity-tradeoff.",
    "href": "Machine_Learning/introduction_big_data.html#returning-to-the-complexity-tradeoff.",
    "title": "11¬† Diving into Machine Learning on Big Data",
    "section": "Returning to the complexity tradeoff.",
    "text": "Returning to the complexity tradeoff.\n\nRecall: Overfitting a model is making the model overly complex to that accuracy falls on the test data.\n\n\n\nWe will talk about ways to methodologically hit the ‚Äúsweet spot‚Äù of model complexity.\nHow do we find this sweet-spot? Cross validation\nFirst though, let‚Äôs illustrate why overly-complex models can UNDER-perform."
  },
  {
    "objectID": "Machine_Learning/introduction_big_data.html#the-source-of-the-sweet-spot-in-model-complexity.",
    "href": "Machine_Learning/introduction_big_data.html#the-source-of-the-sweet-spot-in-model-complexity.",
    "title": "11¬† Diving into Machine Learning on Big Data",
    "section": "The source of the ‚Äúsweet spot‚Äù in model complexity.",
    "text": "The source of the ‚Äúsweet spot‚Äù in model complexity.\nThe complex model is super accurate on the training data.\n\nWith new data, the complex model is much worse. Notice that the simple model performs about the same."
  },
  {
    "objectID": "Machine_Learning/introduction_big_data.html#operationalizing-cross-validation",
    "href": "Machine_Learning/introduction_big_data.html#operationalizing-cross-validation",
    "title": "11¬† Diving into Machine Learning on Big Data",
    "section": "Operationalizing Cross-Validation",
    "text": "Operationalizing Cross-Validation\n\nSplitting data and Cross-Validation\n\nIn this course, we will use scikit-learn to illustrate this.\n\n\n\nScikit-learn has nice built-in functions to split our data into training and test data.\n\nThis is the first step of cross-validation approaches.\n\nWe are going to set aside the data and make sure we never use it again until the very end.\n\n\n\nWe train the model on a second split of the data\n\n\nTo train our model, cross-validation creates a second split of the training data. ML algorithms will iteratively try different models/coefficients on this second spit, using whichever performs best on the training-test data.\nBut we can do MORE than that!\n\n\n\nSplitting into MANY Splits and Folds\n\n\nFirst slice your data into n-folds (here \\(n\\) is 5)\nCreate Split \\(n\\) by defining the test-data as fold ùëõ and the training data as the other Folds.\nTest your model on all different splits.\n\nReport the mean-squared error (MSE) over all the splits\n\nWhy are we doing this?\n\nIteratively find the best model.\n\n\n\n\nFinal model performance analysis\n\n\nOnce the best set of parameters are found, the model is compared against the test data from the first split.\nFinal performance assessment then is done with calculating the MSE of the model prediction of Test_X for Test_Y\nThis method is SUPER FLEXIBLE\n\nIt could compare totally dissimilar models in a rigorous way.\nWill helps us choose ‚Äútuning-‚Äù or ‚Äúhyper-parameters‚Äù"
  },
  {
    "objectID": "Machine_Learning/introduction_big_data.html#issue-1-many-lines-might-work.-which-one-should-we-choose",
    "href": "Machine_Learning/introduction_big_data.html#issue-1-many-lines-might-work.-which-one-should-we-choose",
    "title": "11¬† Diving into Machine Learning on Big Data",
    "section": "Issue 1: Many lines might work. Which one should we choose?",
    "text": "Issue 1: Many lines might work. Which one should we choose?"
  },
  {
    "objectID": "Machine_Learning/introduction_big_data.html#svms-naming-digression",
    "href": "Machine_Learning/introduction_big_data.html#svms-naming-digression",
    "title": "11¬† Diving into Machine Learning on Big Data",
    "section": "SVMs: naming digression",
    "text": "SVMs: naming digression\nThose nearest points determine the ‚Äòsupport vectors‚Äô. I think of it as a little person heroically holding up the margin lines."
  },
  {
    "objectID": "Machine_Learning/introduction_big_data.html#svm-math-preliminaries",
    "href": "Machine_Learning/introduction_big_data.html#svm-math-preliminaries",
    "title": "11¬† Diving into Machine Learning on Big Data",
    "section": "SVM: math preliminaries",
    "text": "SVM: math preliminaries\nWith more than two dimensions, we can define this as a hyperplane\n\\[\nX \\beta+b=0\n\\]\n\nPoints above a hyperplane will have \\(x_i^T \\beta+b&gt;0\\), while points below have \\(x_i^T \\beta+b&lt;0\\)\nDenote the distance from a support vector \\(x_s\\) to the hyperplane as \\(M\\) (the margin)."
  },
  {
    "objectID": "Machine_Learning/introduction_big_data.html#svm-objective",
    "href": "Machine_Learning/introduction_big_data.html#svm-objective",
    "title": "11¬† Diving into Machine Learning on Big Data",
    "section": "SVM: objective",
    "text": "SVM: objective\nOur goal will then be to choose \\(\\beta, b\\) to maximize \\(M\\), subject to the constraint that all points must be at least a distance \\(M\\) from the hyperplane.\nWe can write this as \\[\n\\begin{gathered}\n\\max _{\\beta, b} M \\\\\n\\text { such that } \\\\\ny_i \\frac{\\left(x_i^T \\beta+b\\right)}{\\|\\beta\\|} \\geq M \\forall i\n\\end{gathered}\n\\]\n\nBut, maybe no lines work perfectly. What can we do?\n\n\nNew goal: fit as many instances as possible between the lines while limiting the margin violations.\nWe can write this as \\[\n\\begin{gathered}\n\\max _{\\beta, b} M \\\\\\\\\n\\text { such that } \\\\\\\\\ny_i \\frac{\\left(x_i^T \\beta+b\\right)}{\\|\\beta\\|} \\geq M \\forall i \\\\\\\\\n\\sum_i \\xi_i \\leq \\gamma\n\\end{gathered}\n\\]\n\n\\(\\sum_i \\xi_i \\leq \\gamma\\): add some sort of penalty on misclassifications\nMany forms of this penalty term are possible. Here is the simplest one that just says limit sum of misclassification to be below some threshold \\(\\gamma\\).\nYou might be wondering, wouldn‚Äôt this all depend on the \\(\\gamma\\) value?\nYes it does. And we will use Cross-validation to find the best value for this ‚Äúhyperparameter‚Äù."
  },
  {
    "objectID": "Machine_Learning/introduction_big_data.html#we-choose-gamma-via-cv",
    "href": "Machine_Learning/introduction_big_data.html#we-choose-gamma-via-cv",
    "title": "11¬† Diving into Machine Learning on Big Data",
    "section": "We choose \\(\\gamma\\) via CV!",
    "text": "We choose \\(\\gamma\\) via CV!\n\nIteratively try all values of \\(\\gamma\\).\nWhichever one predicts best across the many splits is what we will use.\nThus, we have systematically determined exactly how many outliers we should ignore\n\nFrom the perspective of out-of-sample performance."
  }
]